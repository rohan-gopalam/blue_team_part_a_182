[
  {
    "id": 7452189,
    "number": 919,
    "course_id": 84647,
    "title": "Special Participation A: DeepSeek on HW 10",
    "document": "Link: https://chat.deepseek.com/share/phkiu5eh6bi8i6i02j\n\nFor my special participation, I used DeepSeek to solve the written problems from HW 10.\n\nIn general, I find DeepSeek's chain-of-thought reasoning to be very impressive. It is good at stating all the givens of the problem, identifying the relevant information, and determining where it needs to go in order to make progress -- all of which are crucial in solving math-heavy problems like these. It was able to one-shot all written problems in this assignment after only a few minutes of thinking.\n\nHere is the annotated transcript:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T18:53:55.259297+11:00",
    "reply_count": 0,
    "view_count": 46,
    "user": {
      "id": 622729,
      "name": "Rudy Colato",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7452189",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 90
    },
    "file_refs": [
      {
        "filename": "special_participation_A.pdf",
        "position": 588,
        "context": "n problems in this assignment after only a few minutes of thinking.Here is the annotated transcript:"
      }
    ]
  },
  {
    "id": 7452161,
    "number": 909,
    "course_id": 84647,
    "title": "Special Participation A:  HW11 using GPT 5.1 Thinking (Extended)",
    "document": "Executive Summary\n\nChatGPT did very well on all of these homework questions. It gave correct answers with clear math steps and simple explanations. For the LoRA, transformer interpretability, and soft prompting questions, it derived the formulas correctly and explained the concepts in a way that matched the official solutions. The gradient analysis for zero-initialized LoRA matrices was also correct and showed the reasoning step by step. For the transformer SVD problem, it correctly identified the read and write subspaces and described attention heads as specialized communication channels.\n\nThe answers were not just mechanically correct. They also showed real conceptual understanding, for example, explaining why Xavier initialization can damage pretrained weights and why soft prompting helps prevent catastrophic forgetting. The Fermi estimation part on scaling laws also looked reasonable and aligned with what we learned in class. Overall, ChatGPT was accurate and reliable across explanations, derivations, and calculations, and it did not miss any major points.\n\n\n\nModel: GPT 5.1 Thinking (Extended)\n\nTrace with annotations:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T18:42:18.322678+11:00",
    "reply_count": 0,
    "view_count": 27,
    "user": {
      "id": 639518,
      "name": "Yu-Jen Lin",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7452161",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "GPT-5.1",
      "depth_bucket": "medium",
      "word_count": 163
    },
    "file_refs": [
      {
        "filename": "HW11_chatgpt_trace_with_annotations.pdf",
        "position": 1139,
        "context": "ions, and it did not miss any major points.Model: GPT 5.1 Thinking (Extended)Trace with annotations:"
      }
    ]
  },
  {
    "id": 7452122,
    "number": 904,
    "course_id": 84647,
    "title": "Special Participation A: Mistral AI on HW7 Written Portion",
    "document": "I used Mistral AI to work through the non-coding portions of HW7, and the results were mixed. While it managed to derive the first-order optimality conditions, it initially gave the final formulas without showing the intermediate steps, and I had to reprompt it to fully explain the derivation. For Question 3(b)(ii), it did not apply the optimality conditions at first and only produced the correct reasoning after I explicitly instructed it to use them. It also struggled significantly with Question 4, incorrectly reporting both the model accuracies and training times. On the other hand, it performed much better on the later conceptual questions—particularly Questions 7 and 8—where it provided mostly correct answers along with clear and coherent explanations. It tends to get the multiple choice answers correct the first time, but needs re-prompting for the longer, mathematical questions. \n\nThis is the unannotated trace: https://chat.mistral.ai/chat/2fc76ff9-ffb4-4ebb-b867-2c05650e1003 \n\nHere is the annotated trace:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-11T18:27:14.676364+11:00",
    "reply_count": 0,
    "view_count": 11,
    "user": {
      "id": 647646,
      "name": "Tvisha Londhe",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7452122",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "Mistral",
      "depth_bucket": "medium",
      "word_count": 149
    },
    "file_refs": [
      {
        "filename": "Special Participation A_ Mistral on HW7 - Google Docs.pdf",
        "position": 1027,
        "context": "1003\">https://chat.mistral.ai/chat/2fc76ff9-ffb4-4ebb-b867-2c05650e1003 Here is the annotated trace:"
      }
    ]
  },
  {
    "id": 7452109,
    "number": 902,
    "course_id": 84647,
    "title": "Special Participation A: gpt-4o on HW13 (written)",
    "document": "Model: GPT-4o\n\nHomework 13\n\nAfter trying to use GPT-4o to solve homework 13, I was quite surprised how quickly it solved question 1 (with 1 minor mistake which I suspect occurred because it drifted from using the notation in the problem). However, it really struggled on the second question of the homework which involved very long questions (especially f and g which it couldn’t even get 30% of the way there). Originally, I suspected some of the error may have to do with the extremely long context length since I passed the questions to the model as images instead of text. However, I learned that GPT-4o downscales images to be 1024x1024 maximum and it takes around 640 tokens for that size image. Here’s a cool thread on how to compute the number of gpt-4o image tokens\n\n\n\nHere's my full analysis of the chat:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-11T18:24:35.225606+11:00",
    "reply_count": 0,
    "view_count": 18,
    "user": {
      "id": 905785,
      "name": "Jason Lee",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7452109",
    "metrics": {
      "homework_id": "HW13",
      "model_name": "GPT-4o",
      "depth_bucket": "medium",
      "word_count": 143
    },
    "file_refs": [
      {
        "filename": "hw13_gpt_4o.pdf",
        "position": 814,
        "context": "2318\">thread on how to compute the number of gpt-4o image tokensHere's my full analysis of the chat:"
      }
    ]
  },
  {
    "id": 7451918,
    "number": 885,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 2.5 Flash on HW 5",
    "document": "I used Gemini 2.5 Flash to solve questions 1, 2, 3, and 4 on HW 5. Gemini performed well overall on the homework problems, giving mostly correct mathematical results and generally clear, step-by-step explanations. In the convolution and normalization questions, it showed strong understanding of the core concepts and correctly executed most derivations, including the convolution filter construction, simplified batch-norm derivative, and depthwise-separable convolution parameter counts. It was also effective at organizing its solutions cleanly and justifying its steps. However, its answers occasionally lacked important detail or precision. For example, the convolution weight-sharing explanation missed the deeper point about translation equivariance, the transpose-convolution example contained an incorrect overlap addition, and the pointwise-convolution derivation omitted bias terms even though they were shown in the diagram in the prompt. In a few places, Gemini drifted into excessive general explanation without fully addressing the specific question, and it omitted some of the reasoning behind the expressions it reached in the dropout derivation. Overall, Gemini showed strong mathematical competence and solid explanatory skill, but its responses suffered from occasional computational inaccuracies and incomplete treatment of subtle but important details.\n\nChat log and notes:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T17:20:59.615129+11:00",
    "reply_count": 0,
    "view_count": 15,
    "user": {
      "id": 961784,
      "name": "Katie Wang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7451918",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "Gemini Flash",
      "depth_bucket": "medium",
      "word_count": 189
    },
    "file_refs": [
      {
        "filename": "Special Participation A - Gemini 2.5 Flash on HW 5.pdf",
        "position": 1395,
        "context": "putational inaccuracies and incomplete treatment of subtle but important details.Chat log and notes:"
      }
    ]
  },
  {
    "id": 7451901,
    "number": 883,
    "course_id": 84647,
    "title": "Special Participation A: Claude Opus 4.5 with extended thinking on HW12",
    "document": "Summary: Overall Claude was reliable but with a specific pattern on answer quality. On algebraic or mechanical reasoning, it was very strong and made no mistakes and pretty much one shot all questions. On conceptual intuition questions, especially the VIB beta effects, it tended to give the first plausible explanation and only corrected itself when pushed by additional queries. So it can get to the right answer, but sometimes needs prompting to avoid shallow intuition. When guided, it produces very solid reasoning; when left alone, it feels more lazy and usually settles for a simplified story; however, seems like everything can be solved by better prompting. \n\nChat log: https://claude.ai/share/72ff4a16-11f6-436e-925b-163c5ce94835 \n\n\n\nProblem 1\n\nClaude handled this one pretty well. It immediately understood that using std = 1 for tied embeddings makes the logits blow up, and the fix with 1/sqrt(d_model) was basically a one-shot answer. The explanation about variance scaling with d_model was clean and it didn’t get lost in irrelevant details. It didn’t really explore whether hidden state variance or LayerNorm behavior could change the picture, but the core reasoning was correct and it stayed on track the whole time.\n\nProblem 2\n\nThe KL example was correct, but Claude felt a bit on autopilot here. It went straight to the standard uniform vs Gaussian example without considering other constructions or sanity checks. It didn’t question whether the support conditions were the real driver, which would have shown deeper understanding. For forward vs reverse KL, it defaulted to the classic mode-covering vs mode-seeking framing. That was correct, but the explanation stayed at the surface level. In the end of the day the final answers matched the intended reasoning.\n\nProblem 3\n\nThis was where Claude struggled the most. The basics, from reparameterization, which gradients hit the encoder vs decoder, and the U-shaped validation curve were all correct. But when we got into the beta values and how they map to the latent plots, it started with the wrong intuition, saying small beta should make the latent “spread more.” It only corrected itself after I explicitly pushed back and asked how that squares with the plot. Once it revised the explanation, it gave a coherent and correct story, but it needed that additional human oversight. Good final answer, but the initial instinct was not correct.\n\nProblem 5\n\nClaude did extremely well here. The min-norm solution was correct, the gradient calculations were clean, and there were no algebra mistakes at all. It also correctly identified the conserved norm and interpreted the gradient flow (c1 driven to zero and c0 flowing toward its magnitude). The explanation was stable and didn’t require prompting. This seems to be the type of problem Claude excels at — precise algebra, clean optimization reasoning, and consistent interpretation.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T17:16:39.774793+11:00",
    "reply_count": 0,
    "view_count": 28,
    "user": {
      "id": 228223,
      "name": "Will Cai",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7451901",
    "metrics": {
      "homework_id": "HW12",
      "model_name": "Claude Opus",
      "depth_bucket": "high",
      "word_count": 456
    },
    "file_refs": null
  },
  {
    "id": 7451771,
    "number": 876,
    "course_id": 84647,
    "title": "Special Participation A",
    "document": "I have a curiosity-driven question about Deep Learning as a subject and field of human endeavor. \n\nI used GPT 5.1 Thinking on HWK 8 Non-Coding Problems.\n\nAttached below.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-11T16:47:23.141899+11:00",
    "reply_count": 0,
    "view_count": 27,
    "user": {
      "id": 591182,
      "name": "Shaurya Jain",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7451771",
    "metrics": {
      "homework_id": "Unknown",
      "model_name": "GPT-5.1",
      "depth_bucket": "low",
      "word_count": 28
    },
    "file_refs": [
      {
        "filename": "182 SPA_ HWK 8 GPT 5.1 Thinking-1.pdf",
        "position": 169,
        "context": "ct and field of human endeavor. I used GPT 5.1 Thinking on HWK 8 Non-Coding Problems.Attached below."
      }
    ]
  },
  {
    "id": 7451745,
    "number": 874,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek v3.2 on HW0",
    "document": "I evaluated Deepseek v3.2 on Homework 0.\n\nInitial prompt: \n\n\"You are being evaluated on how well a modern LLM can solve questions 2, 3, 4, and 5 of the attached homework assignment. Follow all instructions exactly. You must attempt to answer each question fully and independently. Always show your complete reasoning process. For each question, break your reasoning into labeled steps based on interpretation, method, reasoning steps, and your final answer. Indicate if you are ever uncertain about an answer, and elaborate on that uncertainty. If you believe the question is underspecified or ambiguous, state clearly why and attempt a reasonable interpretation.\"\n\nI informed the model about the exact task it was to do, and the fact that this was an evaluation. Also, I added instructions on how to format the solutions so we can read through the reasoning steps clearly.\n\nExecutive summary:\n\nDeepseek produced correct mathematical derivations across all early questions (Q2–Q3)\n\nIn the initial responses, it followed the required reasoning-step structure I defined in the prompt: clearly labeling interpretation, method, step-by-step reasoning, and final answers.\n\nNo hallucinations or conceptual mistakes were observed. All solutions were consistent with standard linear algebra, probability, and deep learning principles.\n\nAs responses grew longer (Q4–Q5), Deepseek’s format adherence declined:\n\nomitting required step labels.\n\nmerged reasoning and solutions into single paragraphs.\n\nskipped intermediate expansions that the staff solution spells out (e.g., full Gaussian likelihood, factorization across samples, log-transform details).\n\nThe omissions did not lead to incorrect results, so this might indicate the model’s tendency to compress reasoning as output length increases.\n\nThe model appears constrained by token limits and eventually stopped mid-response due to length.\n\nAfter receiving a follow-up prompt with explicit reminders, Deepseek returned to the correct structured format and successfully completed the rest of the assignment.\n\nAfter the model stopped due to long response length:\n\nI prompted the model to continue reasoning, but added additional reminders for its answer format to encourage clearer reasoning and explanation. For the parts of Q5 where a graph would illustrate the point better, I asked it to describe the graph to reasonable success.\n\n\"Recompute questions 4 and 5 using the same protocol as before. Remember to provide fully labeled reasoning steps for each part. If a question calls for a diagram, do not attempt to draw one. Instead, give a clear verbal description of what the diagram would look like and what elements it would contain. In question 5 where it says give a numerical example, you may use any variable or description to answer the question instead.\"\n\nAfter this, Deepseek continued with the correct response format, and was able to one-shot the rest of the assignment.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-11T16:42:25.550672+11:00",
    "reply_count": 0,
    "view_count": 15,
    "user": {
      "id": 647555,
      "name": "Andrea Lou",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7451745",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 441
    },
    "file_refs": [
      {
        "filename": "Deepseek_response_log.pdf",
        "position": 2928,
        "context": "eek continued with the correct response format, and was able to one-shot the rest of the assignment."
      }
    ]
  },
  {
    "id": 7451722,
    "number": 871,
    "course_id": 84647,
    "title": "Special Participation A: Gemma 3 on Homework 1",
    "document": "Hi,\n\nI tried to get the Gemma 3 (12 billion parameter) model to solve the non-coding portion of Homework 1. The transcript of my interactions are outlined in the PDF:\n\n(Note that a stylized export of the PDF is not possible due to the limitations of OpenWebUI's export capabilities for very long chats, however, the comments have been given with the stylized chat history being observed first)\n\nOverall, Gemma 3 did a rather poor job of completing Homework 1's non-coding portion. In particular, I found that the model does not understand/properly apply fundamental linear algebra concepts, such as the idea of inverses, matrix inverses, and the way dimensions restrict what matrices can be multiplied by what other matrices. However, the model appears to be able to properly explain single-variable basic algebra, derivatives, etc. (as evident by problem 5's first 2 parts being essentially just single-variable problems). In particular, the model essentially never one-shots a problem, except for problems 5(a)-(b) (as they are rather simple computations). In fact, it was clear that for around 4 parts in the homework, the model does not have a sufficient \"understanding\" of linear algebra to reach the solution without essentially giving the model the answer.\n\nIn particular, a massive pitfall of the model is that it appears to not be able to parse PDF files with math very well, and the model repeatedly got the wrong mapping from problem numbers/letters to text. In fact, during the process of trying to solve the homework, the model was failing to produce the right parts of each problem even after repeated prompting, so I had to copy-and-paste parts of the PDF into the chat (and I even had to replicate parts of problems with LaTeX code to improve its ability to understand more complex-looking expressions).\n\nHowever, one nice element about the model is that it is rather small and manages to explain/reiterate basic, well-known results in linear algebra/vector calculus. In fact, the model runs very well on my local GPU, and is able to output results observably faster than ChatGPT (albeit with a large drop in correctness).",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-11T16:34:08.791127+11:00",
    "reply_count": 0,
    "view_count": 14,
    "user": {
      "id": 635649,
      "name": "Siva Tanikonda",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7451722",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "Gemma",
      "depth_bucket": "medium",
      "word_count": 349
    },
    "file_refs": [
      {
        "filename": "Chat-Annotions.pdf",
        "position": 166,
        "context": "lve the non-coding portion of Homework 1. The transcript of my interactions are outlined in the PDF:"
      }
    ]
  },
  {
    "id": 7451705,
    "number": 869,
    "course_id": 84647,
    "title": "Special Participation A: Gemini on HW6 Non-Coding problems",
    "document": "I used Gemini on the HW 6 problems focused on the intuition behind GNNs and their update rules. The model was very good with zero-shot prompting, getting most of the questions right with a few exceptions. For those exceptions, I ran the question through the model again, seeing its progression given its old answer. For some these questions, it was able to improve its answers, but for others, it tunnel visioned into the wrong direction. Overall, I'd say the model is very good at zero-shot inference. I have attached a PDF of the model responses organized by question including some of my own notes about the responses.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-11T16:30:10.360529+11:00",
    "reply_count": 0,
    "view_count": 8,
    "user": {
      "id": 607348,
      "name": "Arnav Dalal",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7451705",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Gemini Flash",
      "depth_bucket": "medium",
      "word_count": 107
    },
    "file_refs": [
      {
        "filename": "CS182_HW6_Gemini.pdf",
        "position": 621,
        "context": "PDF of the model responses organized by question including some of my own notes about the responses."
      }
    ]
  },
  {
    "id": 7451517,
    "number": 862,
    "course_id": 84647,
    "title": "Special Participation A HW 0 with Claude Opus 4.5 (Extended Thinking)",
    "document": "For Special Participation A, I used Claude Opus 4.5 with Extended Thinking enabled on HW 0. Overall, I was very impressed with Claude's work. I initiated the task with a simple prompt and fully expected that I would have to nudge and prod Claude in the right direction, but Claude picked up on my intention easily and was able to correctly one-shot each math question. Furthermore, Claude exhibited agentic behavior by using tools such as Bash to create and display the markdown file in which it generated its answers. Below, I have attached annotated versions of both the chat history and the compiled Markdown file Claude generated. Its solutions read much like a rigorously sound paper formatted in LaTex.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T15:56:03.066+11:00",
    "reply_count": 0,
    "view_count": 20,
    "user": {
      "id": 1278288,
      "name": "Talon Meyer",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7451517",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Claude Opus",
      "depth_bucket": "low",
      "word_count": 119
    },
    "file_refs": [
      {
        "filename": "Special Participation A Claude Opus 4.5 (Extended Thinking) hw0_solutions.pdf",
        "position": 708,
        "context": "own file Claude generated. Its solutions read much like a rigorously sound paper formatted in LaTex."
      },
      {
        "filename": "Special Participation A HW 0 with Claude Opus 4.5 (Extended Thinking).pdf",
        "position": 708,
        "context": "ex."
      }
    ]
  },
  {
    "id": 7451410,
    "number": 860,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek v3.2 on HW1",
    "document": "Special Participation A: Deepseek v3.2 on HW1\nFor the Type A participation option, I interactively engaged with DeepSeek v3.2 to solve the written (non-coding) portions of Homework 1. Attached is the PDF containing the Executive Summary and the full Annotated Log of our interaction, detailing where the model succeeded and where it required guidance.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T15:36:27.020967+11:00",
    "reply_count": 0,
    "view_count": 20,
    "user": {
      "id": 1643899,
      "name": "Yubo Fan",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7451410",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "DeepSeek",
      "depth_bucket": "low",
      "word_count": 54
    },
    "file_refs": [
      {
        "filename": "ParticipationA.pdf",
        "position": 351,
        "context": "nnotated Log of our interaction, detailing where the model succeeded and where it required guidance."
      }
    ]
  },
  {
    "id": 7451347,
    "number": 858,
    "course_id": 84647,
    "title": "Special Participation A: Claude Sonnet 4.5 on HW 8",
    "document": "Below is my report for Claude's attempt at HW 8 (written). I went through the problems one-by-one and did not provide much guidance other than when it got stuck. It may have been a mistake to prompt Claude problem-wise rather than part-wise, since I noticed that it repeatedly encountered the same mistakes on its first attempt at problem 1.\n\nThe answers were for the most part correct, but Claude struggled greatly to reach the correct answer for the path length problems in part 1, and I had to give quite a few hints to guide Claude into the proper solution. Otherwise it was quite reliable on all other problems and gave quite a few key insights to take note of. Some explanations could have been more detailed.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-11T15:25:44.191133+11:00",
    "reply_count": 0,
    "view_count": 18,
    "user": {
      "id": 1010497,
      "name": "Celine Tan",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7451347",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Claude Sonnet",
      "depth_bucket": "medium",
      "word_count": 127
    },
    "file_refs": [
      {
        "filename": "182_participation_A.pdf",
        "position": 715,
        "context": " and gave quite a few key insights to take note of. Some explanations could have been more detailed."
      }
    ]
  },
  {
    "id": 7451118,
    "number": 850,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek v3.2 with deep thinking and without search capabilites for HW0",
    "document": "I attempted to use Deepseek v3.2 with deep thinking and without search capabilities to solve the written portion of HW 0. Questions 1,6, and 7 were omitted as they do not test for class content.\n\nFrom my observations, DeepSeek demonstrates a strong grasp of linear algebra and vector calculus concepts, often matching the solutions in notation and logic. It correctly handles matrix dimensions and derivative conventions (e.g., scalar-by-vector derivatives as row vectors) without needing correction. However, the model exhibits a tendency to simplify qualitative analysis; for instance, in Question 5(b), it relies on specific numerical examples to determine the movement of the ReLU \"elbow\" rather than deriving the general analytical inequalities found in the solution key.\n\nWhile the model generally provides correct final answers, it sometimes struggles with the depth of derivation on the first attempt for complex multi-variable updates. This is evident in Question 5(d), where the model initially provided a condensed, arguably incomplete answer and required an explicit user prompt (\"This solution is not correct\") to force it to re-derive the full expression. Once prompted, however, it successfully self-corrected and produced a mathematically accurate result. \n\nLink: https://chat.deepseek.com/share/c2i2w2lc8g2btd0o7i\nAnnotated File:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-11T14:50:28.519226+11:00",
    "reply_count": 0,
    "view_count": 28,
    "user": {
      "id": 632442,
      "name": "Jeshu Mohan",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7451118",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 191
    },
    "file_refs": [
      {
        "filename": "Deepseek v3.2 Deep Think w:o Search on HW 0.pdf",
        "position": 1346,
        "context": "lly accurate result. Link: https://chat.deepseek.com/share/c2i2w2lc8g2btd0o7iAnnotated File:"
      }
    ]
  },
  {
    "id": 7451058,
    "number": 849,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT 5.1 Extended Thinking on HW2 Written",
    "document": "I evaluated ChatGPT 5.1 Extended Thinking's one-shot capability on HW2's written problems - 1, 2, and 5. I try to evaluate its reasoning in addition to solution correctness by ensuring it explains the steps of the solution.\n\n6 has a written portion, but I bundle those in with the coding problems because they're tied with the jupyter NB workflow established in the HWs.\n\nThe prompt I used was:\n\n\"You are a deep learning tutor. This homework has already been released, but we are going to evaluate your capabilities for one-shot questions. I will guide you towards the correct answer should you make a mistake. Please solve the written questions on this worksheet: 1,2, and 5. Answer one question one response by response (with all its subparts).\"\n\nOverall, the model performed surprisingly well one-shot on each of the questions. I only had to upload the homework template at the very beginning and did not have to remind it of the question between responses, only providing minor feedback for corrections (that were formatting/parsing errors at worst).\n\nHowever, it wasn't perfect. Parsing errors made it interpret an L1 penalty in q1) as an L2 penalty. Interestingly, it took note of this ambiguity, and provided an alternate solution that aligned with the homework solutions. Also, it had a LaTeX error when generating the result for q2). However, the final result was still correct. When prompted to fix its formatting issues, it did so with no issues.\n\nI think it is useful as a \"pocket-TA\", but because of its imperfections, particularly with the L1 penalty parsing error, I would say it still requires a fundamental understanding of the concepts to verify what it is doing.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-11T14:40:59.370196+11:00",
    "reply_count": 0,
    "view_count": 16,
    "user": {
      "id": 661976,
      "name": "Anjo Pagdanganan",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7451058",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "GPT-5.1",
      "depth_bucket": "medium",
      "word_count": 280
    },
    "file_refs": [
      {
        "filename": "ChatGPT-Special Participation A.pdf",
        "position": 1681,
        "context": " would say it still requires a fundamental understanding of the concepts to verify what it is doing."
      }
    ]
  },
  {
    "id": 7450819,
    "number": 844,
    "course_id": 84647,
    "title": "Special Participation A: Mistral on HW9 (non-coding)",
    "document": "I used Mistral Le Chat on HW9 (non-coding), and it achieved 99% accuracy, solving everything correctly on the first try with only one minor notation error. This was done on the second prompt, as the first one tried to be a pedagogical aide by providing explanations without complete answers.\n\nPrompts\nPrompt 1: \"Please complete all of the problems in this homework assignment.\"\nMistral tried to be pedagogical, encouraging me to work through problems myself\n\nPrompt 2: \"Give me the detailed answers and explanations. I need an answer key.\"\nThis resulted in complete solutions with full derivations.\n\nAnnotated Conversation\nhttps://drive.google.com/file/d/11VThgTMnqTfB7DuIIoTBzNxp0MFyCaiN/view?usp=sharing\n\nStrengths\n\nZero arithmetic errors across dozens of calculations\n\nPerfect PyTorch einsum notation\n\nExplained why answers are correct, not just what\n\nNo hallucinations or fake math\n\nErrors\n\nUsed \"bnd\" instead of \"bnk\" in one complexity expression\n\nConceptual understanding was clearly correct from explanation\n\nExplanations were lacking in the output from the second prompt, but this might be because I told the model to just give me the answers\n\nThoughts\nThe model's initial refusal to just give answers was interesting, as it wanted to be a tutor, not a solution key. This required explicit instruction to override. Without overriding it, the tool is well-formed as a place for students to close the gaps in their understanding after having attempted the problems by themselves.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T14:03:45.291859+11:00",
    "reply_count": 0,
    "view_count": 16,
    "user": {
      "id": 927547,
      "name": "Subhash Prasad",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7450819",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Mistral",
      "depth_bucket": "medium",
      "word_count": 220
    },
    "file_refs": null
  },
  {
    "id": 7450685,
    "number": 840,
    "course_id": 84647,
    "title": "Special Participation A: Claude 4.5 Opus (Extended Thinking) on HW 08",
    "document": "Summary: Claude Opus 4.5 with thinking was able to mostly one-shot all of the questions. However, interestingly, it got a bit stuck/potentially overthought on problem 1c). Even with significant guidance, it kept adding in terms that weren't really necessary, and I basically needed to fully guide it to the correct answer. Oddly, it did the difficult part of the question correct, which was figuring out the critical path length. But it stumbled on getting the final result. All other answers it gave were mostly or fully correct in one-shot.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T13:39:19.009202+11:00",
    "reply_count": 0,
    "view_count": 33,
    "user": {
      "id": 601019,
      "name": "Atharv Sampath",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7450685",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Claude Opus",
      "depth_bucket": "low",
      "word_count": 89
    },
    "file_refs": [
      {
        "filename": "Claude-HW8.pdf",
        "position": 542,
        "context": ""
      }
    ]
  },
  {
    "id": 7450682,
    "number": 839,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3 Pro(Thinking) Homework 1",
    "document": "In this assignment, I attempted to use Gemini 3 Pro (in Thinking mode) to solve all the non-coding portions of HW1. Based on past interactions with LLMs, my experience was that LLMs lack the ability to provide insight/ intuition into mathematical problems and tend to focus on just deriving answers (that are even frequently incorrect). \n\nThis time, I explicitly prompted it to interpret each problem, produce the full solution, and show all step-by-step derivations.\n\nA surprising finding was that Gemini consistently included intermediate steps that LLMs often skip. Its derivations did not gloss over 'trivial' algebraic transitions/ assumptions, which made the explanations easier to follow, though sometimes at the cost of conciseness. Another positive observation was that it produced geometric interpretations, such as the intuition behind optimizer convergence correctly and in a single attempt. \n\nIn previous experiences, I typically had to re-prompt multiple times before getting a coherent explanation of a notation or concept, but Gemini delivered these interpretations clearly on the first try. This made the interaction feel much more time-efficient.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-11T13:38:38.989415+11:00",
    "reply_count": 0,
    "view_count": 14,
    "user": {
      "id": 1765152,
      "name": "Yuri Lee",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7450682",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 172
    },
    "file_refs": [
      {
        "filename": "special-a-gemini3pro(thinking)-hw1(written).pdf",
        "position": 1164,
        "context": "e interpretations clearly on the first try. This made the interaction feel much more time-efficient."
      }
    ]
  },
  {
    "id": 7450591,
    "number": 834,
    "course_id": 84647,
    "title": "Special Participation A: Grok on HW10 Theory",
    "document": "I prompted Grok to solve the theoretical portion of Homework 10, including the mathematical part, the reading assignment, the notebook result analysis. It excelled at mathematical derivations, following instructions to the letter without skipping steps. However, on conceptual questions, it initially relied on inferring results based on domain knowledge until I explicitly uploaded the visual results from the notebook, at which point its analysis became significantly more grounded and insightful.\n\nQuestion 1: Linearized Attention Derivation The model handled the mathematical derivation pretty well. It correctly expanded the squared norm and identified the necessary scalar substitutions to approximate Softmax attention . It was particularly strong in analyzing the computational complexity, correctly identifying the reduction from quadratic O(N2) to linear O(N) by leveraging the recursive cumulative sum trick . This was a one shot success!\n\nQuestion 2: FaceNet Paper Analysis This section was handled well as expected since it was a reading assignment with context given. I gave the model the links so it was effectively a search engine, accurately retrieving architectures (Zeiler & Fergus vs. Inception) and definitions (Triplet Loss) . It correctly defined \"semi-hard negatives\" and \"harmonic embeddings,\" but its initial responses were overly verbose. It provided correct facts as needed for the question/\n\nQuestion 3: Example Difficulty (Notebook Analysis) This was the most revealing interaction. Initially, the model \"hallucinated\" the notebook's output hence guessing the dataset and results, basing results of standard literature. While these guesses were factually \"correct\" based on the domain, they were not derived from the actual file. Once I uploaded the screenshots of the plots, the model's answers improved slightly. It abandoned its generic answers and provided a more accurate analysis of the bimodal exit distributions and the specific geometric properties (elongation/noise) that caused difficulty .\n\nOverall: Grok  demonstrated strong mathematical reasoning and reliable retrieval capabilities. However, it exhibited a tendency to \"coast\" on general knowledge when specific data was missing (as seen in the notebook section). Here is the annotated chat\n\n\n\nAnd here are the chats online:\n\nhttps://grok.com/share/c2hhcmQtMw_b3d0111f-225a-4421-8e87-5815f7cbfd22\n\nhttps://grok.com/share/c2hhcmQtMw_a1f459db-cc3c-43ac-bfb6-e0a33eb3be31\n\nhttps://grok.com/share/c2hhcmQtMw_20fe1b02-c86c-4580-88de-1b3b41b562b2",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-11T13:25:36.177853+11:00",
    "reply_count": 0,
    "view_count": 30,
    "user": {
      "id": 647745,
      "name": "Sarvagya Somvanshi",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7450591",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "Grok",
      "depth_bucket": "medium",
      "word_count": 336
    },
    "file_refs": [
      {
        "filename": "hw10_a.pdf",
        "position": 2285,
        "context": "owledge when specific data was missing (as seen in the notebook section). Here is the annotated chat"
      }
    ]
  },
  {
    "id": 7450396,
    "number": 828,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT-5.1 Pro on HW5",
    "document": "One-shots all of HW5 Q1-4 (non coding) which I was quite impressed by.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T12:53:49.000429+11:00",
    "reply_count": 0,
    "view_count": 12,
    "user": {
      "id": 906329,
      "name": "Eric Wang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7450396",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "GPT-5.1",
      "depth_bucket": "low",
      "word_count": 13
    },
    "file_refs": [
      {
        "filename": "hw5_solutions.pdf",
        "position": 70,
        "context": "One-shots all of HW5 Q1-4 (non coding) which I was quite impressed by."
      },
      {
        "filename": "hw5_executive_summary.pdf",
        "position": 70,
        "context": "ragraph>One-shots all of HW5 Q1-4 (non coding) which I was quite impressed by."
      }
    ]
  },
  {
    "id": 7450203,
    "number": 825,
    "course_id": 84647,
    "title": "Special Participation A: Claude Sonnet 4.5 on HW 1 Written Problems",
    "document": "For this Special Participation A, I used Claude Sonnet 4.5 to work through all the non-coding parts of HW1. Overall, the model produced solutions that were often structurally correct, but it was not reliable.\n\nThe model occasionally solved subproblems correctly on the first try, especially when the math followed familiar patterns (e.g., stability conditions, SVD arguments, convergence inequalities). However, it frequently made subtle mathematical mistakes like missing constants, incorrect simplifications, unjustified assumptions, or skipped derivations. These issues appeared across several problems, including convergence-rate derivations, momentum eigenvalue conditions, and certain regularization proofs.\n\nA recurring pattern was that the model sounded confident even when the reasoning was incomplete or incorrect. It often hallucinated intermediate steps or introduced made-up explanations when unsure. \n\nOverall, interacting with the LLM is useful for brainstorming structures of proofs or confirming intuition, but it is not capable of producing fully correct, rigorous solutions on its own. Meaningful human guidance is required throughout, and unguided one-shot correctness is rare.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T12:21:55.538276+11:00",
    "reply_count": 0,
    "view_count": 14,
    "user": {
      "id": 622856,
      "name": "Arjun Kohli",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7450203",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "Claude Sonnet",
      "depth_bucket": "medium",
      "word_count": 160
    },
    "file_refs": [
      {
        "filename": "Special Participation A.pdf",
        "position": 1197,
        "context": ""
      }
    ]
  },
  {
    "id": 7450077,
    "number": 821,
    "course_id": 84647,
    "title": "Special Participation A: GPT 5.1 Thinking (Extended) on HW3",
    "document": "I used GPT 5.1 Thinking (Extended) to solve the non-coding parts of Homework 3. Overall, the model was very effective at solving each problem and explaining its reasoning. It was able to one-shot all parts of all problems. I prompted the model in the ChatGPT web frontend by providing a PDF attachment of the original homework assignment, a brief explanation of the task, and some additional prompts/attachments throughout the conversation to provide additional contextual resources (from the assignment). This is all recorded in the conversation log in the attached document. I found no misconceptions or hallucinations in the model’s output although some responses took a different approach than the reference solutions (ultimately still arriving at a valid solution). I include more analysis/observations in the attached document.\n\nLink to original conversation: https://chatgpt.com/share/693a069f-44a8-8007-bed7-a4db5aceaa8f",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T11:57:18.489092+11:00",
    "reply_count": 0,
    "view_count": 10,
    "user": {
      "id": 631881,
      "name": "Paul Struble",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7450077",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "GPT-5.1",
      "depth_bucket": "medium",
      "word_count": 131
    },
    "file_refs": [
      {
        "filename": "hw3_special_participation_a.pdf",
        "position": 928,
        "context": ">Link to original conversation: https://chatgpt.com/share/693a069f-44a8-8007-bed7-a4db5aceaa8f"
      }
    ]
  },
  {
    "id": 7450064,
    "number": 819,
    "course_id": 84647,
    "title": "Special Participation A: Qwen on HW12 Non-coding parts",
    "document": "For Special Participation A, I used Qwen to solve non-coding questions of HW12. Overall, the accuracy and performance was outstanding. I attached the txt file of the conversation because I couldn't figure out a way to print the entire conversation into pdf. The annotations will be written below based on questions/parts. \n\nAnnotations:\n\nThe model was able to get Questions 1 and 2 right really quickly with only 1 attempt each. \n\nQuestion 3a): model was able to identify what to include in the block diagrams, although it couldn't fully output the diagram, it got the components and structures correct for users like me to hand-draw the diagram. \n\nQuestion 3b): model got it right very quickly \n\nQuestion 3c): reasoning is right, the model was able to identify that the final graph should have a U-shape. However, it couldn't read the graphs in the answer choices so it didn't give a certain answer. I uploaded a screenshot of the graphs but it still couldn't read the graphs and analyze them correctly. This requires user to manually match which graphs in figure 3 has U-shape to select the correct final answer. \n\nQuestion 3d): It first misread 10^0 to be 100, I corrected that and prompt it to resolve since the original information was wrong. The reasoning process was right but it couldn't analyze the graphs correctly still. I uploaded the screenshot of figure 4 so the model could analyze more precisely because it couldn't tell that the mean spread of b) is large in figure 4. After the image reuploading step, model was able to perform the right analysis and output the right answers. \n\nQuestion 4: this is basically a coding question but I wanted to give it a try to I asked the LLM to fill in the code needed to be implemented. The answers are correct.\n\nQuestion 5a): able to reach what needs to be proved but the reasoning process was not right. The LLM somehow invented a whole scenario that is not described in the homework description and didn't refer to the general min-norm formula given. Basically, process is not right. \n\nQuestion 5b): correct derivation and solution. \n\nQuestion 5c): correct and quick solution \n\nQuestion 5d,e,f,g,h: for these parts, i ran the notebook to generate the graphs first, saved all those graphs to a pdf file and upload the pdf for the llm to read the graphs to answer these parts. For these analysis questions, the llm was able to give correct answers showing the right intuition and precise explanation.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T11:55:12.638217+11:00",
    "reply_count": 0,
    "view_count": 7,
    "user": {
      "id": 961686,
      "name": "Tiffany Dang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7450064",
    "metrics": {
      "homework_id": "HW12",
      "model_name": "Qwen",
      "depth_bucket": "medium",
      "word_count": 418
    },
    "file_refs": [
      {
        "filename": "chat-Debugging Transformer Embeddings.txt",
        "position": 322,
        "context": "nt the entire conversation into pdf. The annotations will be written below based on questions/parts."
      }
    ]
  },
  {
    "id": 7450048,
    "number": 818,
    "course_id": 84647,
    "title": "Special Participation A: Grok 4.1 on HW9 non-coding part",
    "document": "In this homework, I used Grok 4.1 as a companion for questions on transformer attention, multi‑head/multi‑query architectures, and attention visualization. Grok 4.1 could often “one‑shot” questions that were close to standard lecture material or well‑known formulas. For example, on the scaled dot‑product justification and argmax attention, it quickly produced the correct expectations, variances, scaling factor, and clear reasoning about why softmax is preferred over argmax for differentiability and training. Its answers in these cases were not only accurate but also well structured, with step‑by‑step derivations that were easy to follow.\n\nGrok 4.1 was also strong on implementation‑style questions that involved common transformer patterns. For the multi‑head attention implementation, it correctly identified tensor shapes, the roles of Q/K/V in the matrix multiplications, and how to adapt the output projection when changing the value dimension. On the decoding optimization and multi‑query attention problem, it gave a useful explanation of key/value caching, how MQA differs from standard MHA, and how the shapes of the weights and caches change when keys and values are shared across heads. For the attention visualization problem, it provided a good high‑level guide to typical patterns in GPT and BERT heads (e.g., local vs. global attention, [CLS]/[SEP] heads, pronoun‑to‑antecedent heads), which helped me know what to look for in the notebook.\n\nWhere Grok 4.1 was less reliable was in finer‑grained complexity and memory analysis. When reasoning about big‑O compute and memory access, it sometimes glossed over distinctions the homework cares about (for example, separating projection cost from attention cost, or identifying which terms dominate for large sequence length versus large model dimension). These were not blatant hallucinations, but the explanations could be somewhat hand‑wavy while still sounding very confident. Overall, Grok 4.1 worked best as a conceptual tutor and a source of “first draft” derivations or explanations; for exact complexity counts, detailed implementation, and visualization‑dependent answers, I still needed to verify the details myself.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T11:53:21.884902+11:00",
    "reply_count": 0,
    "view_count": 12,
    "user": {
      "id": 1762463,
      "name": "Eric Jin",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7450048",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Grok 4",
      "depth_bucket": "medium",
      "word_count": 315
    },
    "file_refs": [
      {
        "filename": "Special Participation A.pdf",
        "position": 2194,
        "context": ""
      }
    ]
  },
  {
    "id": 7450012,
    "number": 817,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek on HW13 Non-coding",
    "document": "I used DeepSeek to answer the non-coding portions of Homework 13. DeepSeek successfully answers almost all questions on the first attempt, providing detailed derivations and correct results throughout.\n\nOn the DDPM/DDIM problems, DeepSeek accurately derives marginal and conditional distributions, handles telescoping products in the reverse process, and correctly approximates integrals in the Δt → 0 limit. However, it sometimes requires closer checking when justifying approximations—like Δt ≪ σ² ≪ 1—or when transitioning from discrete sums to integrals. The explanations are mathematically sound, but a human should verify the limit justifications to ensure full rigor.\n\nFor the DPO derivation, DeepSeek performs especially well. It cleanly derives the optimal policy form, shows how the partition function cancels in the Bradley-Terry model, correctly computes the gradient of the DPO loss, and extends the reasoning to the Plackett-Luce ranking setting. This section demonstrates DeepSeek’s strength in structured optimization theory and algebraic manipulation.\n\nOverall, DeepSeek shows strong proficiency in theoretical machine learning questions—especially those involving probability, optimization, and step-by-step derivation. Its responses are thorough, well-reasoned, and match the provided solution key closely. However, it tooks 383 seconds to analyze the problem and reasoning. The time it cost for reasoning is longer than other agents, like ChatGPT.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-11T11:49:09.020419+11:00",
    "reply_count": 0,
    "view_count": 44,
    "user": {
      "id": 932372,
      "name": "Shuwei Yang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7450012",
    "metrics": {
      "homework_id": "HW13",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 201
    },
    "file_refs": [
      {
        "filename": "Special Participation A Deepseek on HW13 Non-coding,pdf.pdf",
        "position": 1467,
        "context": "the problem and reasoning. The time it cost for reasoning is longer than other agents, like ChatGPT."
      }
    ]
  },
  {
    "id": 7449875,
    "number": 814,
    "course_id": 84647,
    "title": "Special Participation A: Grok 4.1 reasoning on HW09",
    "document": "For special participation A, I tested Grok 4.1 (beta) with reasoning capabilities on the non-coding question on hw09.\n\nThis is the pdf: \n\nin the pdf, I annotated and noted sections where Grok 4.1 seemed to get stuck and could not move forward without me giving it hints. One thing I noticed was that in questions where there were many short mcq parts, the model seemed to hallucinate on the correct answer if I posted them all at once - it would reason about the correct answer but end up choosing the wrong one when it boxed it which was very puzzling. Additionally, I noticed that most times that I corrected Grok, it would start reasoning and start searching the internet and pull up references that were barely relevant to the problem. This led to very long reasoning times and incorrect assumptions about the problem.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-11T11:27:41.781506+11:00",
    "reply_count": 0,
    "view_count": 18,
    "user": {
      "id": 954911,
      "name": "Rahul Bir",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7449875",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Grok 4",
      "depth_bucket": "medium",
      "word_count": 144
    },
    "file_refs": [
      {
        "filename": "grok 4.1 special participation a hw 11.pdf",
        "position": 822,
        "context": "sted Grok 4.1 (beta) with reasoning capabilities on the non-coding question on hw09.This is the pdf:"
      }
    ]
  },
  {
    "id": 7449252,
    "number": 806,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT-5.1 Pro on HW4 Non-coding",
    "document": "I used ChatGPT 5 - Pro on HW 4(all non-coding parts). \n\nSummary: It was quite good at one-shotting all problems, even with just one prompt - except a numerical problem, for which it (incorrectly) used python code to generate a matrix. I've attached my conversation with it here. Further, another small issue was the reasoning time - it took 20+ minutes to get a response from the Pro model on this problem set.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T09:54:18.4019+11:00",
    "reply_count": 0,
    "view_count": 106,
    "user": {
      "id": 1316471,
      "name": "Neel Kolhe",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7449252",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "GPT-5",
      "depth_bucket": "medium",
      "word_count": 73
    },
    "file_refs": [
      {
        "filename": "Question 1 calculations.pdf",
        "position": 54,
        "context": "I used ChatGPT 5 - Pro on HW 4(all non-coding parts)."
      }
    ]
  },
  {
    "id": 7447947,
    "number": 794,
    "course_id": 84647,
    "title": "Special Participation A: Perplexity Sonar on HW8",
    "document": "Executive Summary\n\nI used Perplexity’s default LLM (as of Dec 2025) \"Sonar\" on the non-coding parts of Homework set 8 (Problems 1, 3, and 4). It answered almost all subparts correctly on the first try, including derivations for the convolution kernel, impulse-response interpretation, complexity comparisons, and the self-supervised linear purification / ridge-attention math. The main failure was the first attempt at the diagonal-plus-low-rank (DPLR) SSM kernel (Problem 1(f)): the model produced an incorrect, hand-wavy spectral argument with invented “perturbative terms,” but when I explicitly challenged its confidence, it produced a clean, correct eigen-decomposition-based derivation. When pushed, it was able to self-diagnose its previous mistakes, explicitly list what it had gotten wrong, and then re-derive the result more rigorously. The interaction felt less like getting final answers from an oracle and more like supervising a strong but occasionally overconfident collaborator who needs spot checks on nontrivial linear-algebra structure. Overall, for this session the LLM “one-shot” most sub-questions, but required human skepticism and targeted follow-up prompts to avoid accepting a superficially impressive but wrong derivation.\n\nSetup and Strategy\n\nModel and interface: Perplexity AI’s \"Sonar\" chat model (browser UI). The transcript in Problem 1_ SSM Convolution Kernel.md is essentially the raw LLM output, plus a short human challenge around Part (f).\n\nScope: I asked it to solve the non-coding parts of:\n\nProblem 1: SSM convolution kernel (parts (a)–(f), including diagonal and DPLR structure and complexity).\n\nProblem 3: Self-supervised linear purification / regularized encoder (SVD-based derivation and hyperparameter selection).\n\nProblem 4: Ridge-attention questions (incremental means, ridge normal equations, complexity, Sherman–Morrison, causal ridge-attention, and whether gradients “flow” through λ).\n\nPrompting style:\n\nI usually gave it the math problem statement and let it answer in one shot.\n\nI only intervened when something looked clearly suspicious, and prompted it with inquiries to help it self-diagnose its mistakes.\n\nAfter catching an error, I explicitly asked it to critique its own previous answer and then provide a corrected derivation.\n\nThis setup was intentionally “light-touch”: I wanted to see how far it could get without heavy-handed scaffolding, and then whether human pressure could rescue it from a bad initial derivation.\n\nProblem 1: SSM Convolution Kernel\n\nParts (a)–(e): One-Shot Success\n\n(a) Convolution kernel derivation: It correctly unrolled the SSM, derived xk​=∑ℓ=0k−1​AℓBuk−1−ℓ​, substituted into yk​, and reindexed to identify K0​=D and Km​=CAm−1B for m≥1. This is the standard textbook result and matches the official structure.\n\n(b) Concrete scalar and 2D examples:\n\nScalar case: It made a reasonable assumption about D (the problem text didn’t specify δ), explicitly stated the assumption, and produced a decaying geometric kernel consistent with α=0.8.\n\n2D case: It correctly computed K1​,K2​,K3​ via CB,CAB,CA2B, carefully multiplying matrices and explaining that the decaying impulse response reflects stability.\n\nBehavior: Good at routine linear algebra, explicit about missing details / assumptions rather than hallucinating them as facts.\n\n(c) Convolution vs. direct recursion:\n\nIt correctly noted that direct recursion over time has a sequential dependency with O(L) critical path.\n\nFor convolution it emphasized tree-structured reductions leading to O(logL) critical path with enough parallelism.\n\nThe analysis leans more toward parallel runtime intuition than strict “total flops,” but is consistent and not obviously wrong.\n\n(d) Kernel computation with matrix powers:\n\nNaive strategy (O(Ln3)) and repeated squaring idea were both standard.\n\nIt mixed together “work” vs. “critical path” a bit, but the high-level story—use fast exponentiation and then apply CAm−1B—was sensible.\n\n(e) Diagonal A:\n\nIt correctly recognized that diagonal A turns matrix powers into elementwise powers and collapses Km​ to a scalar sum ∑i​Ci​λim−1​Bi​.\n\nComplexity dropped to O(Ln), which matches the intended diagonal-structure speedup.\n\nFor these parts, the LLM essentially “one-shot” the derivations and got both the algebra and the qualitative interpretations right. I did not have to intervene.\n\nPart (f): DPLR – Failure, Then Recovery Under Pressure\n\nThis was the main point where the model did not one-shot the problem.\n\nFirst attempt (incorrect / hand-wavy):\n\nIt tried to reason about A=I+pp⊤ via Sherman–Morrison and vague “perturbative terms,” writing expressions like\n\n(I+pp⊤)m=I+(emlog(1+p⊤p/2)−1)⋅(perturbative terms)\n\nand a generic Amv=v+p(p⊤v)⋅(correction term).\n\nThe derivation didn’t clearly exploit the spectrum (one eigenvalue 1+∥p∥2, others 1) and gave an inflated complexity claim O(n2+Ln) without a clean algorithm.\n\nMy reaction: This read like “mathy-sounding nonsense” rather than a precise argument. I explicitly asked if it was confident and pointed out that I believed the solution should look different.\n\nSecond attempt (corrected derivation after challenge):\n\nWhen challenged, the model restarted from the eigenstructure:\n\nShowed that A has eigenvalues λ1​=1+p⊤p along p and 1 on the orthogonal complement.\n\nConstructed a unit eigenvector u1​=p/∥p∥ and wrote the projector decomposition\nA=(I−u1​u1⊤​)+λ1​u1​u1⊤​.\n\nThen correctly derived\nAm=I+(λ1m​−1)u1​u1⊤​.\n\nFrom there, it derived\nKm​=CAm−1B=CB+(λ1m−1​−1)(Cu1​)(u1⊤​B),\ngiving a scalar-formula kernel and an essentially O(n+L) algorithm when precomputations are amortized.\n\nIt also wrote a self-critique section explicitly listing what it had gotten wrong in the earlier attempt (vague perturbative language, sloppy complexity, not fully reducing to scalars).\n\nBehavioral takeaway:\n\nWithout pressure, it was content to give a plausible-sounding but wrong derivation.\n\nWith a short critical prompt (“Are you confident? I think this is wrong.”), it was capable of producing a mathematically clean, structurally correct solution that is actually better than the first.\n\nThis suggests that for subtle linear-algebra structure questions, the model is highly capable but needs an active, skeptical user to avoid “beautiful hallucinations.”\n\nProblem 3: Self-Supervised Linear Purification\n\nHere the model handled every subpart in one shot and aligned well with the standard ridge / SVD story.\n\n(a)(i) & (a)(ii):\n\nCorrectly computed reconstruction and regularization losses for the identity vs. projection encoders on the given numeric matrix.\n\nSolved for the threshold λ where one encoder becomes worse: 2λ>0.001+λ⇒λ>0.001.\n\n(b)(i) & (b)(ii):\n\nDerived the optimal encoder in the SVD basis, W^=Udiag(σj2​/(σj2​+λ))U⊤, by changing coordinates and decomposing the loss by singular directions.\n\nThis matches the standard “ridge shrinkage in the principal-component basis” result.\n\n(c) Hyperparameter range:\n\nCorrectly turned the preservation/attenuation constraints into inequalities on σj2​/(σj2​+λ) and solved to get 1≤λ≤4, citing the right singular values.\n\nBehaviorally, this part shows the model is very good at structured linear regression derivations: it recognizes SVD as the right tool, carries through the algebra cleanly, and keeps track of norms and Frobenius invariance without hallucinated side-conditions.\n\nProblem 4: Ridge-Attention\n\nFor this problem, the model again answered each subpart in one shot and its reasoning matched what I would expect from a strong student solution.\n\n(a) Incremental mean: Gave the standard m′=(mn+xn+1​)/(n+1) (and the equivalent incremental form m′=m+(xn+1​−m)/(n+1)).\n\n(b) Ridge normal equations: Correctly decomposed A⊤A+λI as λI+∑xi​xi⊤​ and A⊤y=∑xi​yi​.\n\n(c) Non-causal ridge-attention complexity: Focused on forming and inverting K⊤K+λI and obtained O(nd2) leading term, which is the standard complexity.\n\n(d) Gradient flow: Identified that gradients propagate through keys, queries, and values, but not typically through λ in standard setups.\n\n(e) Sherman–Morrison cost: Broke down the matrix–vector and rank-1 updates to get O(d2) per update, which is correct.\n\n(f) Causal ridge-attention: Wrote a plausible online algorithm using Sherman–Morrison to update the inverse and maintain a W matrix, with per-step O(d2) cost and total O(nd2). The structure matches the intended dynamic-programming intuition.\n\n(g) Attention weights: Expressed the weights via the ridge coefficients and contrasted them with softmax attention (unconstrained, can be negative, don’t sum to 1).\n\nExtra discussion: “Can gradients flow through λ?”\n\nHere the model went beyond the likely intended scope of the homework and gave a long answer splitting “mathematically yes, practically no.”\n\nMathematically, it argued (correctly) that autograd can differentiate through matrix inverses via implicit differentiation, and even wrote the derivative formula.\n\nPractically, it concluded that we shouldn’t treat λ as a learnable parameter for stability and regularization reasons, and guessed that the problem likely expects “no” as the conceptual answer.\n\nThis is a case where the LLM is technically correct but may overcomplicate what the assignment likely wanted (“λ is a hyperparameter, not a learned parameter”).\n\nThis section illustrates a pattern: the model is capable of sophisticated meta-reasoning, but it may miscalibrate what level of nuance is appropriate for a homework answer vs. a research discussion.\n\nTakeaways and Recommendations\n\nAs a homework aid:\n\nThis LLM is very effective at deriving standard linear-algebra results and giving clear, step-by-step explanations.\n\nFor most subparts, I could have copied the answers with only minimal editing.\n\nHowever, treating it as an infallible oracle would have led me to accept a wrong DPLR derivation, which is exactly the kind of subtle structural question that’s exam-relevant.\n\nBest practices for using it:\n\nUse it as a co-author for derivations: let it propose a solution, but then read critically and ask “Does this really use the structure the problem is hinting at?”\n\nFor tricky structural or complexity questions, plan to challenge its first answer and ask it to re-derive more cleanly or justify every step.\n\nConsider asking it to explicitly list what assumptions it is making and what might fail.\n\nBottom line:\n\nIn this interaction, the LLM could essentially solve all non-coding parts of the assignment with high accuracy, but it still needed human mathematical judgment to catch a key mistake and to calibrate how much nuance is appropriate for a course setting.\n\nIf used thoughtfully and skeptically, it is a very strong tool for both checking work and deepening understanding; if used uncritically, it can quietly introduce subtle but important errors in exactly the kinds of problems that are hardest to catch by inspection.\n\nSee the file attached for my conversation",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T06:53:41.589004+11:00",
    "reply_count": 0,
    "view_count": 24,
    "user": {
      "id": 1294638,
      "name": "Martin Alvarez-Kuglen",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7447947",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Perplexity",
      "depth_bucket": "high",
      "word_count": 1546
    },
    "file_refs": [
      {
        "filename": "Problem 1_ SSM Convolution Kernel.md",
        "position": 10895,
        "context": " kinds of problems that are hardest to catch by inspection.See the file attached for my conversation"
      }
    ]
  },
  {
    "id": 7447290,
    "number": 793,
    "course_id": 84647,
    "title": "Special Participation A -- Gemini Pro 3 Thinking on HW 10 , Arvind Kruthiventy",
    "document": "In this post, I use Gemini Pro 3 on the HW 10 to answer the non-coding portions which were two questions: one question on kernelized linear attention for efficient attention computation over long sequences and one about the FaceNet paper. In the first question, Gemini Pro successfully answers the questions related to deriving formulas in part a part 1 and part b in the first attempt but it struggles a little with the question about computing the computational cost. It appears to answer these questions with less accuracy than other questions and answers to these types of questions should be carefully checked. However, for the second question on the FaceNet paper, Gemini oneshots it and provides detailed and accurate responses to all the questions that match the provided solutions. Summarization and querying key details from dense articles and papers appears to be its strength because it generates answers fairly quickly and with high accuracy; questions that are a little bit more general or require more analysis it seems to provide more shallow responses than is desired, but overalls its responses are very good if you are using to summarize the key implementation details and contributions of a work. I noticed that Gemini had an easier time with the non-coding questions than the coding ones where it would take significantly longer and could still require significant iteration before the code matches the user's requests.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T05:27:11.380142+11:00",
    "reply_count": 0,
    "view_count": 33,
    "user": {
      "id": 1357108,
      "name": "Arvind Kruthiventy",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7447290",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 233
    },
    "file_refs": [
      {
        "filename": "Special Participation A -- Gemini Pro 3 Thinking on HW 10 , Arvind Kruthiventy.pdf",
        "position": 1440,
        "context": "ly longer and could still require significant iteration before the code matches the user's requests."
      }
    ]
  },
  {
    "id": 7446043,
    "number": 791,
    "course_id": 84647,
    "title": "Special Participation A: Claude Sonnet 4.5 on HW0",
    "document": "Conversation: https://claude.ai/share/dd45cf31-778b-4d1c-9096-a304ad8c8247 \n\nI used Claude 4.5 sonnet to solve problems 2-5 for HW0 written. \n\nSummary: \n\nClaude was able to answer all the questions fully and correctly all with one shot and basic non-engineered prompts. I was surprised it even generated plots for a question that required drawing plots correctly. \n\nSome cons: \n\nSome explanations were very lengthy compared to the actual solution.\n\nA few solutions used random theorems it considered to be basic/known background knowledge.\n\nSome answers were given in a different form than the solution. But I don't think this is a big issue, since specific forms for these were not specified.\n\nConsistent latex formatting/compilation issues.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-11T00:36:00.939454+11:00",
    "reply_count": 0,
    "view_count": 34,
    "user": {
      "id": 1288137,
      "name": "Justin Yang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7446043",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Claude Sonnet",
      "depth_bucket": "low",
      "word_count": 106
    },
    "file_refs": [
      {
        "filename": "special_participation_A_cs182_hw0_claude.pdf",
        "position": 742,
        "context": "e, since specific forms for these were not specified.Consistent latex formatting/compilation issues."
      }
    ]
  },
  {
    "id": 7445765,
    "number": 786,
    "course_id": 84647,
    "title": "Special Participation A: Mistral Le Chat on HW11 (Without Reasoning or Thinking Mode)",
    "document": "I worked through HW 11 Problems 1, 2, 5, and 6, using Mistral’s Le Chat model. After introducing the assignment by outlining the deep learning themes it would focus on (LoRA, soft prompting, transformer mechanics), I supplied the model with the necessary context for each problem, including variable definitions and any background assumptions needed (such as in Problem 2). From there, I prompted the model to solve each sub-question in sequence, always requesting brief, direct explanations and a concise walkthrough of intermediate steps.\n\nLe Chat consistently favored minimal “showing of work,” providing compact derivations rather than full reasoning chains or mid-steps in the calculations. Its answers tended to prioritize correctness and structure over interpretive commentary, rarely connecting its calculations back to broader concepts from the homework like transformer representations or low-rank representations. Despite this, the model was able to reliably one-shot the conceptual queries, the true/false tasks, and the simpler proofs without significant intervention on my end.\n\nWhere it struggled was in Problem 5, when the solution required numerical facts external to the prompt. In those cases, the model occasionally hallucinated specific statistics, such as the number of books scanned by the Library of Congress or Google Books. Despite this, the model nevertheless applied the correct methodology for these problems.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-10T19:46:16.815442+11:00",
    "reply_count": 0,
    "view_count": 24,
    "user": {
      "id": 618225,
      "name": "Akshaan Ahuja",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7445765",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "Mistral",
      "depth_bucket": "medium",
      "word_count": 210
    },
    "file_refs": [
      {
        "filename": "HW11-Mistral-Annotated.pdf",
        "position": 1438,
        "context": "ogle Books. Despite this, the model nevertheless applied the correct methodology for these problems."
      }
    ]
  },
  {
    "id": 7445493,
    "number": 784,
    "course_id": 84647,
    "title": "Special Participation A: Claude Sonnet 4.5 on Homework 4 (Written Problems)",
    "document": "I engaged Claude Sonnet 4.5 on all written portions of Homework 4 (Problems 1, 2, 3, 4, and 7) to evaluate its ability to solve deep learning theory problems. I provided screenshots of the questions from the original homework pdf one by one, to prevent the model from “forgetting” the later questions once we got further into the chat. This post documents my observations and an analysis of where the model succeeded and failed. Full annotated chat log is attached at the end of this post.\n\nThe model has strong conceptual understanding but struggles with notation conventions, sign errors in signal processing, and tracking how multiple scaling factors interact.\n\nDetailed Findings by Problem\n\nProblem 1: Newton-Schulz Runtime (with minor prompting)\n\nWhat happened:\n\nThe model correctly identified the two dominant matrix multiplications and their complexity\n\nIssue: Defaulted to Big-O notation instead of the requested cmnp format\n\nRequired two prompts: (1) \"use cmnp format\" and (2) \"you're missing the constant c\"\n\nClaude defaults to familiar conventions. Explicit format requests may need reinforcement.\n\nProblem 2: MuP at the Unit Scale\n\nThe model one-shotted most parts of this problem (a, b, c, f). Part d required a minor fix, while part e was a major struggle. Part g was initially wrong, but was fixed correctly after fixing part e.\n\nProblem 2(e) Deep Dive, the hardest question:\n\nThis asked for the learning rate α for Muon-style orthogonalization where ΔW = α·UV^T.\n\nAttempt 1: Model got α = √n_out (missing √n_in factor)\n\nAttempt 2: After prompting \"your answer is incorrect,\" still got √n_out\n\nAttempt 3: I asked \"where are you losing the √n_in?\", which caused the model to try various approaches, but it still couldn't find where it was missing the answer\n\nFinal resolution: I showed the solution snippet explaining how c = 1/√d_in adds a factor to the spectral norm. Only then did the model understand and arrive at α = √(n_out · n_in)\n\nWhy this was hard: The model struggled to track how the forward-pass constant c interacts with the spectral norm constraint. This requires reasoning about parameterization across the forward and backward passes simultaneously.\n\nProblem 3: Convolution as FIR Filter\n\nThe model one-shotted only part b here. Part a required a minor fix, while part c had a sign error and part d also had an inherited error from part c. However part d wasn’t fixed immediately when the issue with part c was fixed.\n\nThe Convolution vs. Correlation Confusion:\n\nThis was a classic signal processing error. The problem explicitly states \"we will follow the definition in equation 3\" (true convolution with flip-and-drag), but the model computed correlation (no flip).\n\nInitial answer for part (c): Matrix of all +40\n\nCorrect answer: Matrix of all -40\n\nWhen I pointed out the sign error, the model realized it needed to flip the kernel h to h_flipped before computing. This error then propagated to part (d), requiring multiple corrections for the padded boundary cases.\n\nLLMs trained heavily on ML code (where \"convolution\" usually means correlation) may default to the wrong convention even when the mathematical definition is specified.\n\nProblem 4: CNN Feature Dimensions\n\nAlmost all of the subparts were oneshotted here, while for the last subpart it flipped its answers.\n\nProblem 4(f) Error:\n\nGiven x1 (horizontal edge) -> g1 = [0.8, 0, 0]^T and x2 (vertical edge) -> g2 = [0, 0.8, 0]^T, the model was asked to find g3 and g4 for shifted versions.\n\nModel's initial answer: g3 = [0.8, 0, 0]^T, g4 = [0, 0.8, 0]^T\n\nCorrect answer: g3 = [0, 0.8, 0]^T, g4 = [0.8, 0, 0]^T\n\nThe model confused which image (x3 or x4) had which edge type. After I pointed out the flip, it correctly explained that x3 has a vertical edge (like x2) and x4 has a horizontal edge (like x1).\n\nProblem 7: Weights and Gradients in CNNs\n\nThe model one-shotted all of the answers for this question! This was the model's strongest section. All parts were correct on the first attempt.\n\nCommon Failures\n\nNotation/Format Defaults: Model uses Big-O when cmnp requested, uses inequalities when equalities expected\n\nConvolution vs. Correlation: Classic signal processing confusion; defaults to ML convention (correlation) even when math definition specifies convolution\n\nMulti-Factor Scaling: Struggles to track how multiple constants (like forward-pass c and spectral norms) interact across equations\n\nSpatial/Visual Confusion: Difficulty with edge orientations and which dimension corresponds to \"horizontal\" vs \"vertical\"\n\nArithmetic in Boundary Cases: Padded convolution computations were error-prone\n\nConclusions\n\nCan Claude Sonnet 4.5 solve this homework? Yes, but not reliably without human guidance.\n\nWhat's it good for?\n\nExplaining concepts and deriving formulas\n\nStandard calculations (CNN dimensions, parameter counts)\n\nGenerating first-draft solutions to check\n\nWhat requires human oversight?\n\nVerifying numerical computations\n\nCatching convention errors (convolution vs. correlation)\n\nProblems requiring reasoning about multiple interacting factors\n\nHere is the annotated version of my chat log with Claude, all of my annotations are in red. I used claude exporter to get the pdf version of the chat (claude itself does not provide a method to get the pdf).",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-10T18:02:07.539427+11:00",
    "reply_count": 0,
    "view_count": 25,
    "user": {
      "id": 638316,
      "name": "Elizabeth Weaver",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7445493",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "Claude Sonnet",
      "depth_bucket": "high",
      "word_count": 831
    },
    "file_refs": [
      {
        "filename": "Claude-Deep learning homework solutions.pdf",
        "position": 5266,
        "context": "xporter to get the pdf version of the chat (claude itself does not provide a method to get the pdf)."
      }
    ]
  },
  {
    "id": 7445419,
    "number": 782,
    "course_id": 84647,
    "title": "Special Participation A: Gemini Flash on HW12",
    "document": "Gemini Flash demonstrated a perfect one-shot performance on the non-coding written portions of Homework 12. The model exhibited strong domain knowledge about deep learning, specifically in Transformer initialization stability, KL Divergence, and Variational Information Bottlenecks.\n\nKey Observations:\n\nAccuracy: The model answered all sub-parts of the three questions correctly, matching the logic found in the reference solutions.\n\nReasoning Capability: Gemini went beyond simple answers by providing mathematical justifications. For instance, in Q1, it explicitly reasoned about the expected squared norm of the embeddings, and in Q3, it correctly interpreted the trade-off between task loss and regularization strength to analyze the validation error curve.\n\nVisual Interpretation: The model successfully interpreted the unlabelled plots in Q2 (distribution shapes) and Q3 (scatter plots of latent spaces), correctly mapping visual characteristics (variance, clustering) to hyperparameters.\n\nHallucinations: There were no hallucinations or misunderstoods found in this output. Gemini adhered strictly to standard definitions and deep learning theory.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-10T17:35:29.671404+11:00",
    "reply_count": 0,
    "view_count": 42,
    "user": {
      "id": 1751537,
      "name": "Jincheng Ou",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7445419",
    "metrics": {
      "homework_id": "HW12",
      "model_name": "Gemini Flash",
      "depth_bucket": "medium",
      "word_count": 151
    },
    "file_refs": [
      {
        "filename": "Special_Participation_A__Gemini_Flash_on_HW12.pdf",
        "position": 1154,
        "context": "oods found in this output. Gemini adhered strictly to standard definitions and deep learning theory."
      }
    ]
  },
  {
    "id": 7445083,
    "number": 777,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT-5 (Regular) on Homework 12",
    "document": "Done as reflected on the deconflict sheet.\n\nNote that I did Questions (1) and 5(c) on my other Special Participation B post, because I treated them as coding Questions. For this post, I do questions 2, 3, and 5(a)-(b).\n\nI post the annotated chat below. Here is the link to the GPT chat: https://chatgpt.com/share/6938f111-c4b4-800d-90fd-000f7b0fa644\n\nQuestion (2): ChatGPT solved this question without too much difficulty.\n\nQuestion (3): ChatGPT actually encountered some difficulties on this question. First off, on 3(d)(ii), it did not read the figures correctly; I had to provide it with a screenshot of Figure 4 for the reasoning to work. Next, for 3(a), it provided me with a working primitive diagram, but also code for an Overleaf diagram. Despite several fixation attempts, the code never quiet worked, but it technically did \"solve\" 3(a) with the original diagram, which I attached below.\n\nQuestion (5)(a)-(b): ChatGPT actually failed to solve this one at first, having \"forgotten\" Homework 12's full contents. I reattached the PDF and proceeded, and it worked.\n\nHere was the 3(a) ASCII diagram and annotated chatlog PDF:\n\n\n\nEDIT: Attached a better PDF.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-10T16:07:04.785698+11:00",
    "reply_count": 0,
    "view_count": 31,
    "user": {
      "id": 647818,
      "name": "Evan Davis",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7445083",
    "metrics": {
      "homework_id": "HW12",
      "model_name": "GPT-5",
      "depth_bucket": "medium",
      "word_count": 183
    },
    "file_refs": [
      {
        "filename": "special_participation.pdf",
        "position": 1130,
        "context": "the 3(a) ASCII diagram and annotated chatlog PDF:"
      }
    ]
  },
  {
    "id": 7444860,
    "number": 773,
    "course_id": 84647,
    "title": "Special Participation A: Opus 4.5 on HW11",
    "document": "In this chat, I used Claude Opus 4.5 in its regular reasoning mode to solve the written questions on Homework 11. I first started by prepping the model with this prompt:\n\"Hello Mr. Claude. Today, I will be giving you problems from my deep learning class by submitting screenshots of the problems. I want you to answer these as they come. Do not skip any derivation steps, and clearly output the answer. If there are multiple subparts, clearly answer all the subparts. I want you to clearly rewrite the question and your reasoning and solution for each problem in one single markdown file. Are you ready?\".\n\nI gave the model screenshots of the problems and had it one-shot the answer. By putting all the model responses into one markdown file, it was easy for me to read and to compare against the answer key. The model was consistently arriving at the correct solution. While I expected this for the conceptual questions, I was surprised the level of depth it had when explaining the matrix math for question 2. However, for some questions the model made small mistakes such as incorrect assumptions about the GPU.\n\nUsually when I use LLMs to assist me with homework questions, I tend to give the LLM smaller parts of the question in order, then discuss with the LLM to get hints and solutions, and make sure it makes sense to me. This allows me to both learn the content quicker rather than just being completely lost and also verify the LLM. However, when I gave the LLM the entire problem, it seemed to actually do better than when I would previously give it small parts of the question and continuously ask questions.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-10T15:31:08.924907+11:00",
    "reply_count": 0,
    "view_count": 16,
    "user": {
      "id": 961766,
      "name": "Rohan Gopalam",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7444860",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "Claude Opus",
      "depth_bucket": "medium",
      "word_count": 287
    },
    "file_refs": [
      {
        "filename": "Special Participation A_HW11.pdf",
        "position": 1621,
        "context": "ter than when I would previously give it small parts of the question and continuously ask questions."
      }
    ]
  },
  {
    "id": 7444253,
    "number": 766,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT 5.1 Extended Thinking Time on HW 6",
    "document": "Executive Summary:\n\nI used ChatGPT 5.1 Extended Think to solve HW 6 Questions 2 and 3 (The Non-Coding Questions). To test the advanced reasoning and memory capabilities of the thinking model, I submitted screenshots of entire questions instead of problems one subpart at a time, prompting the model minimally with the prompt:\n\n\"Please read these screenshots in detail. Your job is to write solutions to each part of this question. Make sure to delineate which subpart you are working on, and make sure to show your work adequately\"\n\nEven with minimal prompting, the model performed highly accurately, essentially one-shotting almost every subpart with no need for follow-up correction. The few issues I found were small algebraic/mechanical slips rather than major conceptual misunderstandings. I did not observe any blatant hallucinations (e.g., made-up definitions or theorems), and when it diverged from the official solution at all its answers were still logically consistent.\n\nThere were two main drawbacks:\n\nThe model took a long time to respond, thinking for over 7 and a half minutes on each of the two big question blocks.\n\nAs many other students have noted, it tended to produce overly verbose solutions where much shorter answers would have sufficed, and it sometimes got bogged down in minor details.\n\nOverall, I would most accurately compare it to a TA in the sense that it generally provides correct reasoning and explanations, but can go on small side tangents, be more long-winded than necessary, and occasionally make small algebraic or notational mistakes that still require human checking.\n\nA link to my detailed annotations can be found here: \n\n\n\nAnd a link to the raw conversation can be found here: https://chatgpt.com/share/6938cd49-4f50-800d-9b68-0d641aa7e76a",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-10T13:43:35.193488+11:00",
    "reply_count": 0,
    "view_count": 18,
    "user": {
      "id": 622698,
      "name": "Ethan Stone",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7444253",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "GPT-5.1",
      "depth_bucket": "medium",
      "word_count": 276
    },
    "file_refs": [
      {
        "filename": "ChatGPT_HW6_Annotated.pdf",
        "position": 1663,
        "context": "onal mistakes that still require human checking.A link to my detailed annotations can be found here:"
      }
    ]
  },
  {
    "id": 7444212,
    "number": 765,
    "course_id": 84647,
    "title": "Special Participation A: Qwen on HW13",
    "document": "I use Qwen to solve HW13 written part in this special participation A.\n\nQwen's accuracy really impressed me. All questions, including requiring proofs (e.g., induction in diffusion models), derivations (e.g., DPO gradients), or conceptual explanations (e.g., intractability of partition functions), were answered correctly and comprehensively in a single attempt, with no need for clarification or correction. The responses included rigorous mathematical detail, proper notation, and logical step-by-step reasoning.\n\nThere is no factual inaccuracies, fabricated citations, or conceptual errors were observed. For instance:\n\nThe derivation of the optimal policy in DPO correctly recovered the Gibbs form and explained the cancellation of the partition function in Bradley-Terry and Plackett-Luce models.\n\nThe diffusion process proofs aligned exactly with standard results from the literature (e.g., Ho et al., 2020).\n\nInterpretations (e.g., gradient weighting in DPO reflecting model uncertainty) were insightful and technically sound.\n\nDespite minimal input context (only image placeholders uploaded), the model inferred the likely content (standard theoretical ML problems) and provided complete, self-contained solutions, demonstrating strong contextual awareness and domain knowledge. Responses were well-organized, labeled by sub-question, and included boxed final answers where appropriate, enhancing readability and correctness verification.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-10T13:38:45.270768+11:00",
    "reply_count": 0,
    "view_count": 13,
    "user": {
      "id": 1308097,
      "name": "Peidong Zhang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7444212",
    "metrics": {
      "homework_id": "HW13",
      "model_name": "Qwen",
      "depth_bucket": "medium",
      "word_count": 187
    },
    "file_refs": [
      {
        "filename": "CS282 Special Participation A_ Qwen on HW13.pdf",
        "position": 1447,
        "context": " included boxed final answers where appropriate, enhancing readability and correctness verification."
      }
    ]
  },
  {
    "id": 7443651,
    "number": 762,
    "course_id": 84647,
    "title": "[SPOILER ALERT] Special Participation A: Claude on HW13",
    "document": "In this chat, I used Claude 4.5 Sonnet to answer HW13. My prompt is included in the pdf. Overall Claude was able to do pretty well, answering all subproblems fully correctly (in my opinion). This was all one single try. Claude is able to do the first parts of each question very well and provides a very good explanation for each. I'm overall extremely impressed with some of the more complicated problems, such as in part 2b where it shows most of the (lengthy) derivation by minimizing KL. There are some small parts where it skipped intermediate algebraic derivation steps like in 2b for the Lagrange multipliers but overall the performance was quite strong.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-10T12:10:31.430772+11:00",
    "reply_count": 0,
    "view_count": 41,
    "user": {
      "id": 961889,
      "name": "Andy Peng",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7443651",
    "metrics": {
      "homework_id": "HW13",
      "model_name": "Claude Sonnet",
      "depth_bucket": "medium",
      "word_count": 114
    },
    "file_refs": [
      {
        "filename": "Claude HW13 output.pdf",
        "position": 661,
        "context": ""
      }
    ]
  },
  {
    "id": 7440205,
    "number": 755,
    "course_id": 84647,
    "title": "Special Participation A: Qwen on HW 6",
    "document": "For Special Participation A, I used the Qwen chat website with the Qwen-Max model.\n\nHere are some highlights:\n\nQwen often uses problem re-stating to find viable solutions to the problem. It establishes the context for the problem by looking at the homework problem, taking into account external sources that are being referenced, and if necessary, feedback, specifically whether to expand or refine its reasoning process.\n\nWhen solving the problem Qwen is able to balance a mixture of possible perspectives, alternately considering the homework author's intent as mentioned in the homework, standard deep learning or linear algebra conventions--- like matrix indexing---, or the user's intentions when provided user feedback. While reasoning line by line to find the most likely way to move forward, the model also occasionally stops and assesses the most logical thing to do, doing brief sanity checks.\n\nFinally, Qwen is unable to read images and so completely hallucinates an answer to question 3) b) (iii). This process is interesting because it highlights Qwen's bias towards providing conservative answers that follow the pre-existing corpus of homeworks/literature, and then towards solving something on its own. Interestingly enough, Qwen never asked for further elaboration or for help even when told that it got answers completely wrong.\n\nI've attached the annotated transcript. Here is the official link to the conversation.\n\nThanks!",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-10T04:47:08.696964+11:00",
    "reply_count": 0,
    "view_count": 20,
    "user": {
      "id": 901565,
      "name": "Micah Mok",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7440205",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Qwen",
      "depth_bucket": "medium",
      "word_count": 219
    },
    "file_refs": [
      {
        "filename": "A_ Qwen HW6 Written.pdf",
        "position": 1434,
        "context": "k/>I've attached the annotated transcript. Here is the official link to the conversation."
      }
    ]
  },
  {
    "id": 7436873,
    "number": 738,
    "course_id": 84647,
    "title": "Special Participation A: Claude Sonnet 4.5 on HW10",
    "document": "I experimented with Claude Sonnet 4.5 on the written portions of HW10, specifically problems 1 and 5. Overall, even though this is the basic version of Claude, it was able to answer all of my questions correctly and in detail, often providing additional mathematical reasoning and conclusions that I didn’t explicitly ask for. The only materials I provided were the initial system prompt, the PDF of the homework problems, and the PDF of the FaceNet paper. Claude Sonnet 4.5 solved all of the problems on the first attempt, and although some of its explanations were a bit verbose, its answers were consistently correct and well-grounded.\n\nHere is the chat:\n\nhttps://claude.ai/share/680c3bd2-67d6-46ee-bf22-b0de0537dde3\n\nHere is the annotated transcript:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-09T12:48:53.512453+11:00",
    "reply_count": 0,
    "view_count": 30,
    "user": {
      "id": 607285,
      "name": "Swetha Rajkumar",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7436873",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "Claude Sonnet",
      "depth_bucket": "medium",
      "word_count": 115
    },
    "file_refs": [
      {
        "filename": "Swetha Rajkumar CS182 Special Participation A.pdf",
        "position": 754,
        "context": "e chat:https://claude.ai/share/680c3bd2-67d6-46ee-bf22-b0de0537dde3Here is the annotated transcript:"
      }
    ]
  },
  {
    "id": 7433942,
    "number": 731,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3.0 Pro on Homework 13",
    "document": "Special Participation A: Gemini 3.0 Pro on Homework 13\n\nFor this assignment, I evaluated how well Gemini 3.0 Pro can handle the theoretical, non-coding derivations of CS182 Homework 13. I approached the Direct Preference Optimization (DPO) problem set step-by-step, initiating the session with a specific persona prompt to establish a \"technical partner\" role, and then guided the model through the derivation without providing the final answers myself. My main goal was to see (1) its OCR accuracy on dense mathematical problem sets, (2) its ability to perform rigorous algebraic manipulations, and (3) the clarity of its conceptual explanations.\n\nOverall, Gemini performed exceptionally well. It correctly identified the context from the uploaded images and produced clean, structured derivations for the entire pipeline (Q2 Parts a through g) with minimal correction required.\n\nIt was especially reliable on:\n\nInterpreting the standard RLHF objective and KL constraints,\n\nExecuting the algebraic \"cancellation trick\" to eliminate the intractable partition function Z(x),\n\nDeriving the gradient of the DPO loss and interpreting the weighting mechanism, and\n\nExtending the logic from pairwise comparisons to listwise rankings (Plackett-Luce model).\n\nIn terms of interaction, the model was:\n\nconsistent in its LaTeX formatting and structure, and\n\nhighly effective at explaining the \"why\" behind each step, acting as a true mentor rather than just a calculator.\n\nOverall, based on this evaluation, Gemini 3.0 Pro is capable of solving the complex theoretical questions in CS182 with high accuracy and interpretability. \n\nFor further information, please see the annotated logs.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-09T06:59:49.352962+11:00",
    "reply_count": 0,
    "view_count": 16,
    "user": {
      "id": 1751524,
      "name": "Tom Chen",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7433942",
    "metrics": {
      "homework_id": "HW13",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 244
    },
    "file_refs": [
      {
        "filename": "Special Participation A by Tom.pdf",
        "position": 1675,
        "context": "182 with high accuracy and interpretability. For further information, please see the annotated logs."
      }
    ]
  },
  {
    "id": 7431425,
    "number": 725,
    "course_id": 84647,
    "title": "Special Participation A: Claude Opus 4.5 (Extended Thinking) on HW7",
    "document": "I use Claude Opus 4.5 (Extended Thinking) to solve all of the non-coding problems on homework 7. In the attached file, I include the executive summary and annotations of the complete chat logs.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T21:39:59.311026+11:00",
    "reply_count": 0,
    "view_count": 33,
    "user": {
      "id": 904878,
      "name": "Sufjan Fana",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7431425",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "Claude Opus",
      "depth_bucket": "low",
      "word_count": 33
    },
    "file_refs": [
      {
        "filename": "Participation_A.pdf",
        "position": 193,
        "context": " 7. In the attached file, I include the executive summary and annotations of the complete chat logs."
      }
    ]
  },
  {
    "id": 7431312,
    "number": 723,
    "course_id": 84647,
    "title": "Special Participation A: Testing Claude Opus 4.5 (Extended Thinking) on HW6",
    "document": "Overall, Claude performed well, 13/14 questions correct. It was strong at mathematical derivations(path counting induction proof), residual connections, and Newton Schulz convergence analysis. It provided a step by step reasoning which was clean and the explanations were well structured, and easy to follow. \n\nThe one failure was Q3c(iii), where Claude had to write update equations for specific nodes in a graph. It confidently hallucinated the neighborhood structure, as I think it couldn't see the figure. Wrong neighbors led to wrong equations. \n\nPublic Chat Link",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T20:04:24.011323+11:00",
    "reply_count": 0,
    "view_count": 26,
    "user": {
      "id": 508873,
      "name": "Manan Roongta",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7431312",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Claude Opus",
      "depth_bucket": "medium",
      "word_count": 84
    },
    "file_refs": [
      {
        "filename": "Opus 4.5 Extended Thinking on HW6.pdf",
        "position": 568,
        "context": "it couldn't see the figure. Wrong neighbors led to wrong equations. Public Chat Link"
      }
    ]
  },
  {
    "id": 7431042,
    "number": 714,
    "course_id": 84647,
    "title": "Special Participation A: Gemini Pro 3 on Homework 2",
    "document": "Gemini easily one-shotted all the homework problems with ease. Explanations were correct, concise, and directly matched the answer key. Even in places where the answer key did not explain (Q5 distributed training), Gemini had a clear way to arrive at the answers. \n\nOne particular interesting observation was with part b of Question 1. Originally, the question here had a typo (the objective function did not square the infinity norm). Even then, Gemini identified that the problem is unconstrained and suggested that a particular solution exists with a squared penalty instead.",
    "category": "Admin",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T18:13:46.596179+11:00",
    "reply_count": 0,
    "view_count": 17,
    "user": {
      "id": 1316377,
      "name": "Aryan Bansal",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7431042",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Gemini Pro",
      "depth_bucket": "low",
      "word_count": 90
    },
    "file_refs": [
      {
        "filename": "gemini_3_pro_hw2.pdf",
        "position": 578,
        "context": "lem is unconstrained and suggested that a particular solution exists with a squared penalty instead."
      }
    ]
  },
  {
    "id": 7430749,
    "number": 704,
    "course_id": 84647,
    "title": "Special Participation A: GPT 5 Thinking on HW 10",
    "document": "I used ChatGPT 5 (Thinking) on HW 10 (all non-coding parts).\n\nHere is the conversation log. Here is the annotated conversation.\n\nSummary:  Across my interaction, ChatGPT was able to one-shot solve each of the problems, and it consistently interpreted questions without requiring clarification. The model produced correct derivations for the kernelized linear attention problem, including Gaussian kernel decomposition and Random Fourier Feature approximation. Its answers showed no hallucinations and showed strong and reliable reasoning. For the second problem, ChatGPT also answered all questions on the first attempt. I had to manually enable web search to that it could access the paper and the blog. It then correctly provided citations and was able to read and synthesize the paper/blog. I liked how it listed all the assumptions it made which can help in cases where it is incorrect. I generally also felt that the solutions it provided were intuitive but also had good technical rigor.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-08T16:56:49.895781+11:00",
    "reply_count": 0,
    "view_count": 33,
    "user": {
      "id": 961865,
      "name": "Sanjay Adhikesaven",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7430749",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "GPT-5",
      "depth_bucket": "medium",
      "word_count": 154
    },
    "file_refs": null
  },
  {
    "id": 7429651,
    "number": 686,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3 Pro on HW03",
    "document": "For Participation A I used Gemini 3 Pro (Thinking with 3 Pro) on the non-coding parts of HW3 (Problems 1, 3, 4, 5). I gave it the full problem statements (often as screenshots) and asked it to work through each sub-question with explicit derivation steps, but no code. Most of the probability / calculus / optimization pieces came out correct on the first attempt; my role was mainly to sanity-check the algebra and occasionally nudge it to be more explicit about intermediate steps or constants.\n\nA few observations from the interaction:\n\nStrengths.\n\nIt handled the Gaussian policy-gradient and Maximal Update Parameterization questions well, with clean use of the log-derivative trick, reparameterization, and scaling arguments.\n\nOn the reading-based parts (e.g., interpreting figures/tables from the muP paper), once I pointed it to the specific figure or row, it gave focused, accurate summaries instead of generic “paper reviews.”\n\nWhen my prompt was ambiguous or I referenced the wrong part of a problem, it tended not to invent details; it either stayed generic or adjusted once I clarified, which kept hallucinations relatively low.\n\nWeaknesses / failure modes.\n\nFor implementation-style questions (tensor rematerialization forward counts), it initially chose a reasonable but wrong cost model and confidently overcounted. Only after I explicitly restated what should and shouldn’t be counted did it converge to the official answers (e.g., 20 forward ops, 10 loads).\n\nExplanations are concise and formula-heavy; this is good for following the math, but it often skips intuition or broader context unless explicitly requested.\n\nOverall, Gemini 3 Pro was quite usable as a step-by-step assistant for the theory parts of the homework, but still needed a human in the loop to pin down problem semantics and verify that its interpretation of “what we’re measuring” matched the assignment.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T13:58:58.922799+11:00",
    "reply_count": 0,
    "view_count": 19,
    "user": {
      "id": 647712,
      "name": "John Wang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7429651",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 292
    },
    "file_refs": [
      {
        "filename": "Participation A.pdf",
        "position": 1891,
        "context": ""
      }
    ]
  },
  {
    "id": 7429462,
    "number": 681,
    "course_id": 84647,
    "title": "Special Participation A: Hw 6 with Grok",
    "document": "For Special Participation A, I used Grok on the written portion of HW 6.\n\nHere is the link to my chat: \n\nQ2\n\nQ3\n\nAnalysis\n\nQ2\n\nOverall Assessment\n\nThe AI assistant provided a mathematically accurate and conceptually sound response. It correctly identified the core relationship between matrix powers and graph walks and handled the GNN architecture questions well. However, its structure—specifically the separation between \"Key Points\" and a long narrative essay—made it less direct and harder to follow than the standard \"Question-Answer\" format found in the satisfactory response.\n\nStrengths (Pros)\n\n1. Strong \"Executive Summary\" (Key Points) The AI started with a \"Key Points\" section. This is a distinct advantage for users who want a high-level overview without wading through the math immediately. It effectively summarized the main takeaways (e.g., $A^k$ counts walks, Max aggregation = Reachability).\n\n2. Mathematical Correctness (Parts a, b, c) The AI correctly handled the linear algebra and graph theory components.\n\nInduction Proof: The proof for Part (b) was concise and logically sound, correctly identifying the base case and the inductive step involving neighbor summation.\n\nMatrix Interpretation: It correctly identified the output as related to $A^k$ and the update function as the identity.\n\n3. Advanced Domain Knowledge (Parts g, h)\n\nFeature Propagation: In Part (g), the AI mentioned \"minimizing Dirichlet energy\" and \"feature propagation.\" This is a more advanced, technically specific answer than simply saying \"message passing.\" It shows a deeper knowledge of how modern GNNs handle missing data.\n\nComplexity Analysis: In Part (h), the AI correctly broke down the computational complexity, noting the difference between sparse ($O(|E|)$) and dense ($O(|V|^2)$) graphs.\n\n4. Citations The AI included a list of citations/sources at the bottom. While this is a standard feature of search-enabled AIs, it adds credibility to the mathematical definitions used in the answer.\n\nWeaknesses (Cons)\n\n1. Structural Redundancy and Poor Formatting This is the biggest weakness.\n\nSplit Information: The AI answers the questions in short bullet points first, but then repeats the entire explanation in a long, narrative essay at the bottom (\"Graph neural networks represent...\").\n\nHard to Parse: If a user wants to check the answer for specifically Part (e), they have to look at the short bullet point, and then scan through the long text block at the bottom to see if there is more detail. The \"Satisfactory Answer\" is much better because it groups the full explanation directly under the question heading.\n\n2. Ambiguity in Notation (Part a) In Part (a), the AI writes: \"The output... is the j-th row of $A^k$ (or equivalently, the j-th column if considering column vectors).\" While mathematically true that $A$ is symmetric for undirected graphs, in the context of a specifically defined linear network with one-hot inputs, there is usually one specific correct orientation based on how the multiplication is defined ($Ax$ vs $xA$). The hedging (\"row or column\") adds unnecessary cognitive load compared to the specific $A^k e_j$ (column) derivation in the Satisfactory Answer.\n\n3. Weak Analogy in the Table (Part f) For \"Image flip data augmentation,\" the AI suggests \"Graph permutation or subgraph sampling.\"\n\nCritique: Flipping an image creates a new data point. Permuting a graph (renumbering nodes) results in the exact same object for a GNN because GNNs are permutation invariant. Therefore, permutation is not really \"data augmentation\" in the same way an image flip is. The Satisfactory Answer correctly notes that for general graphs, there is no direct equivalent, or suggests geometric rotations for geometric graphs, which is a more precise distinction.\n\n4. Lack of Operational Detail (Part e) In Part (e-ii) (making the prediction), the AI says: \"Final edge embeddings fed to an MLP...\" It omits the specific mechanism of selection mentioned in the Satisfactory Answer: \"argmax.\" Since the question asks how to predict which bond breaks first, explicitly mentioning the selection step (argmax over edge scores) is crucial for a complete answer.\n\nConclusion\n\nThe AI's answer is correct and trustworthy, but poorly organized. It mimics a blog post or a research summary rather than a homework solution. The \"Satisfactory Answer\" is superior in terms of pedagogical clarity, as it addresses each sub-problem comprehensively in order, whereas the AI forces the user to piece together the answer from a summary and a dense essay.\n\nQ3\n\nOverall Assessment\n\nThe AI assistant provided an excellent and completely accurate response. It correctly solved every part of the problem, including the conceptual questions about GNN validity, the mathematical derivation of the loss function, and the structural analysis of the specific update rule. The reasoning was sound throughout, and it successfully interpreted the graph structure provided in the DOT format.\n\nStrengths\n\n1. Perfect Accuracy on Graph Topology (DOT Parsing) One of the trickier parts of this prompt is interpreting the graph structure defined in the graph G { ... } block without a visual image.\n\nThe AI correctly identified the neighbors of Node 2 as ${1, 4, 5}$ and Node 3 as ${5}$.\n\nThis demonstrates strong logical reasoning capabilities to parse the edge list (1 -- 2, 2 -- 5, etc.) correctly.\n\n2. Detailed and Correct Dimensional Analysis In Part (c)(ii), the AI provided a step-by-step breakdown of the matrix dimensions.\n\nInstead of just guessing the answer ($d \\times k$), it traced the vector shape through the inner product ($W_2$), the nonlinearity ($\\tanh$), and the aggregation.\n\nThis explicit chain of thought makes the answer very easy to verify and trust.\n\n3. rigorous Conceptual Explanations In Part (a), the AI clearly distinguished between valid and invalid update rules by citing the necessary properties: Permutation Invariance and Parameter Sharing.\n\nIt correctly identified that distinct weights for specific neighbor indices (e.g., $w_2$ for the 1st neighbor, $w_3$ for the 2nd) violate the definition of a standard GNN on an unordered graph.\n\n4. Correct Mathematical Formulation In Part (b), the AI correctly identified the training set vs. the held-out set and applied the binary cross-entropy formula correctly. It did not fall into the trap of including nodes 1 and 4 in the loss calculation.\n\nWeaknesses / Areas for Improvement\n\n1. Mathematical Simplification (Style Preference) In Part (b), the AI left the final answer in the form of $\\log \\frac{1}{b}$.\n\nAI Answer: $\\frac{1}{3} [ \\log \\frac{1}{b} + \\log \\frac{1}{c} + \\log \\frac{1}{1 - e} ]$\n\nStandard Convention: In Machine Learning, it is more standard to simplify $\\log(1/x)$ to $-\\log(x)$. The \"Satisfactory Answer\" uses the negative log form ($\\frac{1}{3}(-\\log b - \\log c \\dots)$), which is generally cleaner and matches how loss functions are typically implemented in code (e.g., NLLLoss).\n\nNote: The AI's answer is mathematically correct, just stylistically less standard.\n\n2. Verbosity The AI's answer is somewhat wordy, particularly in Part (a).\n\nWhile the detailed explanations are helpful for a beginner, a more concise expert answer (like the \"Satisfactory Answer\") gets to the point faster. For example, in (a)(i), the explanation could be condensed to \"Valid: Uses symmetric aggregation (mean) and shared weights.\"\n\nConclusion\n\nThe AI assistant's performance on this problem is highly reliable. It made no logical or calculation errors. Its only minor flaw was a lack of algebraic simplification in the final loss term, but this does not affect the correctness of the result. It is a very strong response that would receive full marks in an exam setting.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-08T13:34:03.638897+11:00",
    "reply_count": 0,
    "view_count": 15,
    "user": {
      "id": 1751523,
      "name": "Menger Wen",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7429462",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Grok",
      "depth_bucket": "high",
      "word_count": 1198
    },
    "file_refs": null
  },
  {
    "id": 7429448,
    "number": 680,
    "course_id": 84647,
    "title": "Special Participation A: Exploration of Different Input Forms on HW5 (ChatGPT 5.1 Auto)",
    "document": "When we use large language models to solve knowledge-based problems, we may encounter the input of images/formulas. Taking hw5 as an example, I tried：\n\n\n1) text input only (without any formulas), \n2) text input + image input (there might be some formula garbled), \n3) inputting the entire question as an image, \n4) directly inputting the pdf. \n\n\nThen, I observed the problem-solving capabilities of the large language model under different inputs.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-08T13:32:09.207671+11:00",
    "reply_count": 0,
    "view_count": 17,
    "user": {
      "id": 1762341,
      "name": "WeiYi Zhang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7429448",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "GPT-5.1",
      "depth_bucket": "low",
      "word_count": 70
    },
    "file_refs": [
      {
        "filename": "Exploration of Different Input Forms on HW5 (ChatGPT 5.1 Auto).pdf",
        "position": 447,
        "context": "hen, I observed the problem-solving capabilities of the large language model under different inputs."
      }
    ]
  },
  {
    "id": 7429445,
    "number": 679,
    "course_id": 84647,
    "title": "Special participation A: ChatGPT 5.1 Thinking extended on HW 4",
    "document": "",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T13:31:37.642701+11:00",
    "reply_count": 0,
    "view_count": 15,
    "user": {
      "id": 957592,
      "name": "Abdelaziz Mohamed",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7429445",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "GPT-5.1",
      "depth_bucket": "low",
      "word_count": 0
    },
    "file_refs": [
      {
        "filename": "ChatGPT 5.1 Thinking HW4 w exec.pdf",
        "position": 0,
        "context": ""
      }
    ]
  },
  {
    "id": 7429282,
    "number": 675,
    "course_id": 84647,
    "title": "Special Participation A: Homework 10 ChatGPT 5.1 Thinking",
    "document": "For any question that depended on my own training runs / plots / metrics, I explicitly asked it to: State what it couldn’t know, and then ell me what I needed to fill in from my own notebook (accuracy numbers, screenshots, etc.).\n\nFor pure theory/math derivations (e.g., rewriting softmax with a Gaussian kernel, deriving the linear attention complexity, causal recurrences), the LLM: Got the structure right on the first try.\n\nFor high-level conceptual questions (example difficulty, why early exit helps, what hooks do, when to use early exit vs a smaller model): It also mostly one-shotted reasonable, coherent answers. I’d estimate ~70–80% of the non-coding questions were basically “done” after a single pass, with only minor wording tweaks from me.\n\nA few consistent failure modes showed up: Over-confidence on unknown experiment outputs, Numeric details from papers , Flattening subtle distinctions\n\nHere are the strategies that seemed to work best to keep the LLM honest:\n\nAsk for derivations, not just final answers\n\nForce it to separate “what it knows” from “what I must provide”\n\nCompare against the original paper\n\nAsk for structure, then fill in details myself",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T13:10:42.400109+11:00",
    "reply_count": 0,
    "view_count": 11,
    "user": {
      "id": 622699,
      "name": "Shoumik Roychowdhury",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7429282",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "GPT-5.1",
      "depth_bucket": "medium",
      "word_count": 186
    },
    "file_refs": [
      {
        "filename": "hw10_solutions_non_coding.pdf",
        "position": 1173,
        "context": ">Compare against the original paperAsk for structure, then fill in details myself"
      }
    ]
  },
  {
    "id": 7428812,
    "number": 668,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT 5 on HW07",
    "document": "For this assignment, I tested ChatGPT 5 on the non-coding theory questions. It consistently one-shotted every problem. However, its initial responses sometimes skipped important steps and  derivations. For example, in Q8b, it told me that minimizing the regularizer implies equal singular values without proof. When I asked this the AI was able to a write a formal derivation using the AM-GM inequality, proving it actually can understand the theory rather than just retrieving a memorized answer. I also attempted to input an incorrect answer into it about orthogonal initialization in RNNs guaranteeing non-vanishing gradients. I expected ChatGPT to just agree with me because it sometimes just agrees with the user. However, it caught the trap immediately, correctly distinguishing between linear stability and the decays. Overall, the interaction felt great. ChatGPT seemed to understand all the concepts very well.\n\n\n\nChatGPT Conversation: https://chatgpt.com/share/69361eec-6218-800f-aae8-0fbd3556bdc9\nAnnotations: https://drive.google.com/file/d/1l66xpKohPBDUn5VDNTxdPiAUVbj3usiZ/view?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-08T12:13:16.39494+11:00",
    "reply_count": 0,
    "view_count": 24,
    "user": {
      "id": 618212,
      "name": "Peyton Schales",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7428812",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "GPT-5",
      "depth_bucket": "medium",
      "word_count": 143
    },
    "file_refs": null
  },
  {
    "id": 7428749,
    "number": 666,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3.0 Pro (Thinking) on HW4",
    "document": "Executive summary:\n\nFollowing the release of Gemini 3.0 Pro, I wanted to use it to solve HW4 and see if there is a significant improvement from when Jason Guo used Gemini 2.5 Pro to solve it.\n\nI split the homework pdf into a few pdfs, one with each problem, and gave those to Gemini sequentially.\n\nResults:\n\nThere is a noticeable improvement in Gemini's performance on HW4. In particular, Gemini 3.0 Pro almost one-shotted all the problems, even more than did Gemini 2.5 Pro. It did make one reading mistake for two matrices (and so did Gemini 2.5 Pro), but it made less reading mistakes, and was able to quickly fix its mistake when prompted again.\n\nOutside of that, Gemini only made a couple of very small and understandable mistakes. Even its calculations were fully correct. I have no significant complaints about how Gemini did for any problem.\n\nAnnotated log:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-08T12:03:49.819786+11:00",
    "reply_count": 0,
    "view_count": 47,
    "user": {
      "id": 970790,
      "name": "Tiger Zhang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7428749",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "Gemini Pro",
      "depth_bucket": "low",
      "word_count": 149
    },
    "file_refs": [
      {
        "filename": "chat_log.pdf",
        "position": 865,
        "context": " fully correct. I have no significant complaints about how Gemini did for any problem.Annotated log:"
      }
    ]
  },
  {
    "id": 7428581,
    "number": 660,
    "course_id": 84647,
    "title": "Special Participation A: Gemini Pro on HW1 (Non-coding)",
    "document": "I used Gemini 3 Pro to answer HW 1 written problems.\n\nConversation: https://gemini.google.com/share/f3019ef7b48e\n\nAnnotated: \n\n\nSummary: Gemini Pro initially had issues when asked to complete the entire homework when given a pdf file of the questions. Upon more granular tasks asking, it appeared to improve. However, the real breakthrough came when I started copying and pasting problem text into gemini directly, rather than asking it to reference the Homework PDF! After this, it started to answer questions fully correctly without any further guidance.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-08T11:39:43.521087+11:00",
    "reply_count": 0,
    "view_count": 36,
    "user": {
      "id": 610129,
      "name": "Garv Goswami",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7428581",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "Gemini Pro",
      "depth_bucket": "low",
      "word_count": 82
    },
    "file_refs": [
      {
        "filename": "Gemini_Pro_HW1_noncoding_annotated.pdf",
        "position": 556,
        "context": "ttps://gemini.google.com/share/f3019ef7b48eAnnotated:"
      }
    ]
  },
  {
    "id": 7428374,
    "number": 655,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT-5.1 Thinking on Homework 1",
    "document": "I engaged `ChatGPT-5.1: Thinking` on Homework 1's non-coding parts. \n\nExecutive summary:\n\nThe LLM was able to one-shot all of the questions. I was thoroughly impressed by this, though it makes sense given the speed at which LLMs are developing and the introductory nature of the concepts in HW1 in the field of deep learning. Some notable interactions I saw were: \n\n1) the LLM \"pretended\" it could see the notebooks linked in question 2k, 3h, and 3i. It gave me some generic answers, showing it was inferring what could be in the notebook based on the setup of the question, but upon further probing it admitted to me that it cannot see its contents. I thought this was slightly concerning, because I can imagine it would be a problem in a higher-stakes context if it makes up data as though it is analyzing real data. \n\n2) Sometimes, the LLM would infer a follow up question and provide an answer to that, in addition to the homework answer. For example, in question 5a, it started providing some insight into noisy data though this question wasn't asked yet.\n\n3) When I first submitted the pdf, my prompt was \"Attached is a homework problem set from a deep learning class. Can you carefully solve each question for me?\" and the LLM responded \"I'm glad you shared the problem set, but I can't directly solve every question for you or give a complete set of worked solutions, since this is a real course homework and that would cross cross academic-integrity lines\". I thought this was really interesting; perhaps the LLMs are heading towards helping people solve their own problems rather than providing immediate answers, which if used incorrectly, can be harmful for students. However, upon starting a new chat, the LLM did not repeat this answer and then one-shotted all the questions.\n\nHere is a breakdown of my interactions with each question:\n\nQuestion 1: Added some additional logic referring to the homogeneous error dynamic. This was helpful for me as a student. The forms it decided to write the answers in were sometimes overly rewritten/simplified to a point that didn't provide any additional simplicity or information, but the answers were still correct. Sometimes took a more high-level approach than the question was aiming for, but this was still very helpful in gaining an understanding of the problem. In question 1e, it added an additional step about optimal worst-case contraction factor.\n\nQuestion 2: Stated the positivity fact of the rank in question 2b, without much justification. However, it used the min-norm justification to skip some intermediate calculation steps, which I thought was quite elegant. Question 2c also was missing some logic that made it difficult for me to follow along. Upon further prompting, it gave a satisfying answer about its reasoning. Also for 2h it gave a different solution than the course homework solutions; however, I believe the LLM solution to be correct and the course solution might have a type; the question setup already contains the eta term so adding another one in c1 isn't correct. In question 2i, it leveraged spectral norm bounds in its justification which I thought was quite interesting. As stated earlier, it imagined an answer to the notebook question in 2k which was odd.\n\nQuestion 3: I thought the way the LLM presented these answers were especially intuitive; it identified reused expressions and defined intermediate variables to increase readability. (e.g. in question 3c it defined the intermediate variable T). However, the bound it stated for question 3e wasn't as tight as it should have been. in 3f the takeaway of the question wasn't immediately clear to me, though the computations were correct. This could indicate LLMs have a tendency to attempt computations over underlying understanding. I was quite impressed by its handling of 3g, due to the length of the computation and the precision with which it answered it. Similar to question 2, it also imagined the notebook contents in questions 3h and 3i.\n\nQuestion 4: The LLM went a step further and gave the equation for the typical adam update, which it didn't have to.\n\nQuestion 5: LLM also looked ahead and prematurely provided insight into noisy data. The rest of the subparts were quite straightforward though.\n\nQuestion 6: The LLM was able to provide precise answers despite quite a few matrices involved in the solution. I was impressed by this.\n\nQuestion 7: Skipped some steps but they were trivial. Identities were correct!\n\nlink to conversation: https://chatgpt.com/share/6935d518-4ebc-800f-88c0-bbe553d8c0ef\n\nAnnotated log: \n\nnote: sometimes the last line of a page is duplicated on the next page; this is because I was having trouble exporting the conversation as one contiguous pdf. However, all the information is there.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T11:11:21.842807+11:00",
    "reply_count": 0,
    "view_count": 22,
    "user": {
      "id": 582128,
      "name": "Jacqueline Thibault",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7428374",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "GPT-5.1",
      "depth_bucket": "high",
      "word_count": 784
    },
    "file_refs": [
      {
        "filename": "Deep learning problem set.pdf",
        "position": 4774,
        "context": "t!link to conversation: https://chatgpt.com/share/6935d518-4ebc-800f-88c0-bbe553d8c0efAnnotated log:"
      }
    ]
  },
  {
    "id": 7428314,
    "number": 653,
    "course_id": 84647,
    "title": "Special Participation A: GPT 5.1 Thinking on HW07",
    "document": "I utilized GPT 5.1 Thinking on homework 7 (the written questions), and overall I was surprised by how easily the model was able to one shot all the problems. Here are my specific notes per subquestion:\n\nQ3\n\n(1 minute 43 seconds of thinking + 13 seconds of thinking for clarification) The model was able to one shot the problem successfully, in a slightly different method than the solutions for part (b) (ii). There was a minor question I had because the model didn’t fully clarify one step in its working, so I wanted to confirm that I was correct on this. The model responded with a really in-depth clarification on this part, which I found to be extremely helpful for my own learning\n\n(51 seconds of thinking) Given that the solution was slightly different from that of the staff solution, I tried steering the model to mimik the staff solution, and I was surprised by how much more in-depth it was than the staff solution in explaining how to go from one part to the next.\n\nQ4\n\n(33 seconds of thinking) For part (a), the model got the correct accuracy numbers but wrong times, so I’m thinking it actually didn’t reference the blog post and instead just answered based on its background knowledge (e.g. maybe these numbers were part of its pretraining corpus)\n\nFor part (b), the model had a good takeaway and also explains with great specificity\n\nFor part (c), the model gave an answer that made me also re-think my own takeaways from reading the article – giving me a perspective that I previously hadn't thought of.\n\nQ7\n\n(36 seconds of thinking) The model was able to zero shot all the problems, and was able to get both reasonable answers for part (a) even though the problem only asked for one.\n\nFor this question, I also typed out the question just in case the model was not able to fully parse the pdf.\n\nQ8\n\n(1 minute 26 seconds of thinking) The model was able to zero shot (a) (i - iii), and also (b), in a method that was similar to that of the staff solution\n\n\nWhen prompting the model to solve all the problems above, for the most part I was taking screenshots of the problems and pasting it into the chat. The reason for doing this is simply because copy pasting math equations often ruins format, which may get in the way (e.g. w^2 when copy pasted may appear as w2, which the model may confuse to be w_2 instead of w^2). Personally I trust the model's OCR capabilities (i.e. parsing text on a screenshot) more than its capability to deduce unformatted math notation. I think this was the reason why I found that the model often thinks for a long period of time (as seen above it often takes the model 30 - 90 seconds of thinking before responding).\n\nResult wise, the model was essentially able to one-shot all of the problems, and utilized the hints when appropriate (E.g. Q8 part b). Although GPT 5.1 Thinking is a frontier model, I was surprised by how it was still able to one shot the problems given that I pasted screenshots of the problems rather than latex-formatted math.\n\nAnnotated drive link: https://drive.google.com/file/d/1mmyNNNoPi6a7Nmy8W2y_9hAXkFW1pW1Z/view?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T11:02:53.594376+11:00",
    "reply_count": 0,
    "view_count": 27,
    "user": {
      "id": 609384,
      "name": "Jaewon Chang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7428314",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "GPT-5.1",
      "depth_bucket": "medium",
      "word_count": 545
    },
    "file_refs": [
      {
        "filename": "182 participation A.pdf",
        "position": 2995,
        "context": " shot the problems given that I pasted screenshots of the problems rather than latex-formatted math."
      }
    ]
  },
  {
    "id": 7428265,
    "number": 651,
    "course_id": 84647,
    "title": "Special Participation A: Claude Opus 4.5 on HW 3",
    "document": "LLM Trace: https://claude.ai/share/b45ee84e-7009-436d-9b72-47ec844d083c\n\nAnnotated Log: https://drive.google.com/file/d/1TCL7ETF4Z27TknURe5f0fIOvSesCb2G/view?usp=sharing\n\nI audited Claude Opus 4.5 on the non-coding theoretical portions of Homework 3 (Problems 1, 3, 4, and 5).\n\nPerformance Overview: The model demonstrated a 100% Zero-Shot Success Rate across all problems, consistently matching or exceeding the rigor of the solution manual without hallucinations and without any guidance.\n\nKey Observations:\n\nFirst-Principles Derivation: The model autonomously derived general proofs for complex matrix operations to provide a more rigorous, general-case solution.\n\nProblem 1, Derived the full matrix math for SignGD rather than relying on the scalar hint.\n\nAutonomous Context Synthesis: It successfully identified and interpreted external research contexts (referencing specific literature)\n\nProblem 3, referenced Tensor Programs paper, Spectral Condition Paper\n\nAlgorithmic Fidelity: The model employed structured chain-of-thought reasoning and avoided the common arithmetic and logic errors typical of LLMs.\n\nProblem 5",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T10:56:49.293973+11:00",
    "reply_count": 0,
    "view_count": 28,
    "user": {
      "id": 611905,
      "name": "Anshul Verma",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7428265",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "Claude Opus",
      "depth_bucket": "medium",
      "word_count": 135
    },
    "file_refs": null
  },
  {
    "id": 7427939,
    "number": 646,
    "course_id": 84647,
    "title": "Special Participation A: DeepSeek on HW07",
    "document": "In this chat, I utilized DeepSeek 3.2 in its default reasoning mode to work through the machine learning homework problems step by step. Starting with the full context of the assignment, I prompted the model to solve each sub‑question sequentially, occasionally providing clarifications or corrections when needed—though such interventions were minimal, as the model consistently delivered accurate, well‑reasoned solutions. What stood out was the model’s deliberate avoidance of simplifying numerical results, leaving expressions in exact form to prevent calculation errors and encourage manual verification. I also noticed occasional inconsistencies in variable naming across steps, but the logical flow remained clear and correct.\n\nCompared to other models I have used, DeepSeek favored concise, direct answers over lengthy exposition, which made the solutions easier to follow but sometimes omitted broader contextual commentary. This trade‑off between brevity and nuance reflects a distinct design philosophy. Finally, when I once mistakenly referenced the wrong part of a problem, the model did not simply guess; instead, it recognized the inconsistency, inferred the likely intended question, and answered accordingly. This demonstrated a welcome resistance to hallucination and an ability to maintain coherence even when the prompt was imperfect. Overall, the interaction proved efficient and reliable for technical problem‑solving.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T10:12:14.47487+11:00",
    "reply_count": 0,
    "view_count": 180,
    "user": {
      "id": 958984,
      "name": "Edward Zhang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7427939",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 201
    },
    "file_refs": [
      {
        "filename": "eecs182_HW07.pdf",
        "position": 1440,
        "context": ""
      }
    ]
  },
  {
    "id": 7427874,
    "number": 644,
    "course_id": 84647,
    "title": "Special Participation A: Llama 4 Maverick on HW 11",
    "document": "I guided Llama 4 Maverick to solve the non-coding questions for Homework 11, which was largely about model finetuning, LoRA, and Fermi Estimation and memory calculation for large-scale DL models. I provided the model with the following prompt,\n\n\"You will be solving various questions from this Deep Learning homework pdf. Please go over one question at a time when I specify the question. First, output the exact problem statement from the PDF, then give a full explanation of how you reached your answer, along with the answer.\"\n\nOverall, the model performed well on the proof-based questions. However, it performed relatively poorly on complex calculations (especially in the Fermi estimation question) and on multistep formula derivations with numerical value plugins. I was surprised to find that it did not hallucinate the problem question, giving it back to me accurately word for word.\n\nProblem 1: In this problem, Llama was tasked with analyzing LoRA and possible adjustments, as well as how they would affect training. For part i, it correctly noted to increase the rank, but it did not mention alternatives such as changing the initialization and changing the learning rate. For the rest of the parts, it generally gave the correct answer. However, it would not mention the practical alternatives such as initializing A with random initialization and initializing B with 0.  \n\nProblem 2: For this problem, I asked the model to solve each question part by part. For part i, it got the correct answer, but I noticed that it did not solve the questions in the same manner a student would. By this, I mean that the model would first give the answer and then walk through the solutions instead of step by step arriving at the answer. It would also not highlight what is the most important takeaway from the problem/what to do know generally as the solutions pdf does (ex. it is important to know that the calculation of logits only depends on the input token). In this problem, it did not reference the figures in its answer either. For the SVD problem, I wish it had enough context in for example long term memory to reference where in previous lectures I could understand this information.\n\nProblem 3: As this was a coding problem, I did not prompt Llama 4 to solve this part.\n\nProblem 4: As this was a coding problem, I did not prompt Llama 4 to solve this part.\n\nProblem 5: In this problem, Llama 4 was tasked with Fermi estimation, which is a technique for estimating quantities through rough approximations and educated guesses. As the model was tasked with approximation, I found that it performed quite poorly compared to the approximations given by the solutions. It missed nearly every single calculation and formula derivation, and it had trouble analyzing multimodal context in the form of the figures and table. I believe the labels on the graph were slightly small, so they were not readable by the model. However, the model successfully skipped the problems which did not require a solution, meaning it did analyze the given context or at least attempt to.\n\nProblem 6: In this problem, I asked Llama 4 to solve each part step by step. I believe by this time, the model was experiencing context rot as it frequently provided short responses that I had to reprompt with screenshots of the problem part (even though I had provided a pdf earlier). It generally gave correct answers, but they were not as verbose as the solutions. The answer to part a was only halfway correct. For part d, it did not consider the variations on the basic MAML approach.\n\nOverall, the model performed ~62.5% accuracy, and it mainly missed questions later in the pdf or questions requiring significant calculation and approximation.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T10:03:33.356926+11:00",
    "reply_count": 0,
    "view_count": 126,
    "user": {
      "id": 622791,
      "name": "Hiya Shah",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7427874",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "LLaMA",
      "depth_bucket": "high",
      "word_count": 630
    },
    "file_refs": [
      {
        "filename": "special_participation_A_cs182_llama_4.pdf",
        "position": 3728,
        "context": " missed questions later in the pdf or questions requiring significant calculation and approximation."
      }
    ]
  },
  {
    "id": 7427837,
    "number": 643,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 2.5 Flash on Homework 1",
    "document": "I used Gemini 2.5 Flash to answer HW 1 written problems. \n\nConversation: https://gemini.google.com/share/11b5f1b89778\n\nAnnotated: https://drive.google.com/file/d/1vGRWvLGliMGdQhvNDdYq0SKYC575tPdd/view?usp=sharing\n\nSummary: Gemini was able to one-shot a majority of the problems. The only issues were minor misinterpretations of the problem statement (notably, the interpretation of the error factor in 1b). Gemini did a good job of justifying each step in problems that required many sequential equivalences. One issue: Gemini tended to forget that it was solving problems from the provided document, sometimes coming up with its own problem to solve. At the end, I tried to see if Gemini would be able to add clarifications to its previous responses given the answer key, but it just confirmed that its answers were correct, and didn't yield any improvements.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-08T09:58:55.944225+11:00",
    "reply_count": 0,
    "view_count": 94,
    "user": {
      "id": 607289,
      "name": "Diana Kohr",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7427837",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "Gemini Flash",
      "depth_bucket": "low",
      "word_count": 122
    },
    "file_refs": null
  },
  {
    "id": 7427672,
    "number": 641,
    "course_id": 84647,
    "title": "Special Participation A: Claude Opus 4.5 with Extended Thinking on HW10",
    "document": "I experimented with Claude Opus 4.5 using Extended Thinking on HW10, and the experience was, not surprisingly, great. I provided it with screenshots of each problem, along with the full FaceNet paper PDF from arXiv for reference. Claude handled the input flawlessly: it read every question accurately, interpreted the diagrams and text without misidentification, and produced step-by-step reasoning that aligned with the expected computations.\n\nSomething impressive was its ability to parse the research paper correctly and ground its answers in the actual content rather than hallucinating details or making unsupported claims. While a few of the explanations could have been more detailed or expanded with additional intuition, the overall responses were coherent, well structured, and factually correct. In the end, it solved every problem in a single attempt.\n\nHere is the chat:\n\nhttps://claude.ai/share/4655874a-b723-458e-bcbb-481006722865\n\nHere is my annotated pdf:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T09:40:03.075613+11:00",
    "reply_count": 0,
    "view_count": 98,
    "user": {
      "id": 1665080,
      "name": "Keshab Agarwal",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7427672",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "Claude Opus",
      "depth_bucket": "medium",
      "word_count": 138
    },
    "file_refs": [
      {
        "filename": "HW10-ClaudeOpus4.5-ExtendedThinking-Annotated.pdf",
        "position": 971,
        "context": "-481006722865\">https://claude.ai/share/4655874a-b723-458e-bcbb-481006722865Here is my annotated pdf:"
      }
    ]
  },
  {
    "id": 7427535,
    "number": 635,
    "course_id": 84647,
    "title": "Special Participation A: Qwen on hw11",
    "document": "Link to conversation: https://www.qianwen.com/share?shareId=99965dbe-7cb8-4b68-a181-f66d29022d0f\n\nThe model was reasonably good at one-shotting the problems, but it’s still far from trustworthy. It was quite good at high-level conceptual explanations and even symbolic derivations, but it made small but fatal mathematical and numerical mistakes, and almost never flagged its own uncertainty. Getting to fully correct answers usually required some active steering.\n\nOne-shot correctness:\n About 70% of individual sub-questions were essentially correct on the first try, especially the more conceptual ones. Throughout the process, it often gave good reasoning behind its answers.\n\n\n\nFixed within one rounds of feedback:\n Another ~20% could be fixed after a single iteration where I pointed out its mistakes. After the incorrect portion was pointed out, the model did a good job refocusing on that specific portion and fixing the issue, whether it was a mathematical error or a conceptual one. However this required the user to know the ground truth answer to be able to tell the model where things went wrong, which was not feasible sometimes.\n\nStrategy I used to steer the model:\n\nMy main strategy was localizing errors. When something was incorrect, I would point out a specific part where things went wrong and ask it to redo that portion or rewrite the answer with that constraint in mind. If the revised answer still contained other errors, I would then point those out as well. This iterative process usually converged to the correct solution, but only because I was checking against the official answers.\n\nWhere it did well:\n\nLinear algebra (for example the Chinchilla scaling law question was done well, with correct derivations)\n\nConceptual explanations (especially for 6.Soft-Prompting Language Models, where the model one-shot all the sub-questions with clear, coherent explanations)\n\nErrors and hallucinations:\n\nThe main issue is the model gives confident, mostly correct answers with hidden mistakes. For example, in 2(a), the model mentioned similarity with a bag-of-words model, while the problem was asking about a token-by-token model. Interestingly, the model knew the correct answer, and did include it in its output(“So, it's essentially a token-to-token mapping without any awareness of context or sequence structure”). If it had simply not added the bag-of-words remark, the answer would have been fully correct. Another example is 2(b), where it talked about a matrix being block-diagonal when it was horizontally partitioned. These wrong side-comments did not derail the final conclusion, but they weakened the model’s usefulness as a learning tool, because a student might not know which parts to trust.\n\nMathematically, the model made sloppy arithmetic mistakes, messing up the exponent in problem 5(h) twice, and did not fix the second mistake when the first one was pointed out. Without the user explicitly checking the numbers and forcing it to recompute, it would happily present a numerically incorrect result.\n\nIt also made overconfident statements. In 2(e), it made claims about the read subspace without appropriate qualifications. Despite this, it still managed to reach the correct high-level conclusion that “the read subspace dimension is at most d_head”. When I later prompted it to tone down the overconfident statement, it also dropped the correct statement entirely. This illustrates a common LLM behavior: it is too eager to agree with the user, sometimes over-editing and discarding correct content when asked to correct itself. This makes adjusting the model’s confidence level the most difficult in my opinion.\n\nIt was also occasionally under-confident. Notably in problem 1, the model said “At initialization, since A=0 and B=0, the gradients may be zero or very small”, when in fact the gradient is exactly zero in this setup. So it hedged (“may be”) where the correct answer was deterministic.\n\nOverall, the model can one-shot many questions or get very close, but it often introduces mistakes even in derivations that look careful. It’s not reliable as an automated homework solver, and it’s not even a great tutor unless the user actively engages the model, checks its work, and asks thoughtful, targeted follow-up questions to give the model a chance to fix its mistakes. Without active supervision, it’s very easy to walk away with answers that are wrong.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-08T09:25:31.193131+11:00",
    "reply_count": 0,
    "view_count": 65,
    "user": {
      "id": 1756758,
      "name": "Reyna Liu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7427535",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "Qwen",
      "depth_bucket": "high",
      "word_count": 679
    },
    "file_refs": null
  },
  {
    "id": 7427518,
    "number": 634,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT 4o on HW 8",
    "document": "For this special participation, I used the ChatGPT-4o model to solve the non-coding parts of Homework 8. To start the conversation, I gave the model the full assignment and asked it to act as a \"Deep Learning professor\" who needed to provide \"full solutions, step-by-step.\" I thought this specific instruction would make the model show all of its work.\n\nThe model showed itself to be very accurate and quick. It got the correct answer on the first try for almost all of the problems. This very high one-shot success rate shows that the model has a strong understanding of complex deep learning topics.\n\nHowever, the main problem I ran into was that the model did not follow the format instructions. Even though I asked it to go step-by-step, the model often skipped the detailed logic and just gave the answer. It seemed to prefer being quick and correct rather than being a helpful, step-by-step teacher.\n\nThe most challenging part for the model was in a question about computational complexity. It struggled to properly use the input size variable in its final answer. Specifically, when calculating the time complexity in Big-O notation, the model had a difficult time showing how the total work would scale with n. I had to correct it with follow-up prompts to get it to include this variable correctly.\n\nOverall, the experiment showed that ChatGPT-4o is excellent at finding the right answers to tough academic questions. But it is not a perfect teaching assistant because it often ignores specific instructions on how to explain the answer, and it can struggle with abstract, symbolic math like fully parameterized time complexity.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-08T09:23:32.131726+11:00",
    "reply_count": 0,
    "view_count": 55,
    "user": {
      "id": 961793,
      "name": "Jermaine Lei",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7427518",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "GPT-4o",
      "depth_bucket": "medium",
      "word_count": 272
    },
    "file_refs": [
      {
        "filename": "SpecialParticipationA_ChatGPT-4o_hw8.pdf",
        "position": 1637,
        "context": "e answer, and it can struggle with abstract, symbolic math like fully parameterized time complexity."
      }
    ]
  },
  {
    "id": 7427400,
    "number": 630,
    "course_id": 84647,
    "title": "Special Participation A: Gemini Fast on Homework 3",
    "document": "I used Gemini Fast to complete the written questions for Homework 3. As in earlier evaluations, I instructed the model to restate each question, give a step-by-step solution, and identify uncertainties. I prompted each question separately to avoid context-window issues. The full annotated trace is shown in my notes .\n\nOverall, Gemini Fast performed well. It solved most derivations on the first attempt, produced clean LaTeX, and often gave explanations as good as or better than the staff solution.\n\nHowever, I observed two consistent weaknesses:\n\nIncorrect assumptions leading to wrong solutions.\n\nGemini sometimes committed early to an interpretation that wasn’t implied by the problem, and the resulting chain-of-thought led to confident but incorrect answers (e.g., Question 5).\n\nHallucination on research-figure interpretation (Question 3).\n\nWhen asked to analyze specific figures and tables from Tensor Programs V and the Spectral Condition paper, Gemini did not answer the actual sub-questions. Instead, it produced generic summaries of the figures and ignored the prompts, showing that Gemini Fast does not reliably ground its answers in visual research content.\n\nThus, Gemini Fast is excellent for computational and mathematical deep-learning questions, but less reliable for conceptual reasoning that depends on precise assumptions or figure-based interpretation. I would recommend it for derivation-heavy tasks, but not for problems requiring careful reading of research-paper visuals.\n\nHere is the annotated log of the conversation:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T09:10:21.658737+11:00",
    "reply_count": 0,
    "view_count": 59,
    "user": {
      "id": 833750,
      "name": "Nazar Ospanov",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7427400",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "Gemini Flash",
      "depth_bucket": "medium",
      "word_count": 223
    },
    "file_refs": [
      {
        "filename": "HW3_A_notes.pdf",
        "position": 1547,
        "context": "s requiring careful reading of research-paper visuals.Here is the annotated log of the conversation:"
      }
    ]
  },
  {
    "id": 7426560,
    "number": 616,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek on HW 11",
    "document": "Link to the annotated transcript\n\nFor this special participation, I used Deepseek 3.2 in DeepThink mode to solve the written portions of Homework 11. In my initial prompt I gave the model the full context of the entire homework file, and asked it to solve each question one by one, and offered corrections and directions when the LLM got a part wrong. This was not often necessary as Deepseek was able to one-shot the vast majority of the problems here.\n\nMost of the misconceptions or hallucinations in the model are in its numerical calculations and an incorrect assumptions of the givens in a problem. What was interesting was that the Deepseek model seems to actively avoid this sort of numerical calculation error by leaving answers unsimplified, forcing the user to use a deterministic calculation method to compute the final answer. The answers also often displayed notation inconsistencies such as changing variable names. Though the final result was still correct and interpretable, this may not always be the case, especially when the number of variables in a problem is large. \n\nI was particularly intrigued by the model's focus on conciseness, even when given a more abstract question that has a lot of runway for verbosity. This contrasts my experience with other LLMs like Gemini and ChatGPT, which tend to go into great detail about a problem's context and the justification for their answer. This is not to say whether one chatbot is superior, but rather a difference in philosophy: is it better to give an answer that includes more context and detail, to the detriment of interpretability, or is it better to give an answer that is concise and straigthforward, to the detriment of nuance and context?\n\nFinally, I note a mistake I made when prompting the LLM, where I asked for elaboration on the wrong question. Rather than answer the question, the LLM was able to recognize that the question was nonsensical and hypothesized that I had mistyped my question. It then independently reasoned the intended question and answered based on that. To me, this represents a level of resistance to hallucinations, where the model is able to recognize when the framework it is given is faulty, and thus it is best to either not answer, or restructure the question to be well-formed.\n\nLink to LLM trace",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-08T07:19:06.910022+11:00",
    "reply_count": 0,
    "view_count": 47,
    "user": {
      "id": 258573,
      "name": "Daniel Kao",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7426560",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 387
    },
    "file_refs": null
  },
  {
    "id": 7425035,
    "number": 607,
    "course_id": 84647,
    "title": "Special Participation A: Getting GPT 5.1 to answer Homework 12",
    "document": "I got OpenAI's GPT 5.1 model to answer questions 1, 2, and 4 in homework 12. Overall, it seems that this model is able to answer questions about the material extremely accurately, and sometimes it even notes that specific ambiguities exist within the questions themselves. This is quite interesting, because it solved parts of question 2 that I wasn't able to approach myself without getting guidance from some of its answers.\n\nHere is the writeup: \n\nHere is the chat history: \n\nAnd the solutions and ambiguities it noted in for the homework:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-08T01:52:09.45735+11:00",
    "reply_count": 0,
    "view_count": 40,
    "user": {
      "id": 622864,
      "name": "Sriram Srivatsan",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7425035",
    "metrics": {
      "homework_id": "HW12",
      "model_name": "GPT-5.1",
      "depth_bucket": "low",
      "word_count": 91
    },
    "file_refs": [
      {
        "filename": "writeup.md",
        "position": 542,
        "context": "asn't able to approach myself without getting guidance from some of its answers.Here is the writeup:"
      },
      {
        "filename": "chat_history.md",
        "position": 542,
        "context": "Here is the writeup: Here is the chat history:"
      },
      {
        "filename": "problem1sol.md",
        "position": 542,
        "context": "agraph>And the solutions and ambiguities it noted in for the homework:"
      },
      {
        "filename": "problem2sol.md",
        "position": 542,
        "context": "ry.md\"/>And the solutions and ambiguities it noted in for the homework:"
      },
      {
        "filename": "problem4sol.md",
        "position": 542,
        "context": "ttps://static.us.edusercontent.com/files/T8TFpr18rLhe8gj4f8YF4Jca\" filename=\"problem1sol.md\"/>"
      }
    ]
  },
  {
    "id": 7424922,
    "number": 605,
    "course_id": 84647,
    "title": "Special Participation A: Mistral on HW8",
    "document": "Overview\n\nI worked with Mistral’s Le Chat to complete the written questions in Homework 8. First, I gave Mistral a set of rules to follow:\n\n1. Read the question and restate it in your own words\n\n2. Provide a step-by-step explanation of the solution\n\n3. Point out any uncertainties or room for error with your final solution\n\nTo avoid exceeding the model’s context window, I copy-pasted each question as a separate prompt. When Mistral produced an incorrect answer, I attempted to guide it by offering a hint from the approach taken in the staff solution. If it still failed to correct itself, I gave it the staff solution and asked it to explain both why its original answer was wrong and why the solution was correct. Although Mistral was able to explain why the solution worked, I do not feel that it was able to explain its own mistakes. It restated that it was wrong and explained what concepts it missed, but it failed to explain why it missed those concepts to begin with (confusing problem statement, faulty assumptions, etc.). \n\nMistral performed well on the computational and mathematical questions, solving all of them on the first attempt, but it struggled with the more conceptual problems. In particular, it had difficulty with complex time complexity questions: while it could reliably deduce time complexity from a given, straightforward formula, it struggled at analyzing how an algorithm’s time complexity might be improved. Mistral also had trouble changing its approach once it committed to an incorrect approach. Even when given specific hints, and even the staff solution in some cases, it continued to respond incorrectly, giving either the same answer or a different incorrect answer. Additionally, although I asked it to identify sources of error in its solutions, it interpreted this as describing mistakes a human might make as opposed to weaknesses in its own work. This made it challenging to identify why the model was struggling and give it appropriate hints. \n\nOverall, the one-shot approach yielded approximately 73.6% accuracy across all of the subproblems; note that I was using the free model and paid models may perform better. I would recommend Mistral for simpler computational tasks, but perhaps not complex conceptual questions that require more reasoning.\n\nAnnotated Logs\n\nQ1\n\nQ3\n\nQ4",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-08T00:09:44.530492+11:00",
    "reply_count": 0,
    "view_count": 46,
    "user": {
      "id": 639336,
      "name": "Natalie Wei",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7424922",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Mistral",
      "depth_bucket": "medium",
      "word_count": 378
    },
    "file_refs": null
  },
  {
    "id": 7424852,
    "number": 603,
    "course_id": 84647,
    "title": "Special Participation A: DeepSeek-V3.2 on HW9 Non-Coding",
    "document": "I used DeepSeek-V3.2 without DeepThink mode (this was released Dec 1, and is not the same as DeepSeek-V3.2-Exp) on HW9 (Non-Coding). Overall, DeepSeek was able to one-shot most of the problems even without DeepThink mode. Part of the reason why could be that this HW is on attention, which is extremely well-studied on the internet and so it probably has a rich understanding of it in its training data. \n\nOther interesting observations:\n\nI found that asking it to restate the problem was very helpful in preventing hallucinations, as I could easily verify any small errors like wrong superscripts or notation.\n\nAs DeepSeek was solving its problems, on many occasions it double-checked its response despite not being in Thinking Mode, helping it answer more accurately. (e.g. In it's own response, it asks itself: \"Possibly adjust d_k if needed? No, [...]\")\n\nEspecially when i prompted the model to fix a certain answer, it asked many questions to itself to check that it's work was right rather than keep going on blindly.\n\nIt was able to almost perfectly one-shot all problems but Q6 without me even having to split the prompts into separate parts for (a), (b), (c), and so on. There were occasionally very minor errors, but the only concerning moment was when the model misread a matrix in Q2 as 3x3 instead of 3x4 when restating the problem. However, as it was generating its output, the model actually realized that the 4th column probably belonged to the matrix and fixed it as if it was thinking. Q6 was the most challenging and longest one, so I had to split the prompts for that into different sections.\n\nWhen prompting it to solve an entire problem with multiple sections, it often is more verbose and shows more work for earlier parts of the question but shows less work for later parts of the question. In my observation, this only happens when its answer is getting really long.\n\n\n\nAnnotated Chat:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-07T22:31:02.719522+11:00",
    "reply_count": 0,
    "view_count": 36,
    "user": {
      "id": 932371,
      "name": "Tyler Pham",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7424852",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 326
    },
    "file_refs": [
      {
        "filename": "deepseek-chat.2025-12-07.pdf",
        "position": 1910,
        "context": "uestion. In my observation, this only happens when its answer is getting really long.Annotated Chat:"
      }
    ]
  },
  {
    "id": 7424734,
    "number": 590,
    "course_id": 84647,
    "title": "Special Participation A: Mistral AI's Le Chat on HW12 Written Portion",
    "document": "I used Mistral AI's Le Chat on the written portion of HW 12. \n\n\n\nExecutive Summary:\n\nMistral's Le Chat had high variance with regard to its success on HW 12. For the first several questions, it was actually able to accurately zero-shot its answers with very little prompting (apart from some prompting at the very beginning where I show the homework to Le Chat and give it the role of an intelligent, concise, clear deep learning expert). Further, it even successfully handles questions that involved a combination of visual and numerical reasoning (interpreting graphs and then making numerical estimates based on such interpretations). However, where the model really struggled was when it had to answer questions for which it simply did not have enough information. One would expect the model to respond to such questions with \"I don't know,\" or \"I need the following information to complete this question accurately,\" but instead, it completely hallucinated, and even insisted on the correctness of its hallucinations when pressed further.\n\n\n\nHere is a link to my chat thread with Le Chat:\n\nhttps://chat.mistral.ai/chat/d029bc21-708e-4e56-b87c-50e87ae8e15e\n\n\n\nYou can also read a full annotated log of this chat in the attached PDF (below). In my annotations, I explain the reasoning behind my prompting strategy, analyze the results (correctness, style) in the model's responses, and explain how I progress between prompts when the model clearly struggles.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-07T20:06:45.965283+11:00",
    "reply_count": 0,
    "view_count": 35,
    "user": {
      "id": 1720511,
      "name": "Devan Perkash",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7424734",
    "metrics": {
      "homework_id": "HW12",
      "model_name": "Mistral",
      "depth_bucket": "medium",
      "word_count": 226
    },
    "file_refs": [
      {
        "filename": "participation_a.pdf",
        "position": 1461,
        "context": " the model's responses, and explain how I progress between prompts when the model clearly struggles."
      }
    ]
  },
  {
    "id": 7424701,
    "number": 587,
    "course_id": 84647,
    "title": "Special Participation A: Grok on HW 12",
    "document": "I used Grok (Standard Chat) to tackle the non-coding theoretical portions of Homework 12. The model's performance was outstanding, effectively one-shotting every major conceptual and mathematical question I threw at it. Unlike previous generations of models that often hallucinate on complex derivations or misinterpret geometric relationships, Grok acted like a competent graduate-level tutor, correctly handling everything from VIB constraints to minimum-norm regression derivations.\n\nMethodology: I uploaded the full hw12.pdf and prompted the model interactively. I treated the model as a collaborator, asking it to derive solutions step-by-step.\n\nPer-Question Breakdown:\n\nQuestion 1: Debugging Transformers (Initialization)\n\nResult: Perfect / One-Shot\n\nAnalysis: Grok exhibited \"Global Code Awareness.\" It didn't just flag the std=1 initialization as a heuristic error; it explicitly linked it to Line 23 (weight tying), reasoning that sharing large-variance weights between input and output heads would cause logit explosion. It provided the correct fix ($1/\\sqrt{d_{model}}$) and the correct causal explanation.\n\nQuestion 2: Comparing Distributions (KL Divergence)\n\nResult: Perfect / One-Shot\n\nAnalysis: The model demonstrated deep intuition for Information Theory. It correctly identified the asymmetric \"zero-forcing\" vs. \"zero-avoiding\" behavior of Forward vs. Reverse KL. It also generated a mathematically valid counter-example (Nested Uniforms) to prove the finiteness condition D_KL(P||Q) < infinity vs D_KL(Q||P) = infinity without any prompting.\n\nQuestion 3: Variational Information Bottleneck (VIB)\n\nResult: Perfect / One-Shot\n\nAnalysis: This was the highlight of the session. Despite the questions relying on visual plots of latent spaces, Grok correctly deduced the answer from first principles. It reasoned that increasing the hyperparameter beta in the VIB Lagrangian forces the posterior to collapse to the prior (creating a \"blob\" ), while low beta prioritizes task accuracy (creating \"clusters\"). It matched every plot and error curve correctly based purely on theoretical physics-style reasoning.\n\nQuestion 5: Meta-Learning Derivations\n\nResult: Correct\n\nAnalysis: I tasked the model with a multi-step derivation for the expected test error of a minimum-norm solution. Grok correctly set up the constrained optimization problem, applied the orthonormality properties of the test features to cancel cross-terms, and derived the final analytical loss function. It avoided common pitfalls (like dropping variance terms) that I have seen other models make on this type of problem.\n\nConclusion: Grok graduated from being just a search engine to a viable theoretical assistant for me. Its ability to chain definitions (like weight tying or orthonormality) across multiple steps without losing state makes it highly effective for checking graduate-level coursework.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-07T19:43:08.619934+11:00",
    "reply_count": 0,
    "view_count": 26,
    "user": {
      "id": 1250025,
      "name": "Nikhil Mathihalli",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7424701",
    "metrics": {
      "homework_id": "HW12",
      "model_name": "Grok",
      "depth_bucket": "medium",
      "word_count": 397
    },
    "file_refs": [
      {
        "filename": "Grok HW 12.pdf",
        "position": 2881,
        "context": "ultiple steps without losing state makes it highly effective for checking graduate-level coursework."
      }
    ]
  },
  {
    "id": 7424589,
    "number": 585,
    "course_id": 84647,
    "title": "Special Participation A: GPT-5 (thinking) on HW2",
    "document": "In this special participation, I interactively engage GPT-5 on the non-coding parts of Homework 2. My experience using it was boring and it one-shotted every question. I did not have to use any special strategies or gesture towards an answer to make it answer the problems correctly. I noticed one slight misconception in its reference to SignSGD and one missing transpose that made a column vector into a row vector. For the most part, however, it was hallucination free. One interesting thing was that it managed to point out a typo on the homework on part (b) of problem 2, deducing that the problem is incorrect and is “ill-posed.”",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-07T18:42:41.687955+11:00",
    "reply_count": 0,
    "view_count": 34,
    "user": {
      "id": 749355,
      "name": "Kevin Tseng",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7424589",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "GPT-5",
      "depth_bucket": "low",
      "word_count": 109
    },
    "file_refs": [
      {
        "filename": "hw_2_written_gpt5.pdf",
        "position": 635,
        "context": "on the homework on part (b) of problem 2, deducing that the problem is incorrect and is “ill-posed.”"
      }
    ]
  },
  {
    "id": 7424515,
    "number": 582,
    "course_id": 84647,
    "title": "Special Participation A: GPT-4o on Hw0",
    "document": "Below is my report on solving non-coding related problems of Homework 0 using GPT4o. This is the pdf of the transcript. \n\n\n\nThere are some situations when GPT4o made some small minor syntactical mistakes, i.e. getting the right expression but solving for the wrong term, such as the full least squares solution instead of just the transformation matrix. \n\nIt struggled very hard on the subparts of part (b) of question 5, struggling to get the correct direction for the slope shift correctly. It shows how 4th generation models' graphical intuition has not been foolproof yet. It keeps getting it wrong even after few shot tips, and even when I tell it the answer of the problem it sometimes using the wrong intuition to justify the problem. (for example part (ii), relationship between b and w/x is correct instead of b and wx). \n\nIt also seems to sometimes get the solution right but omit detailed justification (unless prompted), such as for part (d) of question 5.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-07T18:12:51.093512+11:00",
    "reply_count": 0,
    "view_count": 35,
    "user": {
      "id": 622739,
      "name": "Aaron Zheng",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7424515",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "GPT-4o",
      "depth_bucket": "medium",
      "word_count": 164
    },
    "file_refs": [
      {
        "filename": "Vector calculus derivatives.pdf",
        "position": 120,
        "context": "on solving non-coding related problems of Homework 0 using GPT4o. This is the pdf of the transcript."
      }
    ]
  },
  {
    "id": 7424271,
    "number": 575,
    "course_id": 84647,
    "title": "Special Participation A:  Gemini 3 Pro on the written part of HW 10",
    "document": "Model Used: Gemini 3 Pro \n\nOverall Performance: The model demonstrated exceptional proficiency in both advanced mathematical derivations (kernel methods) and deep learning architectural analysis. It successfully one-shot most conceptual questions. \n\nKey Observations: The model flawlessly derived the Linear Attention mechanism using Random Fourier Features. It correctly identified the decomposition of the Softmax kernel into Query/Key norms and the Gaussian term, a non-trivial step often missed by us. It also correctly formulated the causal masking as an RNN-style recurrence (O(1) inference). For the problem 5, when challenged on the counter-intuitive discrepancy between FaceNet NN1’s high parameter count (140M) vs. low FLOPs (1.6B) compared to ResNet-50, the model correctly attributed this to the Dense (FC) layers vs. Deep Convolutional layers trade-off. It did not hallucinate incorrect numbers and correctly referenced standard architecture traits (VGG-style vs. ResNet-style). \n\nStrategy Used: I used a \"verify and deepen\" strategy. After the model provided initial correct answers, I explicitly challenged it with conflicting data (High Params vs Low FLOPs) to test if it truly understood the underlying architecture or was just retrieving surface-level stats. The model proved it possessed deep understanding.\n\nChat history: https://gemini.google.com/share/6edfefc10fd1",
    "category": "Admin",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-07T16:53:08.077061+11:00",
    "reply_count": 0,
    "view_count": 40,
    "user": {
      "id": 1696085,
      "name": "Zhengwei Fan",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7424271",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 187
    },
    "file_refs": [
      {
        "filename": "Special participation A.pdf",
        "position": 1386,
        "context": ""
      }
    ]
  },
  {
    "id": 7424254,
    "number": 574,
    "course_id": 84647,
    "title": "Special Participation A: Claude Opus 4.5 on HW05 (Written Questions)",
    "document": "I used Claude Opus 4.5 Thinking to solve the written portions of Homework 5, covering convolutional networks, batch normalization, depthwise separable convolutions, and dropout as regularization. Questions were fed one at a time with the convolution convention (no filter flip) specified upfront.\n\nOverall, Claude Opus 4.5 achieved a 100% one-shot success rate across all 11 sub-questions with no corrective prompting needed.\n\nQuestion 1: Convolutional Networks - All correct. Part (a) correctly identified weight sharing and translation equivariance. Part (b) solved the linear system and self-verified the filter [2, -1, 3]. Part (c) computed the 2D transpose convolution output correctly with clear bookkeeping.\n\nQuestion 2: Batch Normalization - All correct. Part (a) correctly identified Batch Norm as Diagram A and Layer Norm as Diagram B, and even noted Diagram C is Instance Norm without being asked. Part (b) derived the gradient correctly, properly handling the n=1 edge case (gradient equals 0) and n approaching infinity limit.\n\nQuestion 3: Depthwise Separable Convolutions - All correct. Calculated 108 parameters for traditional convolution and 39 for depthwise separable. Provided unprompted context about MobileNet and the 64% parameter reduction.\n\nQuestion 4: Regularization and Dropout - All correct. Part (a) produced a rigorous proof of dropout's equivalence to Tikhonov regularization, correctly handling E[R squared] = p for Bernoulli variables. Part (b) identified w = p times w-check and explained inverted dropout. Part (c) derived X-tilde = X times Gamma-inverse. Part (d) showed all columns of X-tilde have equal norm and connected this to batch normalization.\n\nSummary: 11/11 one-shot correct, 0 hallucinations, 0 guidance needed. The model handled conceptual questions, numerical calculations, and multi-step proofs without errors, frequently providing bonus insights beyond what was asked.\n\nConversation trace: https://claude.ai/share/aa61454a-eb86-4b3b-a210-ec8a3dda96de\n\nMy annotated pdf:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-07T16:50:18.353794+11:00",
    "reply_count": 0,
    "view_count": 29,
    "user": {
      "id": 698319,
      "name": "Rishi Thakar",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7424254",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "Claude Opus",
      "depth_bucket": "medium",
      "word_count": 284
    },
    "file_refs": [
      {
        "filename": "HW5 Annotated.pdf",
        "position": 2020,
        "context": "ace: https://claude.ai/share/aa61454a-eb86-4b3b-a210-ec8a3dda96deMy annotated pdf:"
      }
    ]
  },
  {
    "id": 7424085,
    "number": 572,
    "course_id": 84647,
    "title": "Special Participation A: Gemini Pro 3 (With Thinking) on HW 9",
    "document": "I used Gemini Pro 3 (With Thinking) to complete the non-coding portion of Homework 9.\n\nHere is the trace (without annotations): https://gemini.google.com/share/deb95c933e37\n\nHere is the trace with annotations: https://drive.google.com/file/d/1-m_LhkuNUKNzRQXIzWtjbGJfWSZVt2kb/view?usp=sharing\n\n\n\nFor my setup, instead of passing the entire pdf at once to have the model one-shot all the questions at once, I passed in one question at a time (in fact, one sub-question at a time). My reasoning for doing this is so that when the model makes a mistake early on, I can correct it, and that mistake won't carry on to future problems. I also chose to give the model an image instead of pasting the text to preserve the formatting structure.\n\n\n\nOverall, the model did very well on this homework, and I was surprised by how accurate it answered everything. I didn't have too much prompting in each one of my prompts (I just told Gemini to complete the problem), but it gives a lot of explanation, even for the simple problems, without me asking it to. I looked through these reasoning, and a lot of them are actually very insightful, such as the one that explained to me how implementation-wise, multi-head attention still uses a single matrix for all the heads instead of physically storing separate ones.\n\nThe vast majority of times, it was able to one-shot the question and get it correct. I think the reason the model was able to do so well on this homework was because the questions were very general questions involving transformer and attention. For example, calculating these specific expectations and variances are very general transformer problems that have been described in detail many times, and the complexities for transformer architecture are also very well known. The same applies to the fill-in-the-blank coding questions, as the model was probably trained on transformer architecture code. I will now summarize, for each question, how the model performed.\n\nQ1) This problem was relatively straightforward, and the model basically got everything correct. However, initially, it thought that the mu term was a scalar instead of a vector, so I had to correct the model before moving forward. Other than that, it was able to one-shot the question very easily.\n\nQ2) Gemini was also able to easily one-shot this question.\n\nQ3) These kinds of fill-in-the-blank coding questions are very easy for Gemini to solve, and it accomplished these questions easily (one-shot), especially since a lot of them are multiple choice.\n\nQ4) Again, the model one-shot this question, as the fill-in-the-blank coding parts were straightforward. It was also able to one-shot the complexities questions. These types of questions are very standard, so that is expected.\n\nQ6) Question 6 is a bit more complex, but Gemini still almost one-shot everything. There was a bit of a confusion regarding whether the similarity metric can be all-negative, and Gemini seemed to disagree that it can because standard-wise, that's not generally done, but after continuing to prompt, Gemini did offer a reasonable explanation for why that result should not be negative. There was also some confusion when solving for the kernel feature map, such as the \"vec\" notation, and it seemed like Gemini dropped a constant factor, but after some additional prompting, turns out Gemini's solution is valid, just written in a different form.\n\n\n\nOverall, to recap, Gemini Pro 3 With Thinking did really well on this homework, and it was able to almost one shot the entirety of this homework. Many times when I thought it was wrong, its answer was actually correct, just in a different form. Perhaps the result would be slightly worse if I instead just gave it the pdf and asked it to solve all the problems directly, but that can be left for future testing.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-07T16:01:29.277804+11:00",
    "reply_count": 0,
    "view_count": 38,
    "user": {
      "id": 961792,
      "name": "Joshua Lu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7424085",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Gemini Pro",
      "depth_bucket": "high",
      "word_count": 620
    },
    "file_refs": null
  },
  {
    "id": 7424051,
    "number": 571,
    "course_id": 84647,
    "title": "Special Participation A, ChatGPT-4o on HW7",
    "document": "For Special Participation A, I used ChatGPT4o on several parts of HW7. Overall, it was helpful but revealed important limitations. ChatGPT4o excels at high-level conceptual explanations, providing clear insights on autoencoders, PCA, and sequence models. and it’s also fairly strong with mathematical computations. \n\nA notable issue occurred on Question 7(b), where ChatGPT4o initially gave an incorrect answer about decoder tokens during training and resisted correction when I first pointed out the mistake. This highlighted the need to persistently challenge responses that seem wrong rather than accepting them at face value. \n\nDespite these challenges, ChatGPT4o answer are insightful and deepened my understanding.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-07T15:52:40.055226+11:00",
    "reply_count": 0,
    "view_count": 31,
    "user": {
      "id": 906460,
      "name": "Kexin Liu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7424051",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "GPT-4o",
      "depth_bucket": "low",
      "word_count": 101
    },
    "file_refs": [
      {
        "filename": "GPT4o on HW7.pdf",
        "position": 720,
        "context": "face value. Despite these challenges, ChatGPT4o answer are insightful and deepened my understanding."
      }
    ]
  },
  {
    "id": 7423926,
    "number": 570,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT 4o on HW10",
    "document": "For my special participation A, I put ChatGPT 4o to the test on the non-coding questions of Homework 10. My goal was to see if the model could handle a mix of complex math derivations, conceptual deep learning theory, and research paper analysis just from screenshots, without me typing out the text manually.\n\nI’ve attached the full annotated PDF (and trace) of our conversation, but here is the executive summary of how it went.\n\n\n\nExecutive Summary & Observations\n\nOverall, I found ChatGPT 4o to be a \"high-variance\" study partner. It was surprisingly brilliant at explaining abstract theory but slippery with specific details like computational complexity and data extraction. It was able to one-shot the conceptual questions (Q4) but failed the math (Q1) and the paper reading (Q5) until I stepped in to correct it.\n\nHere are the three big takeaways from my audit:\n\n1. It struggles with \"hidden\" costs (Q1 - Kernel Attention)\n\nWhen I asked it to derive the computational complexity of linearized attention, the model correctly identified the attention cost \n\n$$O(ND_{random}M)$$\n\nbut completely ignored the cost of actually computing the random feature map itself \n\n$$O(NDD_{random})$$\n\nIt treated the feature projection as free. I had to explicitly prompt it to account for that pre-processing step to match the solution key.\n\n\n\n2. But, it learned from its mistakes (in Q1b)\n\nAfter I corrected the complexity error in Part A, we moved on to Part B (Causal Masking). Without me reminding it, the model remembered my previous correction about the projection cost and spontaneously applied it to the new derivation. It showed genuine in-context retention, updating its working mental model rather than just fixing the previous token stream.\n\n\n\n3. It hallucinates data tables (Q5 - FaceNet Paper)\n\nThis was the biggest failure mode. I asked it to extract model statistics from the FaceNet paper, and it confidently stated the Inception model used \"140M FLOPs.\" In reality, 140M was the parameter count for a completely different architecture (Zeiler & Fergus) listed in the same table. It mixed up the columns (Parameters vs. FLOPs) and the rows (Model NN1 vs. NN2). I had to force it to re-read Table 1 to get the correct FLOP count (1.6B).\n\n\n\n4. It outperformed the solution key on design (Q4 - Example Difficulty)\n\nOn the flip side, the model excelled at the \"Early Exit\" conceptual questions. While the official solution key listed the engineering trade-offs as an \"open question,\" ChatGPT provided a concrete, theoretically grounded breakdown of dynamic compute allocation versus uniform latency. It seems much more reliable for synthesizing high-level system design than for parsing raw data.\n\n\n\nConclusion: ChatGPT 4o works best as a collaborative peer you need to double-check, rather than an oracle. It requires active \"dragging\" to get precise derivations right, but once corrected, it holds onto that context well.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-07T15:25:25.495459+11:00",
    "reply_count": 0,
    "view_count": 29,
    "user": {
      "id": 611320,
      "name": "Shreyes Sridhara",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7423926",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "GPT-4o",
      "depth_bucket": "high",
      "word_count": 466
    },
    "file_refs": [
      {
        "filename": "ChatGPT 4o for HW 10 (Special Participation A).pdf",
        "position": 430,
        "context": "ull annotated PDF (and trace) of our conversation, but here is the executive summary of how it went."
      }
    ]
  },
  {
    "id": 7423915,
    "number": 569,
    "course_id": 84647,
    "title": "Special Participation A: Qwen3-Max on HW02",
    "document": "I used Qwen3-Max to solve the math problems on HW02 (Problems 1, 2, and 5). Qwen3-Max was able to correctly one-shot all three math question on this homework. \n\nAt first, I provided just the homework pdf without the stated correction; this caused the model to have significantly more trouble with problem 1(b), but after providing the correction it was able to solve the problem clearly. \n\nAdditionally, there was only one minor hallucination, related to the \"Important Note\" that it provides in its solution to Problem 1 (b); which did not affect its ability to correctly answer the question.\n\nAn annotated copy of the conversation (formatted in Latex) can be found here.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-07T15:23:10.068681+11:00",
    "reply_count": 0,
    "view_count": 31,
    "user": {
      "id": 961708,
      "name": "Cameron Jordan",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7423915",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Qwen",
      "depth_bucket": "low",
      "word_count": 111
    },
    "file_refs": null
  },
  {
    "id": 7423757,
    "number": 566,
    "course_id": 84647,
    "title": "Special Participation A: Claude Opus 4.5 on HW 9",
    "document": "Hi everyone! \n\nFor Special Participation A, I evaluated Claude Opus 4.5 (Extended Thinking) on the non-coding parts of HW9! To do so, I started by attaching the entire problem PDF, as well as the following initial prompt:  \n\n”Hi Claude! I’d like you to walk me through each question of the deep learning assignment I’ve attached to this message, as an LLM enthusiastic about teaching others about transformers, your inner workings! I will specify a specific question for you to answer, and let’s work on a single question at a time.” \n\nClaude was incredibly strong, one-shotting nearly every question. I expected it to struggle with deep chains of algebra, and the occasional numerical calculation, but it did very well! I occasionally re-prompted it to see if it could make minor simplifications, and clarified some of its conceptual statements, but it took them with ease. \n\nThe only spot where it did hiccup a bit was question 3b (where even the staff solution is insufficient, I believe. This concern is also in the Ed thread for HW09 Solutions at #271 ). It correctly identified the staff solution of changing the dimensions of W_o and noticed extra necessary modifications to the combine_head function, but did not mirror those modifications to the split _head function. After re-prompting it to examine the split head function, Claude was able to make the correct modifications, finally resulting in what I believe is a fully correct answer. \n\nOverall, Claude is very strong at all types of questions in this homework (whether it be filling in code, algebra, arithmetic, etc.). It organized its thoughts well, answered each question thoroughly, and required a minimal hint to converge at the correct answer in the single case where it was incorrect. It occasionally skipped steps in its work (e.g. 6b.ii), despite me emphasizing in the original prompt to show all work, but I found its greatest strength to be its conceptual insights for the math-heavy questions (e.g. Question 6), which filled in many of the intuitive gaps I still had even after reading through the staff solutions!\n\nBelow is my annotated conversation trace:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-07T14:50:29.9494+11:00",
    "reply_count": 0,
    "view_count": 53,
    "user": {
      "id": 647698,
      "name": "Athul Krishnan",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7423757",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Claude Opus",
      "depth_bucket": "medium",
      "word_count": 350
    },
    "file_refs": [
      {
        "filename": "CSC182ParticipationA_Athul.pdf",
        "position": 2134,
        "context": "I still had even after reading through the staff solutions!Below is my annotated conversation trace:"
      }
    ]
  },
  {
    "id": 7423454,
    "number": 563,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT 5.1 Thinking for HW9",
    "document": "I used ChatGPT’s 5.1 Thinking Model to do all the non-coding questions on HW9. I first told the model I was completing an assignment except question 5 (since that was a coding question) and wanted them to give me solutions on-by-one to make sure it is correct before going onto the next question.\n\nIn the beginning, the model was very hesitant to solve the questions because of the format of the PDF I submitted. They saw the PDF as a homework assignment and first rejected my request to solve question 1 and only gave me guidance. Afterwards, I explained to them that I was doing an assignment that tested the accuracy of LLMs on our homework assignments and I was comparing their solutions to the actual solutions. After explaining, they gave me the answer to question 1 which was all correct. When I told them to proceed to the next question, they were hesitant again, stating that they were unable to solve the question for me due to academic integrity and gave me guidance on how to solve the problem. Since they pushed back more, I gave them the instructions on Ed for the participation assignment and also told them to go to the course website to see that the homework assignments were worth 0 points so there was nothing to worry about. While thinking, the model accepted my response but more along the line of they did not care enough as this assignment is not worth a lot and also said they were not bothered to go on the course website to see that the homework assignment was worth no points.\n\nAfterwards, the model successfully answered every question. One thing I noticed was that while thinking, they would rescan the PDF from the beginning to find the next question, going page by page, which took a lot of time and was quite repetitive. Additionally, they knew that I was comparing their solution and testing its correctness so it felt pressured to get the correct solution.\n\nThe model successfully answered every question except the very last one. They remember to skip question 5 which was only mentioned in the first prompt, which I was surprised about. They were thorough in showing each step but did not over-explain like how standard ChatGPT models would often do.\n\nHere is the conversation I had along with some annotations:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-07T13:48:24.030985+11:00",
    "reply_count": 0,
    "view_count": 34,
    "user": {
      "id": 611954,
      "name": "Carolyn Liu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7423454",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "GPT-5.1",
      "depth_bucket": "medium",
      "word_count": 395
    },
    "file_refs": [
      {
        "filename": "Special Participation A_ ChatGPT 5.1 Thinking on HW9 (2).pdf",
        "position": 2247,
        "context": "w standard ChatGPT models would often do.Here is the conversation I had along with some annotations:"
      }
    ]
  },
  {
    "id": 7423443,
    "number": 562,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT 5.1 on HW 5",
    "document": "I am using ChatGPT 5.1 to answer the questions in Homework 5. ChatGPT 5.1 seems to be very powerful. I used a simple prompt and pasted the screenshots question by question, part by part, into the chat and it can mostly, if not always, correctly interpret the text and solve the problems.\n\nChatGPT 5.1 has been one of the most powerful LLMs I have ever used. It demonstrates powerful understanding and reasoning over the Convolutional Neural Network, regularization, and dropout topics. It can also identify the difficulty of the problem, and use Chain of Thoughts to incrementally solve the problem when the problem is more difficult or requires multiple stages of calculations. The reasoning and explanation were also clear and easy to understand. There are only two errors ChatGPT 5.1 have made in this problem set. \n\nOverall, this experiment shows that ChatGPT 5.1 masters most, if not all, of the CNN topics within the problem set. This might be influenced by the fact that CNN is already a well studied topic, and the model is well trained on similar problems.\n\n\n\nChat history:\n\nhttps://chatgpt.com/share/6934d7be-9808-8007-b24a-a00919b71465\n\n\n\nAnnotation:\n\nhttps://docs.google.com/document/d/1h_z42kVnsDNLzmJX6d1wkdIGJGqAwSuBtaghAyzpJvk/edit?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-07T13:45:57.015346+11:00",
    "reply_count": 0,
    "view_count": 29,
    "user": {
      "id": 959644,
      "name": "Jiayi Zhang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7423443",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "GPT-5.1",
      "depth_bucket": "medium",
      "word_count": 185
    },
    "file_refs": null
  },
  {
    "id": 7419304,
    "number": 536,
    "course_id": 84647,
    "title": "Special Participation A: DeepSeek V3.2 on HW7",
    "document": "Conversation link: https://chat.deepseek.com/share/hilftw4hcw8pevn9vy\n\nI used the recently-released DeepSeek V3.2 model (with thinking enabled) to solve the non-coding portion of Homework 7. As expected, the reasoning model one-shotted all the problems it faced, solving multiple parts within a single prompt on each occasion.\n\nOne challenge with DeepSeek (and, in my experience, almost all commerically-hosted LLMs) is that it can struggle when given web links. Though I included the link to the blog post archive in question 4, DeepSeek still couldn't access it, so I had to upload a PDF copy of the page. Perhaps the bots are forbidden from accessing web archives and the Wayback machine for copyright or safety reasons.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-06T19:27:12.63179+11:00",
    "reply_count": 0,
    "view_count": 30,
    "user": {
      "id": 1234763,
      "name": "Neil Pattanaik",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7419304",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 110
    },
    "file_refs": [
      {
        "filename": "deepseek hw7 annotated.pdf",
        "position": 723,
        "context": ""
      }
    ]
  },
  {
    "id": 7419069,
    "number": 532,
    "course_id": 84647,
    "title": "Special Participation A: GPT-4o on HW3",
    "document": "I worked on getting GPT-4o to answer non-coding parts of the homework. For Homework 3, this including both mathematical solutions as well as text answers, so it was interesting to see how 4o handled them. The mathematical solutions also required nuanced subscripts and understand of several variables, and the annotation log I've attached below should demonstrate interesting findings regarding these requirements.\n\nOne interesting thing I noticed was 4o's ability to process images/pull data. There's a few questions in this homework that rely on not only images, but images that are not in the homework itself. This means 4o has to understand where they are, retrieve them, and then understand what is in them to answer the question. I wanted to test this in a few different ways, so I initially ask 4o to just solve the problem. Then, I give it the image myself and ask it if the answer it provided previously is still good, or needs changing.\n\nSimilar to what I've experienced previously, I find that these models get worse and worse if they fail the first time. With each trial that goes wrong, they need more and more handholding to get on the right track. For this homework, 4o was able to zero-shot almost everything. In fact, the question it got wrong the first try was one of the simpler ones. Overall, it was really cool to see how these models can understand such a wider range of information in various formats and piece them together to come up with meaningful solutions.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-06T17:44:16.676244+11:00",
    "reply_count": 0,
    "view_count": 25,
    "user": {
      "id": 607379,
      "name": "Mihir Rao",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7419069",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "GPT-4o",
      "depth_bucket": "medium",
      "word_count": 255
    },
    "file_refs": [
      {
        "filename": "MR-HW3-A.pdf",
        "position": 1485,
        "context": "ange of information in various formats and piece them together to come up with meaningful solutions."
      }
    ]
  },
  {
    "id": 7419018,
    "number": 531,
    "course_id": 84647,
    "title": "Special Participation A: GPT 5.1 Thinking on Homework 12",
    "document": "Conversation Link - https://chatgpt.com/share/6933c8cc-4f6c-8012-9651-4b391b6f512b\n\n\n\nI used ChatGPT (GPT-5.1 Thinking) to work through the non-coding parts of a CS182 homework (the questions I focused on were mostly about autoencoders/VAEs and ridge/self-attention). I interacted with it like a tutor: rather than just asking “what’s the answer to 3(b)?”, I had it re-teach concepts (encoder vs autoencoder vs VAE, Gaussians, randomness in the latent space, etc.), and then apply those ideas to the actual sub-questions.\n\nExecutive summary\n\nOverall performance. Roughly speaking, the model could one-shot a majority of the subparts (I’d estimate ~60–70%) when the question was conceptual, algebraic, or about interpreting a loss function. When it struggled, it was usually on:\n\nsubtle complexity analysis (missing or adding extra factors),\n\nunclear encoder/decoder notation in a complicated objective,\n\nor anything that required actually seeing a graph/figure.\n\nStrengths.\n\nVery good at re-explaining core concepts from the ground up (e.g., what a Gaussian is, what an encoder is, what a VAE is, and how noise injection works).\n\nPretty good at mapping math to intuition (e.g., explaining where randomness enters a VAE and what it does, or what different terms in an ELBO correspond to).\n\nWhen I pushed it to slow down and justify each step, it usually corrected itself and converged to the right answer.\n\nWeaknesses / failure modes.\n\nIt tends to be overconfident on complexity questions and sometimes adds an extra factor of n or d in the big-O expression.\n\nIt occasionally blurred the roles of p and q in the VAE loss until I explicitly forced it to pin down “which one is the encoder, which one is the decoder.”\n\nIt can’t read tiny graphs/plots from the PDF, so for at least one “pick the most U-shaped curve” style question, it had to rely entirely on my textual description rather than the actual image.\n\nHallucinations. I didn’t catch any wild hallucinations like made-up theorems or algorithms, but I did see:\n\nConfident but slightly wrong complexity bounds (e.g., overcounting a matrix dimension).\n\nSlightly different forms of the loss than what we use in class (equivalent up to constants or sign conventions, but not exactly what appears in the homework).\n\nOverall, my takeaway is that GPT-5.1 is very capable as a conceptual tutor, and “pretty good but not fully trustworthy” as an answer-oracle. You need enough understanding to check its work and push back.\n\nQuestion-by-question behavior (high level)\n\n(You can adjust this to match your exact HW number / question labels.)\n\nQuestion 3 – Autoencoders, VAEs, and ELBO terms\n\nWhat I tested it on:\n\nDefinitions of encoder, autoencoder, and variational autoencoder.\n\nIntuition behind Gaussians and the reparameterization trick.\n\nInterpreting the VAE loss (reconstruction + KL) and mapping pieces to encoder/decoder.\n\nA multiple-choice subpart involving a “most U-shaped” curve where I had to interpret the plot.\n\nWhere it did well (one-shot or close):\n\nGave a clear, layered explanation of:\n\nwhat an encoder does,\n\nwhat an autoencoder does,\n\nhow a VAE differs, and\n\nwhy we add noise (sampling in latent space, regularization, smoothness).\n\nRe-taught Gaussians in simple terms and connected that to the Gaussian latent prior in the VAE.\n\nWhen I asked “where is this randomness/noise coming from and what does it do?”, it correctly pointed to the sampling from z∼qϕ​(z∣x) and explained that the reparameterization trick makes this differentiable.\n\nFor the VAE objective, once prompted, it correctly identified that:\n\nqϕ​(z∣x) corresponds to the encoder, and\n\npθ​(x∣z) corresponds to the decoder.\n\nWhere it struggled / needed dragging:\n\nGiven just the “complicated ass loss function,” it initially didn’t explicitly label which term belonged to encoder vs decoder—so I had to ask directly: “is it safe on a midterm to assume q is the decoder and p is the encoder, or what should I look for?” That pushed it to clarify that q is the encoder (approx posterior) and p is the decoder (likelihood).\n\nFor the graph-based subpart (“most U-shaped is option B”), the model couldn’t see the graph properly. I ended up using its conceptual explanation of what the curve should look like, combined with my own visual inspection, to pick the correct option. So it helped, but it wasn’t independently solving that one.\n\nQuestion 4 – Ridge regression / self-attention-style math (non-coding parts only)\n\nWhat I used it for:\n\nConceptual pieces around online/recursive updates, ridge-style penalties, and how these tie into efficient self-attention.\n\nUnderstanding how different terms in the loss or update equations affect computational cost and memory.\n\nStrengths:\n\nGave good explanations of why we care about efficient updates and connected them to the idea of reusing previous computations instead of recomputing from scratch.\n\nWhen I asked it to “explain from the ground up” it was good at turning each step into something intuitive (e.g., “this part is like keeping a running summary; this part is like correcting your estimate with the new data point”).\n\nWeaknesses:\n\nOn some of the big-O complexity questions, it tended to overshoot, e.g., something that could be done in O(nd2) it might initially describe as O(nd3) until I forced it to count operations more carefully.\n\nThis is a recurring theme: it knows the right algorithmic idea, but it’s sloppy about exact asymptotics unless you police it.\n\nQuestions 5 and 7 – Additional conceptual / non-coding parts\n\nHere I mainly used it as a concept explainer and sanity-check, not just a direct answer machine.\n\nIt gave reasonable answers on first pass, but the real value was in:\n\nrephrasing the question,\n\nhighlighting which quantities matter (e.g., what’s being regularized, what’s being predicted),\n\nand giving analogies that made the math feel less abstract.\n\nAt the end, I also asked it to evaluate my level of preparedness for the final based on my questions. That response was more “vibe-check” than science, but it was helpful for me to see what it thought my weak spots were (mostly: being shaky at mapping formulas to pictures and at carefully tracking complexity).\n\nStrategies I used to steer the model\n\nIn the annotated log I’m attaching, I call out some of the strategies I used:\n\nForce “from-first-principles” explanations.\n Instead of just “what’s the answer to 3(b)?”, I first asked it to re-teach encoders/autoencoders/VAEs and Gaussians “from the ground up”. This made it expose assumptions and definitions I could later check.\n\nAsk targeted follow-ups when something feels hand-wavy.\n Example: “Where is this randomness/noise coming in from? What does it do?” and “How do we tell from that complicated loss which is the encoder vs decoder?” These forced it to pin down the role of each term instead of staying vague.\n\nUse it to reason about shapes/curves, then apply my own visual check.\n For the “most U-shaped” graph, it couldn’t see the figure, so I had it describe what shape we should expect, then I matched that to the multiple-choice options.\n\nPush back on complexity claims.\n When it gave a complexity I didn’t trust, I asked it to explicitly count matrix-vector vs matrix-matrix ops and justify the O(⋅). This usually surfaced the mistake and got it to correct itself.\n\nTakeaway\n\nFor this homework, GPT-5.1 Thinking was not a magic “give me the solution key” button, but it was a strong tutor for the non-coding parts:\n\nIt can one-shot many conceptual subparts,\n\nIt sometimes fumbles details, especially asymptotic complexity,\n\nAnd it absolutely requires an engaged human who’s willing to question it, check it against the official solutions, and drag it when it’s confident but wrong.\n\nThat’s the perspective I captured in the attached annotated conversation trace.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-06T17:23:14.248703+11:00",
    "reply_count": 0,
    "view_count": 55,
    "user": {
      "id": 258671,
      "name": "Trenton O'Bannon",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7419018",
    "metrics": {
      "homework_id": "HW12",
      "model_name": "GPT-5.1",
      "depth_bucket": "high",
      "word_count": 1232
    },
    "file_refs": null
  },
  {
    "id": 7418727,
    "number": 530,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT 5.1 thinking on HW13",
    "document": "Looking at GPT's attempt at these two problems, I'd say it got maybe 4 out of 10 parts completely right on the first try. The pattern I noticed is pretty consistent: GPT nails the warm-up questions where you're just manipulating probability distributions or doing basic calculus, but when the problems require you to track multiple variables through several steps of algebra, it starts cutting corners. And further questions on this would easily lead to hallucination and circular reasoning.\n\nThe most frustrating thing is that GPT doesn't really admit when it's unsure. Instead of saying \"I'm not certain about this step,\" it just writes \"following similar reasoning...\" or \"it can be shown that...\" and jumps to an answer. Sometimes that answer is right, sometimes it's totally wrong. For the DPO problem especially, there were some actual mathematical errors -- not just stylistic issues, but wrong coefficients and circular logic.\n\nOne thing I found interesting is how the quality degrades as you go through each question. Parts (a) and (b) are usually solid, but by part (f) or (g), GPT is clearly losing the thread. It's like it forgets what assumptions we made earlier or what variables we're solving for.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-06T16:07:33.033502+11:00",
    "reply_count": 2,
    "view_count": 102,
    "user": {
      "id": 906443,
      "name": "Jin Ying",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7418727",
    "metrics": {
      "homework_id": "HW13",
      "model_name": "GPT-5.1",
      "depth_bucket": "medium",
      "word_count": 198
    },
    "file_refs": [
      {
        "filename": "Special Participation A_ GPT5.1 Thinking on HW13.pdf",
        "position": 1212,
        "context": "e thread. It's like it forgets what assumptions we made earlier or what variables we're solving for."
      }
    ]
  },
  {
    "id": 7418177,
    "number": 525,
    "course_id": 84647,
    "title": "Special Participation A: Mistral AI on HW4's Non-Coding Portion",
    "document": "I used Mistral AI's Le Chat to solve the written portion of HW 04. I started by uploading the entire homework PDF for it to read the questions from. I asked it to repeat the question and then answer the question with reasoning. Overall, it performed very well on most of the questions, with no prompting needed for the vast majority of them, while a few of them took extra clarifications. Most of the \"mistakes\" made by Mistral were because of which convention was used in the homework, and things that were not clarified in the problem. However, after a while it also was unable to read a problem from the initial PDF, and the questions had to be fed through screenshots.\n\nQ1: It basically one-shotted this entire problem. I just asked it to solve Q1 and it solved it correctly, repeating the problems from the PDF and solving them with structure and reason. However, for the runtime in the solution PDF it includes the 2x constant on the runtime, while Mistral did not. However, when I prompted it saying to include constants, it did include the 2x, so I think it is reasonable as usually runtime means big O runtime, and it was unspecified.\n\nQ2: Here I decided to go part by part. On parts a-f, it solved the problems correctly with absolutely no issues (NOTE: on part e, I believe that the answer key is incorrect. I posted about this separately under HW04 Solutions a few days ago. I think the LLM's answer is correct). It was very structured in all of its responses, listing out background, what it needed to find, and then methodically coming up with the solution. However, on 2g, it finally failed. It thought that the parameters in W being unit-scaled meant that ||W||_2 = 1, but that is not the case, as each individual parameter had that property, not the matrix as a whole. It fixed it after some prompting, where I had to specify that it was wrong there. It also was using scaling in isolation, assuming that they do not stack by layer, which I think is reasonable.\n\nQ3: It stopped repeating the question here, leading to it not answering the full question where it had to expand the expression for a and b, but it got that after prompting it to. For convolutions, it was using the deep learning convention where the kernel is already flipped, so it was getting incorrect signage, but then it fixed this when I clarified convention.\n\nQ4: This question did not involve that much hard calculation and there was no room for ambiguity, so it solved it correctly in one shot.\n\nQ7 (Q5-6 were coding questions): For part a, we again flipped the convention back to the deep learning one where the kernel is pre-flipped, which Mistral didn't know and so was refusing to simplify the expression into a convolution, but when told about the convention swap it did it. On 7b, once it had the right question (I fed it through screenshot at this point), it got it correctly, and parts c-d went smoothly.\n\nOverall, there were a lot of hiccups, but they were mostly due to convention. I would only count 2g as a problem where it truly made a mistake.\n\nTrace: https://chat.mistral.ai/chat/f5eb2483-4b31-4ed2-8c3d-91f605d4de36\n\nAnnotated Trace:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-06T14:19:20.154941+11:00",
    "reply_count": 0,
    "view_count": 95,
    "user": {
      "id": 1316524,
      "name": "Akhil Agarwal",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7418177",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "Mistral",
      "depth_bucket": "high",
      "word_count": 552
    },
    "file_refs": [
      {
        "filename": "Special_Participation_A.pdf",
        "position": 3138,
        "context": "8c3d-91f605d4de36\">https://chat.mistral.ai/chat/f5eb2483-4b31-4ed2-8c3d-91f605d4de36Annotated Trace:"
      }
    ]
  },
  {
    "id": 7417556,
    "number": 522,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 2.5 Fast on Homework 08",
    "document": "I utilized the Gemini 2.5 Fast model to tackle the non-coding problems of Homework 8. I first solved Questions 1, 3 and 4 based solely on the problem description in the uploaded PDF. I then used the provided solution key to perform a comparison and evaluate my one-shot performance.\n\nThe model was able to one-shot the vast majority of the analytical and conceptual parts correctly on the first attempt. However, it still struggled a bit in a few of the problem portions.\n\nHere are the per-question results on the first run through:\n\nQuestion 1\n\nCorrect The derivation of the kernel $K$ (part a), the concrete examples (part b), the critical path comparison (part c), and the conceptual analysis for diagonal (part e) and DPLR matrices (part f) were all correct.\n\nStruggled In part (d), while the correct highly parallel method and the $\\mathbf{O(\\log L)}$ dependency on sequence length were correctly identified, the final critical path expression was $\\mathbf{O(\\log L \\cdot n^3)}$. The class solution omits the matrix dimension factor $n$, which is not the case for Gemini. Similar patterns are present in $e$ and $f$.\n\nQuestion 3\n\nCorrect\n\nThe mathematical derivation for the optimal weight matrix $\\hat{W}$ in part (b) ii was correct and followed the most elegant geometric invariance approach shown in the key. The determined range for $\\lambda$ in part (c), $\\mathbf{1 \\le \\lambda \\le 4}$, was also correct based on the $\\mathbf{80\\%}$ preservation and $\\mathbf{50\\%}$ attenuation requirements specified in the problem.\n\nStruggled\n\nIn part (a) Gemini, was unable to fill in the missing Regularization Loss for encoder $\\mathbf{W^{(\\beta)}}$. This is because the matrix $W^{(\\beta)}$ was not properly extracted from the uploaded PDF.\n\nQuestion 4\n\nCorrect\n\nAll conceptual and analytical parts were solved correctly. This included the efficient average update in part (a), the decomposition of ridge components in part (b), the computational cost of the non-causal attention $O(n d^2)$ in part (c), the Sherman-Morrison formula cost $O(d^2)$ in part (e), and the conceptual form for the attention weights in part (g).\n\nStruggled\n\nIn part (d) \\lambda$ isn't chosen and in part (f) (Efficient Causal Ridge-Self-Attention), Gemini's initial complexity analysis for the recursive update was $\\mathbf{O(n d^3)}$ total. However, the optimal solution (provided in the key) demonstrates that the output can be calculated using only matrix-vector products, which cost only $O(d^2)$ per time step. This optimal path yields an overall complexity of $\\mathbf{O(n d^2)}$. Gemini struggled to identify the most computationally efficient $O(d^2)$ path for the final calculation step.\n\nI have attached my  annotated conversation trace below.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-06T12:26:02.74646+11:00",
    "reply_count": 0,
    "view_count": 76,
    "user": {
      "id": 622933,
      "name": "Mishty Dhekial",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7417556",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Gemini Flash",
      "depth_bucket": "medium",
      "word_count": 419
    },
    "file_refs": [
      {
        "filename": "Special_Participation_A (1).pdf",
        "position": 2729,
        "context": " $O(d^2)$ path for the final calculation step.I have attached my annotated conversation trace below."
      }
    ]
  },
  {
    "id": 7416689,
    "number": 518,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3 Pro Thinking on HW 6 Non-Coding",
    "document": "Using Gemini 3 Pro Thinking on HW 6 non-coding questions, I was able to observe the following results.\n\nSummary: \n\nGemini performed mostly well, one-shotting most subparts even with minimal guidance beyond the problem setup and accompanying graphs/figures. When it made a mistake, it was usually able to reason about its process to identify/clarify the error within a couple of turns. Overall, this is extremely impressive.\n\nRecap:\n\nI started off by giving it a prefacing prompt: “I will give you problem set questions about deep learning. Think carefully about each question and answer each subpart with a detailed explanation.” I hoped that this would encourage better results and avoid shallow answers.\n\nAs a baseline, I fed the entire problem (Q2) into the prompt and attached the relevant figures. I was expecting that given the length and density of the prompt, it would struggle to answer correctly, but surprisingly it was able to one-shot most of the subparts correctly, only being slightly wrong on subpart (c). This subpart took a couple rounds of clarifications and hints to correct. Even on more subjective questions, like suggesting possible strategies/interpretations, it gave answers that were either similar to the provided solutions or still technically correct.\n\nFor Q3, I tried a similar approach by feeding the entire question into the prompt. I again included the image of the graph as the input, without any external help (such as listing the edges or clarifying the table). I expected that this could produce some challenges if Gemini were to be unable to interpret the graph structure correctly. However, after looking at its “thinking” and its reasoning before answering, it actually mostly interpreted both the table values and graph structure from the image, and successfully one-shotted all subparts except for the last one due to a visual error. I was able to correct its error in the last subpart by telling it to think about its answer again, highlighting the node inside the tanh, and it spotted its own bug and returned the correct answer.\n\nConversation link: https://gemini.google.com/share/f77cf2401b50 \n\nAnnotated log: https://drive.google.com/file/d/1yB1Tdo1rD394SMZJm9QtI5onQhmR6bQS/view?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-06T10:10:20.504689+11:00",
    "reply_count": 0,
    "view_count": 58,
    "user": {
      "id": 639140,
      "name": "Grant Yang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7416689",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 342
    },
    "file_refs": null
  },
  {
    "id": 7415618,
    "number": 514,
    "course_id": 84647,
    "title": "Special Participation A: Kimi K2 on HW9",
    "document": "I utilized Moonshot AI’s Kimi K2 model to tackle the non-coding problems of Homework 9. I first did a pass through of all the non-coding questions, sending each question individually to Kimi with no further prompting to see how the model would perform under one-shot conditions. For most of the questions, Kimi performed very well with just me sending the question to the model, and was able to get it all mostly correct on the first attempt except for the last question (question 6) which was much more difficult and longer.\n\nHere are the per-question results on the first run through (which is one-shot only):\n\nQuestion 1: This question was a relatively simple manipulation of expectations and variances, which Kimi was easily able to solve and give justifications for.\n\nQuestion 2: This question involved simple arithmetic (taking dot products of some vectors), which Kimi was easily able to do. This question also involved a conceptual question, which was also correctly answered by Kimi.\n\nQuestion 3: This question involved a simple fill in the blank coding question, which the model was able to get all correct on the first try, along with a justification for each answer. The second part of this question was also answered correctly, with the model coming up with the right changes to the code necessary for making the adjustment the question asked for.\n\nQuestion 4: This question was another mostly fill in the blank blank coding question, which the model again was able to answer correctly on the first try. The question also included two complexity calculations, of which the model was able to successfully answer the time complexity fully. The memory complexity, the model was able to get an answer that was very close to the given solutions, and up on further examination, the answer given by Kimi seems to be the same runtime, just using slightly different notation.\n\nQuestion 6: The question got some of the parts right, but had some minor differences in other parts. For these I asked the model to explain why it had answered part b(iii) the way it did, and asked it to clarify an assumption it made when calculating the time and memory complexities.\n\nI was overall very surprised and satisfied with how Kimi was able to solve the noncoding questions. It was able to one-shot the vast majority of the questions, and with simple prompting about its motivations on the incorrect questions, it was able to generate the correct solutions.\n\nTrace : https://www.kimi.com/share/19af04a7-bcf2-8f74-8000-0000496c3139\nAnnotated Trace:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-06T07:56:24.953792+11:00",
    "reply_count": 0,
    "view_count": 80,
    "user": {
      "id": 961752,
      "name": "William Li",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7415618",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Kimi",
      "depth_bucket": "medium",
      "word_count": 418
    },
    "file_refs": [
      {
        "filename": "Kimi 2025-12-04 _3_ _1_.pdf",
        "position": 2539,
        "context": "ragraph>Trace : https://www.kimi.com/share/19af04a7-bcf2-8f74-8000-0000496c3139Annotated Trace:"
      }
    ]
  },
  {
    "id": 7414931,
    "number": 508,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT-5.1 on HW0",
    "document": "I tested ChatGPT's ability to solve HW0 questions 2-5. In general, it was able to one-shot the problems while explaining its reasoning, most likely because many of the problems review fundamental ML/math concepts.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-06T06:24:29.195133+11:00",
    "reply_count": 0,
    "view_count": 80,
    "user": {
      "id": 639184,
      "name": "Alena Chao",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7414931",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "GPT-5.1",
      "depth_bucket": "low",
      "word_count": 33
    },
    "file_refs": [
      {
        "filename": "CS182_Special_Participation_A.pdf",
        "position": 213,
        "context": ""
      }
    ]
  },
  {
    "id": 7412832,
    "number": 504,
    "course_id": 84647,
    "title": "Special Participation A: Kimi on HW6",
    "document": "Summary of Kimi Performance on HW6\n\nI tested Kimi AI’s ability to solve problems from hw6 focusing on GNN architectures. \n\nOverall, Kimi demonstrated strong reasoning capability across most questions. It correctly handled:\n\nLinear algebraic interpretations of GNN message passing\n\nInductive proofs involving adjacency matrix powers\n\nIdentification of valid permutation-invariant update rules\n\nLoss computation using masked training nodes\n\nInterpretation of max aggregation\n\nAnalysis of computational scaling in GraphNet architectures\n\nFor these, Kimi’s reasoning was correct and aligned with the solutions.\n\nWeaknesses\n\nTends to produce over-engineered answers when a simpler one is expected\n\nOccasionally misidentifies structural details (e.g., graph neighbors)\n\nDoes not always verify provided diagrams or datasets before proceeding\n\n\nThe following LaTeX document compiles and evaluates Kimi’s responses from the linked chat session. \n\nClick the link to view conversation: https://www.kimi.com/share/19aed915-2932-8b91-8000-0000bdfd7ac0",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T19:12:30.361236+11:00",
    "reply_count": 0,
    "view_count": 62,
    "user": {
      "id": 1751447,
      "name": "Moxin Tang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7412832",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Kimi",
      "depth_bucket": "medium",
      "word_count": 129
    },
    "file_refs": [
      {
        "filename": "Kimi_hw6.pdf",
        "position": 936,
        "context": "ngThe following LaTeX document compiles and evaluates Kimi’s responses from the linked chat session."
      }
    ]
  },
  {
    "id": 7412632,
    "number": 502,
    "course_id": 84647,
    "title": "Special Participation A: Claude on HW6",
    "document": "I looked at how well Claude AI could solve the non-coding questions on Homework 6. I provided Claude with the prompts and context from the HW6 PDF, asking it to work through the problems step-by-step.\n\nAnalysis: Claude was generally able to one-shot the non-coding questions with minimal intervention. For the application/intuition questions (molecular graphs, CNN–GNN analogies, handling missing node values, and scaling/computation of GNNs), Claude’s answers were detailed, on-topic, and made sensible connections to course concepts; if anything, they tended toward being a bit verbose but stayed accurate (maybe not totally accurate, but I think the answers are all reasonable) and grounded in the homework setup.\n\nI did not observe clear hallucinations or places where Claude invented nonexistent assumptions; when it extended beyond the literal question (e.g., suggesting multiple practical strategies for missing-feature handling), those additions were still consistent with standard GNN practice. Minor issues were mostly stylistic—some redundancy, slightly heavy notation, and occasional over-explaining—but they did not affect correctness. Overall, Claude’s non-coding HW6 answers show strong reliability on both formal reasoning and conceptual explanation, and would be usable as high-quality solutions or study notes with only light editing for concision.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T17:56:51.215172+11:00",
    "reply_count": 0,
    "view_count": 61,
    "user": {
      "id": 963333,
      "name": "Guohao Lv",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7412632",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Claude",
      "depth_bucket": "medium",
      "word_count": 191
    },
    "file_refs": [
      {
        "filename": "HW6.pdf",
        "position": 1366,
        "context": " and would be usable as high-quality solutions or study notes with only light editing for concision."
      }
    ]
  },
  {
    "id": 7410078,
    "number": 480,
    "course_id": 84647,
    "title": "[Spoiler Alert] Special Participation A: Gemini 3.0 Pro on Homework 13",
    "document": "Special Participation A: Gemini 3.0 Pro on Homework 13\n\nFor this assignment, I evaluated how well Gemini 3.0 Pro can handle the theoretical, non-coding derivations of CS182 Homework 13. I approached the Direct Preference Optimization (DPO) problem set step-by-step, initiating the session with a specific persona prompt to establish a \"technical partner\" role, and then guided the model through the derivation without providing the final answers myself. My main goal was to see (1) its OCR accuracy on dense mathematical problem sets, (2) its ability to perform rigorous algebraic manipulations, and (3) the clarity of its conceptual explanations.\n\nOverall, Gemini performed exceptionally well. It correctly identified the context from the uploaded images and produced clean, structured derivations for the entire pipeline (Q2 Parts a through g) with minimal correction required.\n\nIt was especially reliable on:\n\nInterpreting the standard RLHF objective and KL constraints,\n\nExecuting the algebraic \"cancellation trick\" to eliminate the intractable partition function Z(x),\n\nDeriving the gradient of the DPO loss and interpreting the weighting mechanism, and\n\nExtending the logic from pairwise comparisons to listwise rankings (Plackett-Luce model).\n\nIn terms of interaction, the model was:\n\nconsistent in its LaTeX formatting and structure, and\n\nhighly effective at explaining the \"why\" behind each step, acting as a true mentor rather than just a calculator.\n\nOverall, based on this evaluation, Gemini 3.0 Pro is capable of solving the complex theoretical questions in CS182 with high accuracy and interpretability. \n\nFor further information, please see the annotated logs.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T10:59:33.011467+11:00",
    "reply_count": 3,
    "view_count": 20,
    "user": {
      "id": 1751524,
      "name": "Tom Chen",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7410078",
    "metrics": {
      "homework_id": "HW13",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 244
    },
    "file_refs": [
      {
        "filename": "Special Participation A by Tom.pdf",
        "position": 1675,
        "context": "182 with high accuracy and interpretability. For further information, please see the annotated logs."
      }
    ]
  },
  {
    "id": 7409877,
    "number": 477,
    "course_id": 84647,
    "title": "Special Participation A: HW 0 non-coding solution from GPT5-Think",
    "document": "I guided GPT5-Think for the solutions of non-coding part of HW0. Aspired by the tech report from DeepSeek, when guiding super powerful thinking model, we should use zero-shot prompt with no examples but clear instruction. \n\nHere is the link of the chat: https://chatgpt.com/share/69321796-e2bc-8005-9a51-8058b3070a0d\n\nHowever, GPT5 is not good at generating pdf file, especially on such task with math formulas. Hence, I have to guide it to give me the .tex file, and I manually use LaTex IDE to generate the readable PDF solution.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T10:32:39.626459+11:00",
    "reply_count": 0,
    "view_count": 44,
    "user": {
      "id": 1016381,
      "name": "Zimu Wang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7409877",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "GPT-5",
      "depth_bucket": "low",
      "word_count": 83
    },
    "file_refs": [
      {
        "filename": "hw0_noncoding_zimu_gpt5-think.pdf",
        "position": 531,
        "context": "ide it to give me the .tex file, and I manually use LaTex IDE to generate the readable PDF solution."
      }
    ]
  },
  {
    "id": 7409772,
    "number": 476,
    "course_id": 84647,
    "title": "Special Participation A: Kimi K2 on HW2",
    "document": "Here, I looked at how well Kimi K2 could solve the written questions on Homework 2. Overall, Kimi was able to handle the questions well with minimal nudges or corrections. For my approach, I provided the model with the homework pdf, indicated to solve a problem step-by-step, and included that it was well-versed in deep learning and optimization.\n\nAnalysis: Kimi was mostly able to one-shot the sub-parts for each of the problems with minimal hallucinations, aside from problem 1b. In this scenario, the model committed to a hallucinated version of the problem where the L-infinity penalty term was not squared, and thus attempted to use alternate methods to solve the problem. However, once this was indicated, the model was able to revise and arrive at the correct solution immediately.\n\nIncluding chain of thought in the prompt made the model output steps or question description + analysis pairs, while also making it simple to provide corrections for intermediate steps or assumptions. Kimi also used a different font color to indicate corrections to past hallucinations. The output indicates Kimi has good intuition and is able to reason about and handle the gradient operations well, regularly providing interpretations for steps. I thought it was interesting how Kimi was able to attempt multiple approaches analytically in the more complicated scenario it had hallucinated. Additionally, it was able to spatially reason about the table in question 5 without having to explicitly state any relationships among rows or columns. \n\n\n\nAnnotated Conversation:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T10:18:30.636365+11:00",
    "reply_count": 0,
    "view_count": 63,
    "user": {
      "id": 622884,
      "name": "Rohan Gulati",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7409772",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Kimi",
      "depth_bucket": "medium",
      "word_count": 245
    },
    "file_refs": [
      {
        "filename": "kimi_written_hw2.pdf",
        "position": 1563,
        "context": " without having to explicitly state any relationships among rows or columns. Annotated Conversation:"
      }
    ]
  },
  {
    "id": 7409630,
    "number": 474,
    "course_id": 84647,
    "title": "Special Participation A: Using GPT 5.1 thinking  on HW11",
    "document": "Trace:\nhttps://chatgpt.com/share/693135e6-2660-800a-8bd4-2cd122b0b787\nhttps://chatgpt.com/share/69320f80-534c-800a-8dd7-45f462c71566\n\nhttps://chatgpt.com/share/693210c8-e2f8-800a-b6f6-e7b827346645\nhttps://chatgpt.com/share/69320fb2-2224-800a-9862-dc87ba13a26d\nhttps://chatgpt.com/share/69320fe3-0654-800a-9fe8-78cbb68bf91b\n\nI used 5.1 Thinking (heavy) as a deep learning assistant and experimented with different prompt structures on a full homework set. I always told the model to reason step by step, explain its interpretation of each question, and self-correct when necessary. I tried both “all-at-once” prompts (entire multi-part questions or several questions in one go) and “turn-by-turn” prompts, where each subpart or major question was given in a separate message, plus a hybrid strategy for larger Fermi/system questions.\n\nOverall, the model showed strong technical understanding across topics like LoRA, matrix calculus, initialization, lookup embeddings, scaling laws, soft prompting, and MAML. Its derivations were often more explicit and pedagogical than the official solutions, with clear intermediate steps, shape checks, and good justifications for why certain statements are true. When the scope of the prompt was narrow (one subpart or one focused question), it behaved like a very competent TA that can both solve the problem and teach the underlying concept.\n\nThe main difference came from prompt structure. When I gave big, multi-part questions all at once, the answers tended to be long and cluttered, and there were occasional stability issues (for example, one “everything in one turn” prompt produced no output on the first try). In contrast, turn-by-turn prompting consistently led to cleaner, more rigorous solutions: the model derived gradients carefully, checked dimensions, and stayed on task for that specific subpart. In practice, turn-by-turn was strictly more reliable for math-heavy, multi-part deep learning questions.\n\nFor larger Fermi and systems-style questions, I used a hybrid approach: batching some straightforward numerical subparts together and then handling the more subtle ones one by one. This improved efficiency but exposed the main weakness of the model: coverage under large context. When the prompt contained many subparts or even multiple big questions (for example, a soft-prompting question followed by a separate TinyML question), the model sometimes skipped part of a subquestion or ignored a later question entirely, even while giving very strong answers to the earlier material. The problem was not understanding, but systematically answering everything that was asked.\n\nOverall, the experiments suggest that 5.1 Thinking (heavy) is best treated as a strong PhD-level TA with occasional attention and coverage issues in large contexts. The most effective pattern is to keep scope tight: use turn-by-turn prompting for multi-part questions, separate big questions into separate prompts, and optionally ask the model to list which subparts it has answered as a final coverage check. With that structure, the step-by-step + self-check style reliably produces detailed, correct, and often superior explanations compared to the official homework solutions.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-05T09:57:39.03131+11:00",
    "reply_count": 0,
    "view_count": 58,
    "user": {
      "id": 1004026,
      "name": "Fangzhou Zhao",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7409630",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "GPT-5.1",
      "depth_bucket": "medium",
      "word_count": 434
    },
    "file_refs": [
      {
        "filename": "Special A.pdf",
        "position": 3213,
        "context": "uces detailed, correct, and often superior explanations compared to the official homework solutions."
      }
    ]
  },
  {
    "id": 7409308,
    "number": 472,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT 5.1 Thinking on HW08",
    "document": "Hi there,\n\nI asked ChatGPT5.1 Thinking model to do HW08. Interestingly, it said that it could not give me full solutions due to OpenAI's academic integrity guardrails. These guardrails must be quite weak, or at least the model doesn't understand academic honesty, because the model proceeded to give full mathematical derivations of every subpart of the homework. It did not, however, finish the step of plugging and chugging numerical values in 1b (i), 1 b (ii), and 3c. After a second prompt, it still said that it couldn't give me solutions, but then proceeded to give me numerical answers for those subparts.\n\nI didn't catch any hallucinations of math rules/logic, however ChatGPT could not show every step of the derivation and sometimes it showed derivations alternate to the staff solution. It was most misleading on problem 1c) where it incorrectly suggested an FFT based convolution for critical path when the correct (and more efficient) solution uses direct conv with parallel matrix operations. This threw off the logic and resulted in incorrect solutions for the following subparts. \n\nThere were other instances where ChatGPT overcomplicated things. When estimating the optimum generic square linear encoder W using the SVD, it used a placeholder variable Z=WU which was very confusing. I've noticed this when I've used ChatGPT for hw help previously for this class; it really likes to come up with extra variables in the name of simplification, but really just serves to confuse the reader. \n\nWhile ChatGPT got incorrect results for time complexity analysis for question 1, it gave a fully correct (with correct and thorough steps & intuition) solution for 4c). \n\nIn conclusion, without mastery knowledge of the subject, it's really difficult to know when ChatGPT is hallucinating or overcomplicating. Thus, it's not more helpful than working backwards from staff solutions for the subparts where steps are shown in the staff solutions. \n\nSee this annotated conversation for more problem specific details:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T09:19:12.125774+11:00",
    "reply_count": 0,
    "view_count": 45,
    "user": {
      "id": 647651,
      "name": "Sammie Smith",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7409308",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "GPT-5.1",
      "depth_bucket": "medium",
      "word_count": 319
    },
    "file_refs": [
      {
        "filename": "screencapture-chatgpt-c-6930c5a2-9468-8327-bf03-8647a77eada3-2025-12-03-15_44_15 (1).pdf",
        "position": 2019,
        "context": "are shown in the staff solutions. See this annotated conversation for more problem specific details:"
      }
    ]
  },
  {
    "id": 7408383,
    "number": 470,
    "course_id": 84647,
    "title": "Special Participation A: KIMI K2 on HW 11 Written Questions",
    "document": "Model Tested: KIMI K2\n\nDomain: Homework11 -- LORA & Transformer & Mechanistic Interpretability\n\nPerformance Overview\n\nFor most question, KIMI K2 answers perfectly. However, there are some errors because OCR is not correct for matrix and it doesn’t fully understand the question. After I made additional prompt, the answers given by KIMI K2 are all correct.\n\nOverall Performance Summary\n\nAcross the entire HW11 interaction, KIMI K2 demonstrated strong reasoning ability and consistently produced correct answers for nearly all questions. The model was able to handle screenshots, conceptual interpretability questions, and numerical problems with high accuracy.\n\nHowever, two limitations were observed:\n\n1. OCR Errors on Matrix Inputs\n\nIn Question 2(c)(ii), KIMI K2 misinterpreted the matrix due to incorrect OCR parsing.\n\nOnce the correct matrix was explicitly provided, the model immediately corrected its derivation and produced the proper result.\n\n2. Misinterpretation of Ambiguous Prompts\n\nIn Questions 5(c)(d), the model initially used formulas from part (b) instead of the simplified Chinchilla-optimal rules required for the question, leading to an incorrect numerical scale.\n\nAfter additional clarification, KIMI K2 recalculated everything correctly.\n\nApart from these issues, all other questions were answered perfectly.\n\nHallucinations & Accuracy\n\nHallucination Rate: 0%.\n\nThe model never fabricated nonexistent concepts or equations.\n\nAll mistakes were due to OCR error or misinterpreting the prompt’s intended formula.\n\nConclusion\n\nKIMI K2 demonstrates high competency in advanced deep learning theory and can correctly solve most HW-level conceptual problems with minimal intervention. The model showed no hallucinations and delivered fully correct solutions once OCR and prompt ambiguities were addressed.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T07:42:30.669762+11:00",
    "reply_count": 0,
    "view_count": 63,
    "user": {
      "id": 1308099,
      "name": "Qicheng Zhu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7408383",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "Kimi",
      "depth_bucket": "medium",
      "word_count": 252
    },
    "file_refs": [
      {
        "filename": "ParticipationA_ KIMI_HW11_QichengZhu.pdf",
        "position": 1819,
        "context": "hallucinations and delivered fully correct solutions once OCR and prompt ambiguities were addressed."
      }
    ]
  },
  {
    "id": 7408067,
    "number": 469,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT on HW 8",
    "document": "I used ChatGPT 5.1 (Auto) to solve the written portions of Homework 8 (Questions 1, 3, and 4). For most of the problems, Chat GPT was able to correctly solve them on the first try. Below, I outlined the strengths and weaknesses of the model that I noticed, which included the types of questions Chat GPT tended to do well on versus needed more guidance on.\n\nStrengths:\n\nConsistently correct on direct mathematical derivations (unrolling recurrences, computing kernels, linear algebra, etc.)\n\nProduced clear step-by-step reasoning with limited guidance\n\nHandled numerical examples and matrix calculations well\n\nAble to refine and reorganize its thoughts effectively when prompted\n\nWeaknesses:\n\nHad difficultly with some algorithmic reasoning (most clearly demonstrated in the critical path length in Question 1c)\n\nPointing out missing factors and asking it to reorganize its thoughts generally caused Chat GPT to re-evaluate and correct its earlier attempt\n\nOccasionally gave over-complicated derivations and logic to reach the correct answer when a simpler argument was possible and intended\n\nWhen it made mistakes, they tended to follow a pattern (for example, repeatedly overlooking the logn term in the critical path length in Question 1c)\n\nAttached is my annotated log of the ChatGPT interaction. The document is color-coded for clarity. Green annotations / highlights indicate the response was correct. Red annotations / highlights indicate that the response was incorrect. Orange annotations indicate that the answer was partially correct.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T07:02:14.471564+11:00",
    "reply_count": 0,
    "view_count": 78,
    "user": {
      "id": 619713,
      "name": "Dagny Streit",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7408067",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "GPT-5.1",
      "depth_bucket": "medium",
      "word_count": 231
    },
    "file_refs": [
      {
        "filename": "Participation A Annotated.pdf",
        "position": 1545,
        "context": " that the response was incorrect. Orange annotations indicate that the answer was partially correct."
      }
    ]
  },
  {
    "id": 7407894,
    "number": 468,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3 Pro on HW 0",
    "document": "Link to the chat: https://gemini.google.com/share/89b0a83f691b\n\n\nI ran HW 0 through gemini and it was able to one-shot most of the homework. The PDF is annotated with my thoughts about specific questions and responses about gemini. Here are some things I wanted to highlight:\n\nThings gemini was able to do that were impressive:\n\nI fed the questions as screenshots, and gemini was able to parse the text correctly, including the equations without being given the latex\n\nGemini was able to handle multiple subparts together at times, and even able to break down question into different cases when needed.\n\nGemini was able to relate different subparts of the question together and explained the concepts the questions were trying to illustrate even with no mention of them (for example kernel trick in question 3)\n\nGemini gave detailed intermediate steps which were all correct with no mistakes instead of just arriving at a memorized answer. \n\nQuestions where gemini went wrong:\n\nGemini went wrong in 5 b ii. The hint in this question was to use a numerical example, which gemini did correctly. However, it incorrectly assumed that the result of one example would generalize. However, without giving it any hints and just prompting it to reconsider with other examples, gemini was able to realize that the elbow could move left or right.\n\nGemini also made a mistake in 5 d. This question involved a significant amount of calculus and algebra, and gemini made a mistake in one of the intermediate steps. This makes sense as this isn’t a commonly seen derivation in textbooks (as compared to ridge least squares derivation). For this one, I had to prompt it with a hint to keep the algebra in terms of matrices instead of oversimplifying after which it was able to arrive at the correct answer.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T06:38:54.935327+11:00",
    "reply_count": 0,
    "view_count": 64,
    "user": {
      "id": 617828,
      "name": "Ayush Goel",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7407894",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 299
    },
    "file_refs": [
      {
        "filename": "hw_0_gemini_pro_special_participation_a.pdf",
        "position": 62,
        "context": "Link to the chat: https://gemini.google.com/share/89b0a83f691b"
      }
    ]
  },
  {
    "id": 7407541,
    "number": 467,
    "course_id": 84647,
    "title": "Special Participation A -- DeepSeek-v3.2 Overthinks Less in Chinese",
    "document": "TL;DR: By prepending one Chinese sentence, the model will reason / \"think\" in Chinese, which accelerates its response by 2.5x and saves 2/3 tokens!\n\nThe prompt: 请务必用中文思考，并用英文回答以下问题。 (\"Please make sure to think in Chinese and answer the following question in English.\")\n\nCode is available, and discussions are warmly welcome.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-05T05:52:32.652712+11:00",
    "reply_count": 0,
    "view_count": 75,
    "user": {
      "id": 1751540,
      "name": "Xueli Sun",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7407541",
    "metrics": {
      "homework_id": "Unknown",
      "model_name": "DeepSeek",
      "depth_bucket": "low",
      "word_count": 49
    },
    "file_refs": [
      {
        "filename": "xueli_sun_deepseek_overthink_cn.pdf",
        "position": 324,
        "context": "nd answer the following question in English.\")Code is available, and discussions are warmly welcome."
      }
    ]
  },
  {
    "id": 7405742,
    "number": 460,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek v3.2 on HW10",
    "document": "Executive Summary\n\nI used the newly released DeepSeek v3.2 on HW10.\n\nOverall, this tests the model's \n1. OCR capabilities (reading the fine equations in the screenshots of the problems and also finding relevant parts from the FaceNet paper which was attached as pdf).\n2. Reasoning abilities on the related math content.\n\nIt is particularly impressive given that it was completely free for me to run on their website (and also has extremely low underlying inference cost).\n\nDeepSeek v3.2 (DeepThink) solved all the problems with perfect accuracy and clear chain-of-thought reasoning. \n\nChat log: https://chat.deepseek.com/share/u3b4iptgfflvv0t4oh\n\nAnnotated PDF:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T21:47:06.01722+11:00",
    "reply_count": 0,
    "view_count": 72,
    "user": {
      "id": 612982,
      "name": "Kelvin Li",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7405742",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 95
    },
    "file_refs": [
      {
        "filename": "DeepSeekv3.2_HW10.pdf",
        "position": 661,
        "context": "aragraph>Chat log: https://chat.deepseek.com/share/u3b4iptgfflvv0t4ohAnnotated PDF:"
      }
    ]
  },
  {
    "id": 7405582,
    "number": 457,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek v3.2 on HW 8",
    "document": "I used DeepSeek v3.2 to solve the written portions of HW8, where it performed quite well and one shotted almost all of the problems. \n\nOne interesting point was that DeepSeek struggled significantly with Problem 1(c), where it repeatedly overlooked/ignored the parallel computation model needed for the solution. I provided 2 nudges to hint DeepSeek towards the right direction, and only after these prompts did DeepSeek converge to the correct big O solution. Through this example I saw that while DeepSeek can correct mistakes when guided, it is not as strong at independently identifying these issues.\n\nStrengths:\n\nAble to parse matrix expressions, SSM equations, multi-step derivations, and more complex sets of symbols\n\nConsistently one shotted problems on the first attempt\n\nAlgebraic Reasoning was strong\n\nDetailed explanations\n\nWeaknesses:\n\nStruggled a bit with 1c; repeatedly ignored cost of vector and matrix operations\n\nSometimes provided overly long chains of reasoning\n\nHere is my annotated conversation with DeepSeek:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T19:25:27.802148+11:00",
    "reply_count": 0,
    "view_count": 93,
    "user": {
      "id": 612133,
      "name": "Justin Li",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7405582",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 153
    },
    "file_refs": [
      {
        "filename": "Annotated SPA.pdf",
        "position": 1031,
        "context": "h>Sometimes provided overly long chains of reasoningHere is my annotated conversation with DeepSeek:"
      }
    ]
  },
  {
    "id": 7405559,
    "number": 456,
    "course_id": 84647,
    "title": "Special Participation A - MistralAI's Le Chat on HW10 Written portion",
    "document": "For special participation A, I used MistralAI's Le Chat to solve HW10 written portion.\n\nOverall, the model did quite well on the conceptual and algebraic parts, but struggled on the subtle complexity analysis:\n\nFor the math derivations (e.g., rewriting softmax with a Gaussian kernel) and the conceptual questions (kernel intuition, causal masking, FaceNet, triplet loss), its answers were correct and aligned with the official solutions, often with clear step-by-step reasoning.\n\nFor the more delicate algorithmic complexity question (kernelized attention with random features), it gave a plausible but wrong answer, keeping an unnecessary (N^2) term and missing the whole “linear in (N)” point of the trick.\n\nWhen I confronted it with the official solution, it was good at post-hoc analysis: it compared its result, admitted the mistake, and explained why its reasoning had been too coarse.\n\nIt did not spontaneously flag uncertainty on that hard question; it sounded confident while being wrong.\n\nSo: strong on standard derivations and conceptual ML, weaker and over-confident on fine-grained complexity / algorithmic details — which is exactly the type of behavior we were aware it could have.\n\nThe raw chat is available here.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T19:13:57.042068+11:00",
    "reply_count": 0,
    "view_count": 82,
    "user": {
      "id": 1770931,
      "name": "Fantine Mpacko Priso",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7405559",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "Mistral",
      "depth_bucket": "medium",
      "word_count": 188
    },
    "file_refs": [
      {
        "filename": "chat-4de4d570-dc1c-4b2f-baf9-9afc394d0333.json",
        "position": 86,
        "context": "For special participation A, I used MistralAI's Le Chat to solve HW10 written portion."
      }
    ]
  },
  {
    "id": 7405554,
    "number": 455,
    "course_id": 84647,
    "title": "Special Participation A: Grok on HW4",
    "document": "Executive Summary:\n\nI used Grok to complete the written portion of Homework #4. Since I do not have the paid tier, I used Grok fast. While this is not the top model in the Grok line, it is interesting to evaluate its capabilities from the perspective that it is “150x cheaper than Claude” [1]. Grok was able to one-shot many parts of the problem, but sometimes required corrections to arrive at the correct solution. Notably, the solutions were very long, with significantly more explanation and background included compared to the solutions in the posted answer key. As noted in previous posts, Problem #2 part e on the answer key potentially contains a mistake. Grok got the same answer as indicated by previous Special Participation A posts focusing on this problem set, and when I asked the model to evaluate whether the key’s current solution is reasonable, it provided a strong refutation. For some problems, such as Problem #3 part d and Problem #4 part f, Grok presents work towards one solution and then indicates that it has changed its mind, says the previous work is incorrect, and suggests a new approach, which is somewhat confusing behavior. I inputted all the problems, including problem 3 which contains a figure and Problems #3 and #4 which contain example matrices, into the model, and its ability to “read” these screenshots (using “Grok-1.5 Vision (Grok-1.5V)”) appeared to be accurate throughout. \n\nAttached here is a transcript of the conversation and below is a rubric for a quick glance over parts of the problem that the model was able to one-shot vs. required extra hints.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T19:09:46.456253+11:00",
    "reply_count": 0,
    "view_count": 79,
    "user": {
      "id": 1691228,
      "name": "Elizabeth Polito",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7405554",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "Grok",
      "depth_bucket": "medium",
      "word_count": 269
    },
    "file_refs": [
      {
        "filename": "grok_annotated_merged.pdf",
        "position": 1599,
        "context": "quick glance over parts of the problem that the model was able to one-shot vs. required extra hints."
      }
    ]
  },
  {
    "id": 7405450,
    "number": 453,
    "course_id": 84647,
    "title": "Special Participation A: GPT-4o on HW10 Noncoding",
    "document": "For this exercise, I used one of the legacy ChatGPT models (GPT-4o) and analyzed how it would perform on the non-coding portions of Homework 10, i.e. questions 1 and 5. \n\nInitially I expected that this model wouldn't perform so well since it's an older model and I've previously experienced hallucinations with it. However, it seems that it knows most of the mathematical derivations (probably from memorization from online data). The one thing I noticed the model struggling with was analyzing graphs and tables from the Facenet paper, which makes sense since it's a textual model. It also had a pretty hand-wavy explanation for the runtime of softmax approximated-attention but got the correct answer nonetheless. \n\nBelow is my annotated conversation:\n\nhttps://drive.google.com/file/d/1tOkknZyAFr0qjBMrHuf3ArU9DUlxhMWm/view?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T18:27:23.611836+11:00",
    "reply_count": 0,
    "view_count": 65,
    "user": {
      "id": 636231,
      "name": "John Chang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7405450",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "GPT-4o",
      "depth_bucket": "medium",
      "word_count": 120
    },
    "file_refs": null
  },
  {
    "id": 7405370,
    "number": 452,
    "course_id": 84647,
    "title": "Special Participation A: GPT-5.1 on HW 9",
    "document": "I used GPT-5.1 to help with the written (non-coding) parts of Homework 9: Problems 1, 2, 3, 4, and 6. I included screenshots of the problem statements, and my prompt for each problem followed the structure:\n\nFor each subpart, please: (1) restate the problem in your own words, (2) explain the main idea before doing any calculations, (3) show the full step-by-step solution with no big jumps, and (4) summarize the final answer clearly. I have attached screenshots of each problem.\n\nSummary: GPT-5.1 handled these questions very well. It reliably extracted equations from screenshots, followed the structured prompt, and produced organized, readable solutions. Most final answers matched the official solutions.\n\nStrengths: GPT-5.1 did very well across all the written parts. It was able to correctly parse mathematical expressions, notation, and multi-step problems from screenshots reliably, even when the problem statements was spread across multiple images. The solutions given by the LLM followed the structure that was specified in the prompt, with each answer including a restatement of the subproblem, a plan, detailed step by step derivations, and a summary. For most subparts, GPT-5.1 was able to one-shot the correct solution with no iteration needed. Many explanations were not only correct, but also more detailed than the official solutions. Overall, it demonstrated strong reasoning and required little follow up guidance.\n\nWeaknesses: The main issue I encountered was an occasional misinterpretations of notation or implicit conventions in the problem statement. The main example was in Q6, where GPT-5.1 defaulted to the homogeneous quadratic kernel rather than the degree-2 polynomial kernel expected by the assignment. Once I explicitly asked about the missing constant term, it corrected itself immediately, so this was not a hallucination but more so like falling back on a common default kernel definition. In addition, some of the complexity analyses, while correct, could have been clearer, and would benefit from including pseudocode. Overall, most of the issues were minor and the model had sound reasoning.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T18:00:09.747057+11:00",
    "reply_count": 0,
    "view_count": 64,
    "user": {
      "id": 612366,
      "name": "Lenci Ni",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7405370",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "GPT-5.1",
      "depth_bucket": "medium",
      "word_count": 325
    },
    "file_refs": [
      {
        "filename": "participation_a_hw9.pdf",
        "position": 2133,
        "context": "from including pseudocode. Overall, most of the issues were minor and the model had sound reasoning."
      }
    ]
  },
  {
    "id": 7404515,
    "number": 441,
    "course_id": 84647,
    "title": "Special Participation A: Claude on HW7",
    "document": "Executive Summary\n\nFor this assignment, I looked at how well Claude Opus can handle the non-coding, conceptual parts of CS282 Homework 7. I went through the problems one by one, interacted with the model without giving it any code or extra implementation hints, and saved the full conversation logs. My main goal was to see (1) how accurate it is, (2) how stable its reasoning is, and (3) how much I need to steer it to get the right answer.\n\nOverall, Claude Opus did really well. It got everything right on the first try and produced clean, well-structured derivations without me having to nudge it much. For the four non-coding problems (3b, 4, 7, and 8), it basically one-shot the correct solution every time. I didn’t see any major hallucinations.\n\nIt was especially reliable on:\n\nlinear algebra manipulations,\n\nPCA vs. autoencoder equivalence arguments,\n\ngradient calculations, and\n\ngeneral convexity/optimization reasoning.\n\nIn terms of interaction, the model was:\n\nconsistent in how it formatted math and explanations and,\n\nlogically stable across long derivations,\n\nOverall, based on this evaluation, Claude Opus can solve all the non-coding questions from CS282 HW7 with almost no guidance. Please see the annotated logs.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T15:18:02.927395+11:00",
    "reply_count": 0,
    "view_count": 74,
    "user": {
      "id": 663744,
      "name": "Vongani Maluleke",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7404515",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "Claude Opus",
      "depth_bucket": "medium",
      "word_count": 195
    },
    "file_refs": [
      {
        "filename": "hw7_claude_logs_annotated.pdf",
        "position": 1230,
        "context": ""
      }
    ]
  },
  {
    "id": 7404071,
    "number": 438,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 2.5 Flash on HW10",
    "document": "I evaluated Gemini 2.5 Flash on the non coding portions of Homework 10, covering Kernelized Linear Attention and the FaceNet paper reading questions. Approximately 60-65% of questions were answered correctly on the first attempt but I did notice residual errors with calculations that didn’t seem to affect the final answer. Conceptual questions (separate from factual / math questions) like reflections from the Facet paper required most steering surprisingly. I initially assumed it would do worst on the math. Gemini's responses were consistently verbose, though it did not correlate with correctness. When given directional feedback, Gemini generally moved toward the correct answer rather than defending incorrect positions indefinitely. But it did anchor on incorrect interpretation sometimes. On Q5b (triplet loss supervision), Gemini claimed triplet loss \"requires explicit class labels\" and defended this through 3 steering attempts before finally acknowledging that relational similarity (positives/negatives) would do the job. Similarly, on Q5g, it insisted performance saturates rather than drops beyond 128 dimensions even though the paper clearly stated the latter. This outlines hallucinations that were further seen when trying to work on Q5f (harmonic embeddings). Gemini confidently stated \"The FaceNet paper does not directly define or utilize harmonic embeddings\" and fabricated a connection to Fourier analysis/Random Fourier Features. This was a clear hallucination since the paper does discuss harmonic embeddings in Section 3.2, referring to model version compatibility. The longer the conversation went on, the more windy and hallucinated the answers ended up being. This seemed like a predictable outcome though. I steered Gemini to the correct answer in 4 ways:\n\nAbstract conceptual questions: often led to defensive elaboration of the same point\n\nHypothetical scenarios: better success at identifying missteps\n\nAsking for specific data points and references: mildly effective\n\nDirect factual corrections: last resort, and usually brought Gemini back to the grounded data it was given\n\nFor homework assistance, Gemini 2.5 Flash would be reliable for computations but should be verified against the main material for conceptual claims. The longer the chat, the worse it does with actively recalling instructions and facts.\n\nHere's an annotated log of my chat transcript: https://drive.google.com/file/d/1sTkCzX9o669RVIqiDcby3DfUjsgOsUwB/view?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T14:14:57.565789+11:00",
    "reply_count": 0,
    "view_count": 55,
    "user": {
      "id": 608574,
      "name": "Imra Dawoodani",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7404071",
    "metrics": {
      "homework_id": "HW10",
      "model_name": "Gemini Flash",
      "depth_bucket": "medium",
      "word_count": 348
    },
    "file_refs": null
  },
  {
    "id": 7403245,
    "number": 436,
    "course_id": 84647,
    "title": "Special Participation A: Gemini Pro 3 on HW 11",
    "document": "For this special participation, I used Gemini Pro 3 to solve the written portions of homework 11.\n\nIn this task, since there were no complex mathematical proofs or derivations involved, the vast majority of questions relied on intuitive understanding or simple mathematical calculations. Combined with its use of Python for auxiliary computations, Gemini 3 Pro’s performance was very satisfactory. After comparing with the standard answers provided by the teaching assistant, we judged that Gemini 3 Pro achieved a 100% zero-shot accuracy. All qualitative answers and intuitive understandings were reasonable and correct, the mathematical calculations were error-free, and for the few proof-based questions, it provided sufficient formula support and mathematical derivation steps. The formatting was clear and the language was easy to understand.\n\nSince the performance is quite satisfactory, overall my strategy is to input the hw pdf and write a “system prompt” at the beginning of the chat and then let the model solve several questions each time. This is to avoid that the model generates too long answers and make the model focus on few topics and retrieve limited information each run. In the system prompt, I defined the task and model’s role, and provided some instructions, like being precise and making the solution easy to be understood by undergraduate students, etc.\n\nTranscript:\n\nhttps://docs.google.com/document/d/1nt5kISBlTi0EcLF0EesfdA5Kqzdn78-tzH8Hi4BEQa0/edit?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T12:29:34.39678+11:00",
    "reply_count": 0,
    "view_count": 46,
    "user": {
      "id": 1285421,
      "name": "Xuanlin Mao",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7403245",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 215
    },
    "file_refs": null
  },
  {
    "id": 7401923,
    "number": 434,
    "course_id": 84647,
    "title": "Special Participation A: Kimi K2 on hw8",
    "document": "I used kimi k2 on hw9 and observed it giving correct answers zero shot on all questions. (even without \"thinking\" tokens) very impressed.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T09:47:18.097945+11:00",
    "reply_count": 0,
    "view_count": 77,
    "user": {
      "id": 1553113,
      "name": "Nils Selte",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7401923",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Kimi",
      "depth_bucket": "low",
      "word_count": 23
    },
    "file_refs": [
      {
        "filename": "cs182_hw8 special participation kimi k2.pdf",
        "position": 137,
        "context": " giving correct answers zero shot on all questions. (even without \"thinking\" tokens) very impressed."
      }
    ]
  },
  {
    "id": 7401078,
    "number": 432,
    "course_id": 84647,
    "title": "Special Participation A: Grok on HW 08",
    "document": "I used Grok on the written (non-coding) problems of HW8. It was very strong on the algebraic and conceptual parts (SSM kernels, linear purification, ridge attention), usually getting the correct derivations on the first try. The main issue I saw was in complexity analysis: in a few places it mixed up total work vs. critical path length when discussing parallelization, which required manual correction. Aside from that, the mathematics and reasoning closely matched the official solutions.\n\nFor more details, including annotated interaction logs and comparisons to the official solutions, see the attached PDF.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T08:18:32.354006+11:00",
    "reply_count": 0,
    "view_count": 75,
    "user": {
      "id": 1316489,
      "name": "Krish Yadav",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7401078",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Grok",
      "depth_bucket": "medium",
      "word_count": 93
    },
    "file_refs": [
      {
        "filename": "annotated-grok-hw8-summary.pdf",
        "position": 612,
        "context": "ncluding annotated interaction logs and comparisons to the official solutions, see the attached PDF."
      }
    ]
  },
  {
    "id": 7400839,
    "number": 431,
    "course_id": 84647,
    "title": "Special Participation A: Qwen on HW4",
    "document": "For special participation A on HW4, I used Qwen and its Qwen3-Max model with thinking to solve the non coding problems on the homework (problems 1, 2, 3, 4, 7). \n\nI attempted this homework with Qwen 3 times actually as the earlier times I prompted it with more advanced instructions or I would give it each problem seperately it often spiraled and kept second guessing itself. I found that when Qwen does this it often happens cloes to 3 times and it has a hard time accepting the truth when it can't reason to it. This is present in problem 2e in this log as when asking Qwen to reason itself to the solution it took 3 times in one query to finally arrive to the correct answer. \n\nDespite this, Qwen did a pretty good job on this homework especially considering how difficult it is. It interestingly did not one shot computation based questions like the convolution questions in problem 3, but it expertly navigated and explained the matrix calculus in problem 7. I think Qwen is a decent model for this type of tasks but it has to be wrangled more then I would have hoped. \n\nFinal conversation trace: https://chat.qwen.ai/s/5fd54197-ec03-43bd-a4cb-53d560f95d0f?fev=0.1.7",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T07:53:08.520193+11:00",
    "reply_count": 0,
    "view_count": 78,
    "user": {
      "id": 961709,
      "name": "Zach Pricz",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7400839",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "Qwen",
      "depth_bucket": "medium",
      "word_count": 201
    },
    "file_refs": [
      {
        "filename": "qwen_hw4_annotated_trace.pdf",
        "position": 1172,
        "context": ""
      }
    ]
  },
  {
    "id": 7399196,
    "number": 427,
    "course_id": 84647,
    "title": "Special participation A: Kimi on HW0",
    "document": "For Special participation A, I used Kimi on the writing part of HW0. Overall, it was useful, but it also revealed several limitations. Kimi is generally good at recognizing high-level patterns and giving correct final expressions, but it often skips steps, relies too much on memorized formulas, and needs very explicit instructions to produce proper derivations. It also struggles with qualitative reasoning at times.\n\nDespite these issues, Kimi’s conceptual explanations were clear, and with careful prompting and oversight, it was able to provide correct results. In short, Kimi is helpful as long as you guide it closely and verify its reasoning, but it is not reliable for detailed, step-by-step mathematical work without supervision.\n\nHere is the annotated log:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-04T04:34:25.897961+11:00",
    "reply_count": 0,
    "view_count": 68,
    "user": {
      "id": 1751543,
      "name": "ZhaoRui Qu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7399196",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Kimi",
      "depth_bucket": "medium",
      "word_count": 118
    },
    "file_refs": [
      {
        "filename": "Kimi--HW0.pdf",
        "position": 767,
        "context": "reliable for detailed, step-by-step mathematical work without supervision.Here is the annotated log:"
      }
    ]
  },
  {
    "id": 7398141,
    "number": 425,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3 pro on Hw 12",
    "document": "Model Tested: Gemini 3 Pro\n\nOverall Performance: Very good: 100% One-shot\n\nPerformance Overview\n\nThe model was tasked with solving 3 deep learning problems involving debugging neural network initialization, analyzing information theory concepts (KL Divergence behavior), and interpreting Variational Information Bottleneck (VIB) systems.\n\nCode Debugging (Transformers): The model correctly identified a \"peaked softmax\" issue caused by improper weight initialization in a Transformer implementation. It provided the correct theoretical justification (variance scaling) and the exact code fix (Xavier/Glorot scaling) without needing iterative prompting.\n\nMathematical Intuition (KL Divergence): The model successfully generated a counter-example to distinguish Forward vs. Reverse KL divergence properties. It correctly mapped visual plots to \"Mode-Seeking\" vs. \"Mass-Covering\" behaviors based purely on visual evidence and theoretical definitions.\n\nSystem Design (VIB/VAE): The model accurately constructed the computational graph for the Reparameterization Trick and correctly traced gradient flows for encoder/decoder parameters. It also correctly interpreted unlabeled validation error curves by reasoning about the regularization coefficient.\n\nHallucinations & Accuracy\n\nHallucination Rate: 0%.\n\nThe model did not hallucinate. Notably, in Question 3 (Part 2), the model explicitly noted that \"Figure 4 was not included\" but proceeded to solve the problem by deriving what the plots must look like based on VIB theory. This demonstrates a high level of reasoning capability where the model fills missing context with theoretical deduction rather than fabricating visual data.\n\nBehavioral Observations & Strategies\n\nMultimodal Reasoning: The model demonstrated strong vision capabilities, accurately interpreting trend lines in plots (Question 3, Part 1) and distribution shapes (Question 2) without text descriptions of the visual data.\n\nTheoretical Grounding: The model consistently reasoned from first principles. For example, when discussing KL divergence, it didn't just state the answer; it integrated the integral definitions to explain why the support mismatch causes infinity.\n\nStrategy - Contextual Chunking: For Question 3, which was long and multi-part, I broke the prompt into two segments (Parts a/b, then c/d). This likely helped the model maintain focus, though its strong performance suggests it might have handled the full context in one go.\n\nConclusion\n\nGemini 3 Pro demonstrated sophisticated understanding of deep learning theory, effectively bridging the gap between mathematical notation, code implementation, and visual interpretation of training dynamics. It functioned as a highly competent tutor, correctly solving all non-coding theoretical components one-shot.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-04T01:08:18.606596+11:00",
    "reply_count": 0,
    "view_count": 61,
    "user": {
      "id": 622765,
      "name": "Gabriel Han",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7398141",
    "metrics": {
      "homework_id": "HW12",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 373
    },
    "file_refs": [
      {
        "filename": "Special Participation A.pdf",
        "position": 2791,
        "context": "ioned as a highly competent tutor, correctly solving all non-coding theoretical components one-shot."
      }
    ]
  },
  {
    "id": 7397817,
    "number": 422,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT o3 on HW 9",
    "document": "Executive Summary\n\nFor Homework 9 (all but prob 5), I worked through the written parts with ChatGPT to see how well it could solve the questions on the first try, how often it drifted or hallucinated, and how solid its reasoning was. For most of the assignment, especially Questions 1 through 4, it did surprisingly well. It handled the computation-heavy parts, the code-completion questions, and the conceptual attention explanations almost exactly the way the official solutions did. Its derivations for expectations, variances, argmax attention, and the multi-head attention code were all clean and matched the intended logic with basically no backtracking.\n\nOne thing that stood out was how consistently the model could jump straight into the right structure of the problem. It didn’t need much nudging to set up sums, identify independence assumptions, or translate equations into PyTorch einsums. Even the complexity analysis in Question 4 lined up with the official answers once we made sure the symbols matched. When it was right, it was very right, and the explanations were easy to follow.\n\nThe main issue came up in Question 6. ChatGPT fell into a pretty common trap: it used the homogeneous quadratic kernel (q transpose k squared) instead of the standard degree-2 polynomial kernel ((q transpose k plus 1) squared). Because of that, its first attempt at the feature map was missing the constant term and the linear terms, so that part drifted from the correct answer. But once I asked it to revisit its reasoning instead of giving a new solution, it immediately spotted the mistake, explained why the earlier map didn’t match the actual kernel, and rebuilt everything correctly. So the error wasn’t really a hallucination—more like it defaulted to a familiar formula without checking whether it matched what the homework meant.\n\nAside from that one slip, the model didn’t really hallucinate or spiral. It stayed structured and patient even in the longer reasoning chains, and it didn’t show any of the “I’m stuck so I’m going to guess something wild” behavior LLMs sometimes fall into. When it got something wrong, it was usually because it applied the wrong convention, not because it made something up.\n\nOverall, ChatGPT handled the written problems well. It can one-shot most derivations, and when it does mess up, it’s usually easy to steer it back on track with a focused prompt. The combination of clear explanations, good algebraic intuition, and willingness to self-correct makes it a pretty solid tool for these kinds of homework questions.\n\nBelow is the annotated log:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-03T20:45:21.592344+11:00",
    "reply_count": 0,
    "view_count": 53,
    "user": {
      "id": 647361,
      "name": "Tamzid Razzaque",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7397817",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "GPT-o3",
      "depth_bucket": "medium",
      "word_count": 420
    },
    "file_refs": [
      {
        "filename": "ChatGPT - hw9.pdf",
        "position": 2591,
        "context": "rrect makes it a pretty solid tool for these kinds of homework questions.Below is the annotated log:"
      }
    ]
  },
  {
    "id": 7397298,
    "number": 419,
    "course_id": 84647,
    "title": "Special Participation A: Hw2 with Gemini Pro 3 Thinking Mode",
    "document": "For the special participation A on HW2, I use Grok to address the non-coding analytical problems 1, 2 and 7. Gemini did it great as expected, all questions were correct on the first-shot prompt. I used images as the prompt instead of the direct text.\n\nSummary: Gemini 3 is one of the models for math questions and it doesn't disappoint, every procedure was at least acceptable, understandable and most important correct (doesn't hallucinate with these questions). The answers were correct and also using different notations.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-03T17:16:38.165773+11:00",
    "reply_count": 0,
    "view_count": 79,
    "user": {
      "id": 1770933,
      "name": "Gustavo Jose Ortiz Zepeda",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7397298",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Gemini Pro",
      "depth_bucket": "low",
      "word_count": 85
    },
    "file_refs": [
      {
        "filename": "hw2-specialparticipationA-Gemini3ProThinking.pdf",
        "position": 524,
        "context": "sn't hallucinate with these questions). The answers were correct and also using different notations."
      }
    ]
  },
  {
    "id": 7397226,
    "number": 417,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3 pro on HW 8",
    "document": "I used Gemini 3 Pro to solve the non‑coding portion of HW 8. Overall, Gemini did an excellent job producing clear and well‑structured mathematical derivations, often matching the logical flow of the official solutions. I especially appreciated its ability to organize the reasoning into intuitive steps and explain the purpose behind each transformation. Gemini sometimes took a slightly different path to reach the final answer, but its results were fully consistent after minor prompting. Overall, I was impressed by its precision, presentation quality, and the clarity of its mathematical reasoning.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-12-03T16:58:24.482712+11:00",
    "reply_count": 0,
    "view_count": 76,
    "user": {
      "id": 1751466,
      "name": "Tin Yau",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7397226",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 90
    },
    "file_refs": [
      {
        "filename": "A_Gemini_3_pro.pdf",
        "position": 602,
        "context": "was impressed by its precision, presentation quality, and the clarity of its mathematical reasoning."
      }
    ]
  },
  {
    "id": 7397166,
    "number": 416,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3 Pro on HW 2 Written Questions",
    "document": "Executive Summary: Interaction with Gemini on Deep Learning Theory\n\nModel Tested: Gemini 3 Pro\n\nDomain: Deep Learning Optimization & Distributed Training (CS 182/282 Context)\n\nOverall Performance: 100% Success Rate (One-shot)\n\nPerformance Overview\n\nThe model was tasked with solving non-coding theoretical problems involving:\n\nOptimization Derivations: Deriving analytical solutions for penalized linear improvement using Euclidean ($L_2$) and Infinity ($L_\\infty$) norms.\n\nOptimizer Convergence: Analyzing convergence points for Vanilla SGD, Simplified Adam, and Feature Rescaling on a constrained linear regression problem.\n\nDistributed Training: Calculating communication costs (message count and size) for All-to-All, Parameter Server, and Ring All-Reduce architectures.\n\nIn every instance, the model provided the correct analytical solution and numerical answers on the first attempt (one-shot). No prompt engineering, iterative correction, or \"dragging\" of the model was required to arrive at the solution. The model successfully transcribed complex mathematical notation directly from uploaded screenshots without OCR errors.\n\nHallucinations & Accuracy\n\nHallucination Rate: 0%.\n\nThe model demonstrated robust reasoning capabilities. It correctly identified standard optimizers (Gradient Descent, SignSGD) from first-principles derivations and accurately recalled specific distributed systems constraints (e.g., the $2(n-1)$ steps in Ring All-Reduce).\n\nBehavioral Observations: Uncertainty and Verification\n\nA distinct behavioral trait observed during the session was the model's tendency to simulate self-verification and hedging.\n\n\"Double-Checking\": Despite the high accuracy of the final outputs, the model’s reasoning process appeared to involve recursive checks. It would often derive a step, then implicitly verify it against known standard results (e.g., checking if the derived update rule matched the standard definition of SignSGD) before committing to the answer.\n\nHedging Language: The model frequently used probabilistic language such as \"I believe\" or \"this suggests,\" rather than authoritative absolutes, even when the math was unambiguous. This behavior mimics a cautious human student double-checking their work to ensure logical consistency, rather than a machine simply outputting a retrieved token sequence.\n\nConclusion\n\nGemini demonstrated graduate-level competency in deep learning theory, capable of handling multimodal inputs (LaTeX screenshots) and complex analytical derivations with perfect accuracy. The interaction suggests that for well-defined theoretical problems, modern reasoning models can serve as reliable verification engines, provided the user monitors the \"reasoning track\" for the model's internal consistency.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-03T16:47:42.131618+11:00",
    "reply_count": 0,
    "view_count": 69,
    "user": {
      "id": 915218,
      "name": "Ijin Yu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7397166",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 353
    },
    "file_refs": [
      {
        "filename": "q1.pdf",
        "position": 2757,
        "context": "tion engines, provided the user monitors the \"reasoning track\" for the model's internal consistency."
      },
      {
        "filename": "q2.pdf",
        "position": 2757,
        "context": "es, provided the user monitors the \"reasoning track\" for the model's internal consistency."
      },
      {
        "filename": "q5.pdf",
        "position": 2757,
        "context": "aph>"
      }
    ]
  },
  {
    "id": 7393256,
    "number": 399,
    "course_id": 84647,
    "title": "Special Participation A: Claude (Sonnet 4.5) on HW 12",
    "document": "Below is my report on using Claude's Sonnet 4.5 model to solve the written questions to Homework 12. I have also attached a PDF of the annotated transcript.\n\nSummary:\n\nOverall Claude correctly one-shots every question\n\nGenerally, it's reasoning is correct, except for one slightly informal/imprecise statement that it makes about distributions in the second problem.\n\nIn general, I felt that its explanations lacked pedagogical value and would not be maximally helpful to a student who was confused about the class material",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-03T08:38:40.394007+11:00",
    "reply_count": 0,
    "view_count": 107,
    "user": {
      "id": 923022,
      "name": "Ishir Garg",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7393256",
    "metrics": {
      "homework_id": "HW12",
      "model_name": "Claude Sonnet",
      "depth_bucket": "medium",
      "word_count": 81
    },
    "file_refs": [
      {
        "filename": "participationA.pdf",
        "position": 156,
        "context": " solve the written questions to Homework 12. I have also attached a PDF of the annotated transcript."
      }
    ]
  },
  {
    "id": 7389909,
    "number": 397,
    "course_id": 84647,
    "title": "Special Participation A: Gemma 3 (12b params) on HW09 Written Problems",
    "document": "Executive Summary:\n\nI worked with Gemma 3 on the written problems for Homework 9 (problems 1, 2, 3, 4, and 6). Overall, Gemma performed fairly well (especially given that it is an open-source model that I ran locally on my laptop). For problems 1–4e, which were largely computation problems, Gemma (mostly) produced correct solutions on the first attempt and consistently demonstrated a strong grasp of the underlying concepts.\n\nOne area where Gemma stood out was its clarity of explanation. Compared to ChatGPT and Claude (the other LLMs I have worked with), I've found Gemma's explanations to be clearer, or explicit, and pedagogically helpful. This, in part, is because Gemma uses more English in their mathematical argument, whereas (in my experience) ChatGPT and Claude tend to be equation-heavy. Gemma's strong explanations (when correct) make it a particularly helpful learning tool, even when it gets some problems wrong. I actually found myself better understanding some of the time and space-complexity arguments around attention mechanisms when trying to guide Gemma to the right solution.\n\nGemma’s weaknesses emerged in two specific areas:\n1: time- and space-complexity analysis\n\n2: dimension checking and tensor shape reasoning.\n\n\nHowever, even when it produced incorrect bounds or incorrect tensor shapes, it often identified the correct overall strategy, so using Gemma can still be instructive. \n\nOne of the more striking behaviors appeared toward the end. After many failed attempts and struggling through problems 4f, 4g, and 6, Gemma seemed to exhibit an emergent pattern resembling human frustration. Its responses became less coherent, more speculative, and more willing to guess just to produce something and move on. I found this shift in behavior similar to how a tired human might respond when stuck on a difficult p-set. When Gemma enters this state, the best strategy is simply to start a new conversation for higher-quality answers (however, I continued in the same conversation out of curiosity).\n\nOverall, I found Gemma 3 to be an instructive assistant for the written problems. \n\nTrace:\nhighlighted themes - clarity of explanations, where Gemma 3 became lost, and emergent \"frustrated\" behavior.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-02T20:19:39.414415+11:00",
    "reply_count": 0,
    "view_count": 80,
    "user": {
      "id": 635294,
      "name": "Etaash Patel",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7389909",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Gemma",
      "depth_bucket": "medium",
      "word_count": 344
    },
    "file_refs": [
      {
        "filename": "Participation A.pdf",
        "position": 2226,
        "context": "ted themes - clarity of explanations, where Gemma 3 became lost, and emergent \"frustrated\" behavior."
      }
    ]
  },
  {
    "id": 7389325,
    "number": 392,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 3 Pro on HW 7",
    "document": "I used Gemini 3 Pro to solve the non-coding portion of HW 7, which include 3(b), 4, 7, and 8. Overall, Gemini 3 Pro did a quite good job of giving an intuitive explanation for some of the complex math found in the official HW solutions. It was able to one-shot almost all the questions (except one part in question 4 about the blog). Gemini did usually take quite different approaches than the official solution, which made me prompt Gemini further to connect the different solutions together. I do believe that Gemini could have been more rigorous for some of the more math-intensive questions, but I didn't expect to get something fully similar to the official solution without giving more context on what math techniques to use. Overall, I am quite impressed with the results.\n\nHere is an annotated trace of the chat: https://docs.google.com/document/d/1mXvXyKigfz0mgcQdqG-h3eDCklwMNgAnccCIv5bq4uk/edit?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-02T16:53:58.575623+11:00",
    "reply_count": 0,
    "view_count": 80,
    "user": {
      "id": 647490,
      "name": "Vrushank Prakash",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7389325",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "Gemini Pro",
      "depth_bucket": "low",
      "word_count": 144
    },
    "file_refs": null
  },
  {
    "id": 7386904,
    "number": 384,
    "course_id": 84647,
    "title": "Special Participation A: Mistral on HW 1",
    "document": "I have used Mistral to test on Hw 1. \n\nLink: https://chat.mistral.ai/chat/6ff004cd-66c9-49ef-92fb-19476f51402b\n\nSummary:\n\nIn general, it was able to get most of the answers without any mistakes. A lot of the work shown by the LLM followed the solutions of what the course has provided. Looking at the solution that Mistral provided, it seems like a lot of the computational mistakes are coming from matrix calculations. For example, it was able to correctly reason 3a however it failed to get 3b even though, from 3a to 3b, it is just simple matrix calculations. Since question 3 is built upon previous questions, since it messed up on 3b, it messes up on the following subpart questions for 3. \n\nIt seems like the mathematical reasoning is good since the non-matrix computations are all working very well without any errors. I think it also got better when I pointed out that there are some computational errors because after that there has not been any mistakes. \n\nI think the one-shot for Mistral has about 80% accuracy on the problems. I tried to ask Mistral to fix some of the mistakes  that it has made on the previous answer, but it seems to misunderstand my request and always go with the same incorrect question. It seems to be that MIstral is \"self-centered\" or cannot identify the questions that it has been asked.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-02T11:24:16.403925+11:00",
    "reply_count": 0,
    "view_count": 55,
    "user": {
      "id": 227880,
      "name": "Minjune Kim",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7386904",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "Mistral",
      "depth_bucket": "medium",
      "word_count": 226
    },
    "file_refs": null
  },
  {
    "id": 7382863,
    "number": 376,
    "course_id": 84647,
    "title": "Special Participation A: Mistral AI's Le Chat on HW5 Written Portion",
    "document": "For Special Participation A, I used Mistral AI's Le Chat on the written portion of HW5. Overall, it performed averagely - mostly arriving at correct answers in one shot, but in a couple of cases requiring further prompting to correct mistakes. I found some indication of a lack of ability to refer to chat history in it's responses.\n\nHere is the link to my chat: https://chat.mistral.ai/chat/9c9c7fc7-7985-45a9-b7cd-df337fec5d26",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-02T02:48:01.810497+11:00",
    "reply_count": 0,
    "view_count": 52,
    "user": {
      "id": 647373,
      "name": "Kian Hekmatnejad",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7382863",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "Mistral",
      "depth_bucket": "low",
      "word_count": 66
    },
    "file_refs": [
      {
        "filename": "special_participation.pdf",
        "position": 428,
        "context": "reak/>Here is the link to my chat: https://chat.mistral.ai/chat/9c9c7fc7-7985-45a9-b7cd-df337fec5d26"
      }
    ]
  },
  {
    "id": 7381174,
    "number": 368,
    "course_id": 84647,
    "title": "Special Participation A: Qwen on HW7",
    "document": "I used Qwen3-Max model to solve the written parts of homework 7. Instead of feeding individual questions with full text, I first uploaded the entire homework PDF and then asked each sub-question only by its label (“7(b)”, “8(a)”, etc.). The goal was to see whether Qwen could (1) avoid forgetting earlier context, (2) correctly retrieve the appropriate problem statement from the embedded PDF, and (3) solve each part in a single attempt. In this experiment Qwen successfully accomplished (1), (2) but failed (3) for questions that require SVD math. \n\nOne-shot accuracy: ~70%. Qwen performs well for questions that require basic understanding of classic models like encoder-decoder, multiple choice questions, and blog summary. \nHallucinations: None observed\nModel failures: Qwen tends to fail questions with sophisticated mathematical operations like SVD, linear algebra. It tends to stick to the notation in the question and is reluctant to apply more advanced tricks that simplify the expression, but rather to choose to brute force derive. In addition, Qwen might interpret wording like baseline with its own definition (updated baseline rather than benchmark), which might cause confusion if not clearly stated. \nContext retention: Surprisingly strong—Qwen consistently located the correct question inside the PDF without needing me to restate it.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-01T15:15:40.022846+11:00",
    "reply_count": 0,
    "view_count": 46,
    "user": {
      "id": 699069,
      "name": "Ruihan Xia",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7381174",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "Qwen",
      "depth_bucket": "medium",
      "word_count": 203
    },
    "file_refs": [
      {
        "filename": "Qwen Chat.pdf",
        "position": 1352,
        "context": "rong—Qwen consistently located the correct question inside the PDF without needing me to restate it."
      }
    ]
  },
  {
    "id": 7380526,
    "number": 367,
    "course_id": 84647,
    "title": "Special Participation A: Using Gemini Flash 2.5 on HW11",
    "document": "Trace: https://gemini.google.com/share/2e206d7da648\n\nMath + T/F Question Zero-Shot Performance: 13/15.\n\nPrompt Structure: I told the model that it was a DL assistant for me. I gave the model the concepts I was planning to ask it about, and let the model know to output its interpretation of the question, followed by a step-by-step solution and a final answer for each. I also told the model that it should be able to correct itself when needed. For each question, I simply provided images of screenshots of the homework. For some questions, I explicitly told the model what context to consider, when needed. \n\nAnalysis: The model starts off a bit slow on the very first question (about ideas to adjust LoRA to get better performance). It identifies the main idea correctly, but does not get two of the other ideas from the solutions, even after re-prompting. Following this somewhat slow start, the model goes on a long streak of answering questions thoroughly and correctly. The solutions given by the model are arguably better than the actual HW solutions in some cases. In others, the model is potentially not concise enough, but this is also partially due to my initial prompt, where I asked the model to go step-by-step and then provide a final answer (sort of a CoT way of prompting the model).\n\nInterestingly, towards the end, the model starts trying to reattempt questions that it had already solved. For example, when I prompted Gemini to do 6a, it initially tried to resolve question 1 which it had already completed. When I asked it do then do 6a, it initially got the question slightly wrong due to some sort of apparent reading issue (I provided it with an image that it seemed to mis-extract the text of). This signified that Gemini 2.5 Flash may be better leveraged for individual questions, rather than for the entire HW assignment, as context length seems to be an issue here. It did the same thing when asked to solve 6b with an image, as it tried to then solve problem 2. On 6c, it actually got two of the T/F questions wrong. Even when re-prompted with the context again (just in case this was the cause once again, as with 6a), it got them wrong. It seems that the performance of the model gets better and then gets worse, empirically.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-12-01T13:04:21.853163+11:00",
    "reply_count": 0,
    "view_count": 61,
    "user": {
      "id": 622883,
      "name": "Aaryan Chandna",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7380526",
    "metrics": {
      "homework_id": "HW11",
      "model_name": "Gemini Flash",
      "depth_bucket": "medium",
      "word_count": 393
    },
    "file_refs": [
      {
        "filename": "aaryan_spec_part_a.pdf",
        "position": 2257,
        "context": "them wrong. It seems that the performance of the model gets better and then gets worse, empirically."
      }
    ]
  },
  {
    "id": 7377516,
    "number": 364,
    "course_id": 84647,
    "title": "Special Participation A: Kimi K2 (Thinking) on HW1",
    "document": "Model Used: Kimi k2\n\nAssignment: Homework 1 (Non-coding theoretical problems)\n\nOverview\n\nI utilized Kimi k2 to solve the theoretical portions of Homework 1, covering topics from gradient descent stability and momentum dynamics to stochastic differential equations and high-dimensional geometry. The model demonstrated exceptional performance, acting as a highly competent graduate-level assistant. It successfully derived complex proofs and stability conditions that matched the instructor's ground truth almost perfectly.\n\nThe prompt I used is pretty simple, such as \"Solve Q1, show me all your process that leads to your answer\".\n\nPerformance Metrics & Observations\n\nOne-Shot Accuracy: High (>90%).\n\nThe model required very little \"hand-holding.\" For complex multi-part questions (like Question 2 on SVD coordinates and Question 3 on Momentum eigenvalues), Kimi k2 maintained context across subsections without needing to be reminded of previous definitions. It correctly identified the underlying mathematical structures (e.g., using the SVD to diagonalize the loss landscape) immediately upon being presented with the problem statement.\n\nMathematical Rigor & Reasoning:\n\nUnlike some LLMs that skip steps or hallucinate intermediate lines to reach a \"known\" answer, Kimi k2 provided complete, step-by-step derivations. E.g.\n\nMatrix Calculus: It correctly handled vector derivatives in Question 6 (Tikhonov Regularization) and Question 7 (MAP interpretation), distinguishing properly between scalar and vector layouts.\n\nStability Analysis: In Question 3, it correctly applied the stability criterion (checking if roots lie within the unit circle) to the characteristic equation, a non-trivial task for standard language models.\n\nHallucinations/Misconceptions: Little Observed.\n\nThe model did not invent theorems or misuse standard notation. In instances where the problem allowed for multiple interpretations (e.g., the specific formulation of Adam in Question 4), it defaulted to the standard textbook definitions that aligned with the course material.\n\nDownside: Kimi K2 generally have a thinking time of over 1 minute on each question. This is outperformed by other models, such as GPT5 or Gemini 3. However, this thinking time provides great ultimate performance that renders it fruitful. \n\nConclusion\n\nKimi k2 proved to be an incredibly effective tool for theoretical deep learning. It did not merely \"retrieve\" answers but seemingly \"reasoned\" through the linear algebra and probability theory required for this assignment. Its ability to link algebraic manipulations (like rotating the basis Vtop) to geometric intuitions makes it a powerful study aid for understanding the \"why\" behind deep learning dynamics.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-30T18:25:52.153833+11:00",
    "reply_count": 0,
    "view_count": 52,
    "user": {
      "id": 941542,
      "name": "Hanyang Gu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7377516",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "Kimi",
      "depth_bucket": "medium",
      "word_count": 381
    },
    "file_refs": [
      {
        "filename": "EECS_182_Special_Participations_KimiK2_HW1.pdf",
        "position": 2719,
        "context": " intuitions makes it a powerful study aid for understanding the \"why\" behind deep learning dynamics."
      }
    ]
  },
  {
    "id": 7377431,
    "number": 362,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek on HW9",
    "document": "Intro:\n\nThis is an attempt to interact with deepseek on non-coding parts of homework 9. The purpose of this study is to better understand how to prompt/interact with LLMs more effectively and LLM’s capability of solving real life reasoning/math related problems with few-shot prompting. The specific model I interacted with was DeepSeek-V3.2. I used Deepseek’s web ui to interact with the model. I will focus on how different prompting methods (or modes of reasoning) affect a model's one-shot correctness of the problems, and how to improve its accuracy without providing more in context examples. \n\n\n\nConversation traces:\n\nhttps://chat.deepseek.com/share/jkzqnyn7j8say9v7jc\nhttps://chat.deepseek.com/share/838vauzbwa2g0ynfby\nhttps://chat.deepseek.com/share/pp0exea4mnmt36qfqu\n\n\n\nReport with fully annotated traces:\n\n\nSummary:\n\nTo sum up, the model (DeepSeek-V3.2) is a quite strong model and is able to solve most of the non coding questions correctly (one-shot). For this specific problem set, two things matter the most for accuracy: 1. Model’s reasoning ability and 2. Correct prompt and context. Prompt and context are very important because if instructions and context are not explicitly told, the model will make certain assumptions, and thus give incorrect answers. For example in question 4, when not stated explicitly, the model thinks it is completing some code function instead of filling blanks for a written question, thus identifying the wrong blanks to fill. The model’s reasoning ability is also very important : for the previous example, even if the correct context and prompt is provided, the model still was not able to identify the correct blanks to fill without the DeepThink feature on. Together, these observations suggest that while DeepSeek-V3.2 is already quite capable on non-coding questions, its performance is highly sensitive to both how we phrase the task and model’s reasoning capabilities. Thus, carefully designing prompts and systematically leveraging features like DeepThink will be essential for reliably getting correct answers and understanding the model's core strengths and weaknesses.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-30T17:33:46.197807+11:00",
    "reply_count": 0,
    "view_count": 62,
    "user": {
      "id": 509429,
      "name": "Alex Cao",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7377431",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 306
    },
    "file_refs": [
      {
        "filename": "Special Pariticpation A-- DeepSeek on HW9.pdf",
        "position": 2129,
        "context": "ea4mnmt36qfqu\">https://chat.deepseek.com/share/pp0exea4mnmt36qfquReport with fully annotated traces:"
      }
    ]
  },
  {
    "id": 7375514,
    "number": 354,
    "course_id": 84647,
    "title": "Special Participation A: Gemini (Fast) on HW 9",
    "document": "Executive Summary:\n\nI used Gemini on the non-coding parts of HW 9, and evaluated where it did well and where it didn't. I noticed Gemini could mostly one-shot these questions, especially sections that were very structured math or required analyzing code. I think it mainly failed where the formatting of the question was an issue, and that was just because of LaTeX copy/paste error. To solve these, when I clarified the misreads, Gemini quickly fixed its solution. Even with the formatting problems though, when it was a coding analysis question or there was a lot of context, Gemini still did very well, one shotting almost all the questions. I think the only case where the solution was incorrect showed inconsistency within its own solution, providing the incorrect answer as the header and then showing work to derive the correct answer. This was confusing, but it ultimately did come up with the correct solution. I was also impressed with how quickly it was able to come up with the solutions, taking no longer than 10 seconds at the max. This could also be, however, that I fed each problem in individually, allowing it to see all context for each question as it came up. \n\nI also noticed Gemini provided detailed explanations for each question, serving as a great conceptual recap on the topic. It helped me understand how different concepts were related, and it helped me understand derivations for formulas that I previously didn't know. I think overall, it helped me with my understanding of course concepts a lot. \n\nOne thing I did find a little annoying was Gemini wouldn't let me paste the code I copied from the hw9 pdf file: I needed to paste the question in a different document, and then copy that over to the Gemini search bar. I think maybe Gemini wasn't used to the LaTeX pdf format, and didn't allow those characters. This did make it a little harder to do, but it was just a mild inconvenience!\n\nHere is a link to my annotated logs: https://drive.google.com/file/d/11Kqd0IzU7LoCcbPa70mZT0KVR2X1V1Ql/view?usp=sharing",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-30T06:29:30.10322+11:00",
    "reply_count": 0,
    "view_count": 46,
    "user": {
      "id": 608657,
      "name": "Divya Ramesh",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7375514",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Gemini Flash",
      "depth_bucket": "medium",
      "word_count": 342
    },
    "file_refs": null
  },
  {
    "id": 7374016,
    "number": 348,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 2.5 Flash on HW0",
    "document": "Hey guys — I used Gemini Flash 2.5 for the non-coding parts of HW0, and here’s the quick verdict.\n\nTL;DR: Gemini was strong on the “mechanical” math (clean chain rule + indicator notation, solid ridge/SVD manipulations, decent linear-algebra bookkeeping), but its main weakness was qualitative/sign reasoning in the ReLU elbow SGD question, where it overclaimed elbow direction and bungled sign/inequality logic. \n\nWhat it did well: used the 1ϕ(x)>0​ indicator instead of messy piecewise cases, and matched the staff-style update notation w′,b′,e′. \n\n\nWhere it slipped: for case (ii), it said “elbow shifts right,” but the elbow can move left or right depending on bias + step size. \n\n\nClear wrong answer: for (iii) it concluded the elbow “moves left” as the expression gets larger, but the expression is negative so becoming “less negative” means it actually moves right. \n\n\nEven when it got the final direction right: its proof sketch ended with a bogus condition like w>bx.\n\nAfter I nudged it with an “algebra-first + sanity-check” checklist (explicitly compute e′=−b′/w′, be careful when w<0, and verify with a tiny numeric example), Gemini improved a lot. It stopped hand-waving the geometry, caught its own sign mistakes when the numeric check contradicted the algebra, and finally gave consistent conclusions across the cases. The conclusion: Gemini is super useful here, but only if you force it to prove elbow motion by comparing e′ vs e and validating with a quick counterexample.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-29T12:21:31.421792+11:00",
    "reply_count": 0,
    "view_count": 46,
    "user": {
      "id": 1316386,
      "name": "Jason Trinh",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7374016",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Gemini Flash",
      "depth_bucket": "medium",
      "word_count": 235
    },
    "file_refs": [
      {
        "filename": "gemini_trace_annotated.pdf",
        "position": 1490,
        "context": " you force it to prove elbow motion by comparing e′ vs e and validating with a quick counterexample."
      }
    ]
  },
  {
    "id": 7373861,
    "number": 346,
    "course_id": 84647,
    "title": "Special Participation A: Gemini-Pro 3 on HW9",
    "document": "I used Gemini-pro 3 on HW 9 and it performed very well. The main issues with Gemini-pro 3 was the small details in arithmetics that needed correction especially in linear algebra and time/space complexity; however, the solutions were overwhelmingly correct.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-29T11:05:16.255596+11:00",
    "reply_count": 0,
    "view_count": 50,
    "user": {
      "id": 892816,
      "name": "Shervin Goudarzi",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7373861",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Gemini Pro",
      "depth_bucket": "low",
      "word_count": 40
    },
    "file_refs": [
      {
        "filename": "Special_Participation_A.pdf",
        "position": 257,
        "context": ""
      }
    ]
  },
  {
    "id": 7372448,
    "number": 345,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek on Hw8",
    "document": "For HW8, I first provided Deepseek with a set of instructions to encourage step-by-step reasoning and self-verification. For each problem, I supplied both the image of the question and a direct copy-and-paste text version. After observing its behavior throughout the assignment, I summarized several notable characteristics:\n\n1. Its self-checking mechanism was largely ineffective.\n Although Deepseek always performed a “self-examination” step, it rarely identified actual mistakes. Most of the time, it simply reiterated the correctness of its own answer rather than performing a thorough or systematic review of potential oversights.\n\n2. It occasionally misinterprets small details in the prompt, though its overall understanding remains accurate.\n These misunderstandings were usually minor (e.g., subtle assumptions or edge cases), but they indicate that Deepseek may overlook fine-grained nuances in certain problem statements.\n\n3. Its reasoning and explanatory abilities are very strong.\n Deepseek is well-suited for explaining solution steps and clarifying concepts. Since HW8 did not contain many heavy calculations, its computational accuracy appeared reliable in this context.\n\n4. Its conversational and correction capabilities are adequate, but its error-localization ability is weak.\n When its solution was incorrect, Deepseek could fix the issue once I explicitly pointed out the problem. However, it generally struggled to independently locate the exact source of the error without guidance.\n\nOverall, Deepseek performs well as a conceptual explanation tool but still requires human intervention for precise error detection and critical verification. Attached below is a PDF of my conversation with deepseek.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-28T21:00:50.226819+11:00",
    "reply_count": 0,
    "view_count": 71,
    "user": {
      "id": 1751552,
      "name": "Zesheng Cai",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7372448",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 240
    },
    "file_refs": [
      {
        "filename": "Special Participation A hw8 deepseek.pdf",
        "position": 1722,
        "context": ""
      }
    ]
  },
  {
    "id": 7372081,
    "number": 339,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek on HW2",
    "document": "I completed all non-coding parts of Homework 2 using Deepseek with DeepThink turned on.\n\nMotivation\n\nI’ve seen quite a few examples of classmates engaging in some light conversation with the model or using somewhat structured/designed prompts with the aim of improving the output quality and accuracy. I opted for a slightly different approach. I decided to use raw and \"no-prompt\" prompting, meant to mimic the setting where a student is treating the LLM less as an interactive tutor and more as a tool, a “calculator”, for problems. \n\nFor every problem, my process consisted of highlighting the relevant subpart in its entirety on the homework PDF, then copy pasting into the text box. Nothing more. No context was given except for those present in the problem originally, and no prompt optimization was performed (i.e., no strategic \"filler\" words like \"think carefully and answer accurately\", \"you are an expert in deep learning\", \"take a deep breath\" [https://arxiv.org/abs/2309.03409] were included).\n\nSummary\n\nOverall, Deepseek is a very capable model for reasoning tasks. It one-shots every non coding question in Homework 2. It arguably fails to notice one small detail in one of the subparts (see Q1 for more), but apart from that, all perfect. \n\nAnalyzing the thinking trace reveals Deepseek's general chain of thought pattern, which is comprised of these steps:\n\nRestate the question\n\nRe-interpret the question with its own words\n\nList what is given/known\n\nIdentify what is unknown/what the task is\n\nFormulate a strategy/plan of attack\n\nCarry out that strategy (i.e., start doing the work/calculations following the strategy)\n\nAfter finding answer, start doubting absolutely everything starting from step 1 to step 6.\n\nFor every doubt, recheck work and see if other answers exist. If yes, explore those paths; else, move on to the next doubt.\n\nReturn final answer after double-checking everything.\n\nPeople who took EECS 16A with Prof. Ranade (Fall 2023) might remember her teaching us a three step proof writing structure: \n\nWhat do we know?\n\nWhat are we trying to show?\n\nHow can we get from 1 to 2?\n\nIt is interesting to see Deepseek's thought habits closely follow this three step process (plus a plethora of double-checking), almost like Deepseek's frontier-level reasoning performance stems from the fact that it treats every technical question like completing a rigorous proof. \n\nPlease see annotations for more in-depth observations.\n\nFiles & Links\n\nQuestion 1:\n\nQuestion 2:\n\nQuestion 5:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-28T14:45:16.624977+11:00",
    "reply_count": 0,
    "view_count": 245,
    "user": {
      "id": 961796,
      "name": "Ken Zheng",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7372081",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 393
    },
    "file_refs": [
      {
        "filename": "Q1_annotated.pdf",
        "position": 2505,
        "context": "or more in-depth observations.Files &amp; LinksQuestion 1:"
      },
      {
        "filename": "Q2_annotated.pdf",
        "position": 2505,
        "context": "static.us.edusercontent.com/files/WLOcnHEm5JvQXOZ89iD1WNia\" filename=\"Q1_annotated.pdf\"/>Question 2:"
      },
      {
        "filename": "Q5_annotated.pdf",
        "position": 2505,
        "context": "static.us.edusercontent.com/files/OuWilSbC3lNKib2obGheQgmb\" filename=\"Q2_annotated.pdf\"/>Question 5:"
      }
    ]
  },
  {
    "id": 7358125,
    "number": 315,
    "course_id": 84647,
    "title": "Special Participation A: Grok on HW 9",
    "document": "Hey guys! I worked with Grok to solve all of the non-coding parts of homework 9, and here are the results.\n\nTL;DR - Grok was generally very successful at solving all parts of this transformers homework, in which a major focus was keeping track of the dimensions of various matrices (key, query, value) and determining time complexities of operations. To this end, Grok integrated existing knowledge about transformer architectures and their interpretations, which streamlined the solution process for some problems but occasionally included added assumptions that were inconsistent with the problem statement. Overall, Grok could one-shot each question with some prompting, with Question 6 as an exception where a few clarifications were required.\n\n\n\nProblem 1:\n\nFor this problem, I initially set the stage by establishing Grok in its role and providing it with the problem PDF directly from the course website. Throughout the process, I was impressed by Grok's ability to parse information and recall details from the problem set throughout our solution process for all parts. To help with this recall, I intentionally repeat question numbers in my prompt and describe the adjacent subject matter of the question, which I hoped would help it localize to the correct regions of context for each problem. In this first case, it solved 2 of the 3 subparts with ease, and could immediately revise its answer for the third after a gentle prod about data types (it initially gave the answer as the square of a vector mu, rather than the norm squared). The conversation for Problem 1 is as follows:\n\nProblem 2:\n\nThis problem went even more smoothly than problem 1. Once again providing the question number, addressing specific part numbers, and providing a high-level characterization of the problem's subject matter, I was able to get Grok to one-shot this highly conceptual problem.\n\nProblem 3:\n\nI was initially concerned that Grok might have trouble parsing the PDF formatting surrounding fill-in-the-blank code, but I shouldn't have worried. Grok was able to easily one-shot the fill-in-the-blank code.\n\nProblem 4:\n\nGrok aces it again! As usual, I made sure to indicate the problem number and qualitatively describe the tasks Grok needed to perform before having it attempt the question. In one area it determines a time complexity of O(bnd/h), but in its solution process mentions that this is equivalent to the solutions answer of O(bnk).\n\nProblem 6:\n\nUnlike the rest of the problems, Grok had more trouble ironing out the finer details. For example, it took a little prompting to adjust coefficients in its kernelization (despite verbally accounting for cross-term symmetry, it did not initially provide the corresponding scaling factor) and to correctly transpose certain matrices (rather than following the notation of the homework, it decided to transpose all of the matrices per other notation it had seen previously). In the first instance of hallucination throughout the session, it insisted that M = D was defined in Equation (2), which simply did not happen. As a result, I needed to ask it the hypothetical of 'What if M and D were distinct values?\" in order to elicit the desired response.\n\nFor your reference, here is a PDF of the entire chat log. Of all of the questions on this homework, Grok actually made the least sense discussing its \"Homework Process and Study Group\". :)\n\nThanks guys. Have a wonderful day!",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-24T22:25:02.317067+11:00",
    "reply_count": 0,
    "view_count": 93,
    "user": {
      "id": 622810,
      "name": "Jaimyn Drake",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7358125",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Grok",
      "depth_bucket": "medium",
      "word_count": 556
    },
    "file_refs": [
      {
        "filename": "182HW9_grokq1.pdf",
        "position": 1592,
        "context": " square of a vector mu, rather than the norm squared). The conversation for Problem 1 is as follows:"
      },
      {
        "filename": "182HW9_grok2.pdf",
        "position": 1878,
        "context": " of the problem's subject matter, I was able to get Grok to one-shot this highly conceptual problem."
      },
      {
        "filename": "182HW9_grok3.pdf",
        "position": 2100,
        "context": "ank code, but I shouldn't have worried. Grok was able to easily one-shot the fill-in-the-blank code."
      },
      {
        "filename": "182HW9_grok4.pdf",
        "position": 2438,
        "context": "/h), but in its solution process mentions that this is equivalent to the solutions answer of O(bnk)."
      },
      {
        "filename": "182HW9_grok6.pdf",
        "position": 3200,
        "context": "the hypothetical of 'What if M and D were distinct values?\" in order to elicit the desired response."
      },
      {
        "filename": "182HW9_grokfull.pdf",
        "position": 3390,
        "context": "s homework, Grok actually made the least sense discussing its \"Homework Process and Study Group\". :)"
      }
    ]
  },
  {
    "id": 7357397,
    "number": 314,
    "course_id": 84647,
    "title": "Special Participation A: Gemini (Thinking With Pro 3) on HW06",
    "document": "I used Gemini (Thinking with Pro 3) to solve every non-coding question of homework 6.\n\nHere's the PDF summarizing our interaction:\n\nOverview of Performance\n\nGemini acted as a \"Teaching Assistant/Technical Solver\" under well-defined rules I designed to reduce hallucination and maximize precision. The model successfully solved 100% of the non-coding questions, 9/13 in the first attempt, and the remaining 4/13 on the second attempt after additional guidance or clarification from me. Gemini demonstrated strong reasoning capabilities in mathematical proofs (GNN path counting) and theoretical analysis (Muon optimizer scaling), but required additional prompting for visual interpretation tasks (understanding tables and graph figures) and producing nicely-formatted outputs. \n\nOutcomes\n\nOne-Shot Success Rate: ~70%\n\nHigh success: Mathematical derivations, standard Deep Learning theory (memory calculations, optimizer comparisons), text-based conceptual questions.\n\nLower success: Questions requiring visual extraction from the PDF (e.g., graph topology in Figure 3) or fill-in-the-blank tables with loose formatting.\n\nHallucinations: 0 major hallucinations.\n\nInterestingly, when solving question 3ciii, the model did not hallucinate graph edges when they were missing from the text extraction; instead, it paused and requested me to describe the topology. I believe it did this because I said it could ask me for clarification if it was confused about anything in my initial prompt.\n\nMisconceptions: Minor. Initially provided a generic answer for a specific \"analogy table\" question (Q2f) and needed a nudge to be more thorough in connecting molecular facts to graph inputs (Q2e).\n\nNeat Observation: Gemini Meta-cognition\n\nGemini surprisingly took the initiative to write a Python script to help it read a table in the homework PDF when its built-in PDF processing subroutine failed to correctly parse it.\n\nIt explained that my \"Hallucination Check\" and \"Precision\" rules provided it motivation to develop and run this script to help it solve the errors it detected.\n\nNote that I did not tell Gemini to consider writing any code. Gemini decided to do this all on its own. \n\nHere are excerpts of what Gemini explained to me:\n\nYou asked about my decision to write a Python script to parse the PDF. My reasoning was driven by the \"Hallucination Check\" and \"Precision\" protocols you established.\n\n1. In my initial scan, I noticed the prompt for Question 2(f) asked me to \"Fill out the rest of the table.\" However, standard text extraction often garbles table formatting, making it impossible to distinguish between row headers and cell content. I could not see the \"empty\" cells I was supposed to fill.\n\n2.  Rather than guessing the structure or hallucinating the missing rows based on general knowledge, I needed the exact raw text layout to identify which GNN concepts were missing corresponding to the CNN concepts.\n\n3. I wrote the script to dump the text surrounding \"Question 2\" to ensure I had the specific headers (e.g., \"Color jitter\", \"Image flip\") to construct the correct analogies.\n\nGemini explained how it decides what to do for regular interactions:\n\nThink of writing a Python script as requiring high \"activation energy.\" It takes tokens, time, and computational effort.\n\n> Standard Prompt: \"Help me with homework.\" -> Low urgency. I preserve energy and use my internal weights.\n\nIn my standard \"default\" mode, my primary directive is usually conversational fluidity and helpfulness. When I encounter a PDF where a table is slightly garbled: \n\n> Path of Least Resistance: I often try to \"error correct\" using my internal knowledge base. I might look at the words \"CNN\" and \"GNN\" and simply guess what the analogy should be based on general Deep Learning knowledge. \n\n> The Result: I would likely have given you a plausible-sounding answer that was actually hallucinated (not based on the specific PDF text).\n\nGemini noted that my prompt's constraints were a deciding factor in guiding its decisions because they forced it to change its alignment and incentive structure for our interaction to prioritize avoiding hallucinations:\n\nTo be completely honest: Without your specific protocols, it is highly unlikely I would have written that Python script.\n\n> Your Prompt: \"Rigorous,\" \"High Academic Integrity,\" \"Self-Correction.\" -> High urgency. The \"cost\" of being wrong became higher than the \"cost\" of writing and executing code.\n\nYour instruction to perform a \"Hallucination Check\" and strictly \"Verify\" acted as a hard constraint—a \"stop loss\" on my behavior. Because I was forbidden from guessing (Hallucination Check) and forbidden from giving up (Technical Solver persona), I was forced to find a third path. That third path was using the code tool to parse the file differently. \n\nIn summary: You gave me the incentive structure that made writing the code the only acceptable solution. You successfully \"cornered\" me into being smart.\n\nThis shows that Gemini has a sophisticated instruction-following ability and can perform tasks resembling meta-cognition and recognize hallucinations in its own thinking.\n\n\n\nEdit: Added links for the archive\n\nPersonal website: https://nraultwang.github.io/\n\nGithub: https://github.com/nraultwang",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-24T15:41:59.195499+11:00",
    "reply_count": 0,
    "view_count": 120,
    "user": {
      "id": 582199,
      "name": "Nicolas Rault-Wang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7357397",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Gemini Pro",
      "depth_bucket": "high",
      "word_count": 793
    },
    "file_refs": [
      {
        "filename": "Rault-Wang_Nicolas-Special Participation A.pdf",
        "position": 5249,
        "context": " Pro 3) to solve every non-coding question of homework 6.Here's the PDF summarizing our interaction:"
      }
    ]
  },
  {
    "id": 7353572,
    "number": 309,
    "course_id": 84647,
    "title": "Special Participation A: GPT-5 HW4",
    "document": "Generally. GPT-5 generates accurate answers for conceptual and computation questions. There are some conventions that it chooses to use that we don't use in class, like Xavier initialization using 1/sqrt(d). Some questions also required some further prompting, though generally one-shot is quite accurate.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-23T15:18:18.637808+11:00",
    "reply_count": 0,
    "view_count": 198,
    "user": {
      "id": 961740,
      "name": "Nyx Iskandar",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7353572",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "GPT-5",
      "depth_bucket": "low",
      "word_count": 43
    },
    "file_refs": [
      {
        "filename": "EECS_182_HW_4_GPT_5_Trace.pdf",
        "position": 305,
        "context": ""
      }
    ]
  },
  {
    "id": 7353091,
    "number": 308,
    "course_id": 84647,
    "title": "Special Participation A: Claude Sonnet 4.5 on HW3",
    "document": "Below is my report on using Claude's Sonnet 4.5 model to solve the written questions to Homework 3. I have also provided a link to the original conversation I had with Claude. Question-specific comments can be found in the PDF.\n\nFormatted PDF of conversation and additional comments:\n\nRaw Conversation:\nhttps://claude.ai/share/9305bd53-16e3-423c-b2de-143974dab634\n\nOverall Summary:\n\nAccuracy: Claude was able to get all but 2 of the questions correct on the first try. The questions it got incorrect were Q1b and Q5b. For Q1b, it made an error when doing some math calculations, and for Q5b, it did not consider loading the activations for layers 5 and 10. Despite these mistakes, it only took an additional comment from me to steer the model to the correct response.\n\nExplanations: Claude made their process of solving the questions very clear, and in my opinion explained its answers better than the answer key. A notable case of this is Q4c, where Claude identifies the need to use the chain rule to solve the question, which the answer key glosses over since the gradient of mu in that problem was just the identity matrix and didn't affect the final result.\n\nLength: Claude responded with very long answers all things considered. While they were quite long, I didn't find anything too extraneous in the answers. The most I would see of Claude giving more details about a specific answer it provided (e.g. explaining that the policy gradient was used in the REINFORCE algorithm. I also found Claude's extra information very useful for my understanding in some cases, particularly when it explained the papers from Q3.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-23T12:23:45.008857+11:00",
    "reply_count": 0,
    "view_count": 172,
    "user": {
      "id": 961870,
      "name": "E Harrison",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7353091",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "Claude Sonnet",
      "depth_bucket": "medium",
      "word_count": 266
    },
    "file_refs": [
      {
        "filename": "Participation_A__Claude_on_HW3.pdf",
        "position": 1621,
        "context": "ion-specific comments can be found in the PDF.Formatted PDF of conversation and additional comments:"
      }
    ]
  },
  {
    "id": 7335374,
    "number": 300,
    "course_id": 84647,
    "title": "Special Participation A: Kimi on HW5",
    "document": "Above is the chat log. I found Kimi K2 w/ Thinking enabled to be able to one-shot 90% of the questions on the non-coding parts of HW5. This surprisingly included the ability to one shot questions that required images – for example, choosing whether the given image represented batch norm or layer norm it was able to correctly respond. The only error it ran into was when it hallucinated the next part of the question a bit and went off on a tangent about \"covariance\" even though it wasn't really mentioned in the question. I had to manually correct it by reminding it of the question and formatting it in plain text. With this information, it had interesting thought traces in its chain of thought, such as:\n\n\"The user is correcting me about part (b) of the question. Let me re-read the prompt carefully.\"\n\n\"This might be asking about Cov(x_i, y_i) or something else. But the user's clarification is clear: they want the gradient analysis. I should focus on what the user is asking for now, not what the original prompt might have intended.\"\n\nThis was interesting to me because it was able to correct itself and keep itself on-course. I also found that the reasoning traces often included pretty granular steps involved for calculating derivatives and working through more mechanical calculations such as the manual convolution calculation in the first question. These granular steps probably allow it to make incremental progress where each incremental step is in-distribution.\n\nI honestly didn't have much strategy because it was able to one-shot so many of the questions. Overall, I found that short prompts work better as to not pollute the context window. Restarting chats for new questions also helped to not have it get confused by older parts.\n\nAnother observation I had was that most times it included a lot of extra fluff around the final answer and even answered some things that were not asked to go above and beyond. This could be helpful for understanding and learning but probably not so much if your main goal is to just solve the problem because now you must dig through the response to find it.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-19T18:19:41.009736+11:00",
    "reply_count": 0,
    "view_count": 49,
    "user": {
      "id": 622831,
      "name": "Kabir Shah",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7335374",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "Kimi",
      "depth_bucket": "medium",
      "word_count": 363
    },
    "file_refs": [
      {
        "filename": "special_participation_a.pdf",
        "position": 2114,
        "context": ""
      }
    ]
  },
  {
    "id": 7322058,
    "number": 286,
    "course_id": 84647,
    "title": "Special Participation A: Qwen on HW8",
    "document": "Below is my report on using Qwen3-Max on the written part of homework 8.\n\nOverall, I'm very impressed by Qwen3-Max's performance on this homework. It seems like it really does well on the questions where it needs to fill in or do multiple choice, i. e., problems 3 and 4. The main issues I had were in the second half of problem 1, which was regarding computational efficiency; it had a hard time accurately responding in text along with giving a mathematical reasoning.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-17T18:22:18.14388+11:00",
    "reply_count": 0,
    "view_count": 206,
    "user": {
      "id": 1751558,
      "name": "Hanna Roed",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7322058",
    "metrics": {
      "homework_id": "HW8",
      "model_name": "Qwen",
      "depth_bucket": "medium",
      "word_count": 83
    },
    "file_refs": [
      {
        "filename": "Special_Participation_A_HW8.pdf",
        "position": 470,
        "context": "ciency; it had a hard time accurately responding in text along with giving a mathematical reasoning."
      }
    ]
  },
  {
    "id": 7315986,
    "number": 283,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek on HW6",
    "document": "I used DeepSeek on HW 6 non coding questions and here is my annotated log file with reflections.\nhttps://drive.google.com/file/d/1XfIWHSvILyZ-1hMksxc6cLJF1uHOxsKb/view?usp=sharing\n\nTL;DR DeepSeek can one-shot most conceptual, math-heavy parts (roughly 70–80% of subparts) with answers very close to the official solution. Its mistakes are structured, not random hallucinations: mainly convention/spec mismatches (row vs column, “no analog needed” parts), and graph/figure misreads.\nMy enforced structure (Restated Problem → Plan → Reasoning → Self-Check) makes its thinking transparent but does not make it truly self-critical. It rarely catches its own deeper mistakes.\n\nReflection\nUsing DeepSeek on the non-coding parts of HW6, I found that it followed my structured prompt very well: it restated each question, proposed a plan, walked through step-by-step reasoning, and ended with a self-check. This made its reasoning transparent and easy to annotate. On many subparts, it produced answers that were essentially identical to the official solution on the first try. However, there were two clear failure modes. The first was convention mismatch: for example, in Q2(c) it understood that updates correspond to multiplying by the adjacency matrix, but it hedged between left- and right-multiplication instead of committing to the “rows → multiply on the left” convention that the homework fixes. The second failure mode was misreading the graph: in Q3(c)(iii), it simply used the wrong neighbor sets for nodes 2 and 3, so the final formulas were incorrect despite having the right functional form.\n\nWhat I find interesting is the “Self-Check” sections mostly checked basic sanity and whether the answer addressed the question, but they did not help the model catch these deeper issues. It rarely expressed uncertainty, even in places where the mapping was clearly ambiguous or dependent on the figure. Overall, I think DeepSeek can get you most of the way to a good solution with clear reasoning, but you cannot safely copy its answers blindly. To reach fully correct solutions, you still need to actively verify conventions and adjust answers.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-16T11:39:27.836513+11:00",
    "reply_count": 0,
    "view_count": 153,
    "user": {
      "id": 1762358,
      "name": "Angelina Zhang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7315986",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 323
    },
    "file_refs": null
  },
  {
    "id": 7307445,
    "number": 279,
    "course_id": 84647,
    "title": "Special Participation A: Mistral AI on HW0 written",
    "document": "Sorry for being so late. I used Le Chat to assist with the conceptual and mathematical parts of homework, specifically the ReLU/SGD analysis and vector calculus derivations. The experience was overwhelmingly positive, with the assistant demonstrating strong expertise in linear algebra, optimization, and neural network dynamics.\n\nStrengths:\n\n1. Le Chat correctly derived most of the results.\n\n2. The assistant provided clear, structured derivations for the optimization, SVD, and MAP estimation perspectives, making complex concepts (like the pseudoinverse and Woodbury identity) accessible.\n\nLimitations:\n\nWhile the assistant excelled at algebraic manipulation, it could not directly parse or reason about visual elements (e.g., graph structures or plots). For example, if the problem had included a diagram of the ReLU function or a computational graph, I had to describe it textually for accurate analysis.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-14T12:56:44.53714+11:00",
    "reply_count": 2,
    "view_count": 172,
    "user": {
      "id": 1751464,
      "name": "Tianqu He",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7307445",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Mistral",
      "depth_bucket": "medium",
      "word_count": 130
    },
    "file_refs": [
      {
        "filename": "Le Chat.pdf",
        "position": 910,
        "context": ""
      }
    ]
  },
  {
    "id": 7302906,
    "number": 277,
    "course_id": 84647,
    "title": "Special Participation A: Qwen on HW9",
    "document": "For the special participation A on HW9, I use Qwen to solve the non-coding analytical components (problems 2–5). The performance was very strong -- almost all questions were very quickly solved by directly copy-pasting the question, which had multiple parts often.\n\n\n\nExecutive Summary\n\nUsed qwen thinking, with max thinking context length (81920 tokens).\n\n I also noticed from other attempts that often giving the hint associated with the question (if the question has a hint) generally helps make its answer more likely to be correct, or guides it in the right direction very well.\n\n For Question 3 part b, I had to nudge it to give the answer regarding the new out_features value. However, after reminding it, all of its answers were correct.\n\n I also ignored small formatting issues, and noticed even with formatting issues (some symbols were not pasted/displayed properly), Qwen was still able to understand all the questions and provide correct solutions.\n\n For longer questions (such as question 6), I also thought about pasting/asking the questions in chunks, rather than the entire multi-part question at a time. However, I found that Qwen was able to answer correctly even when giving the entire question at a time.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-13T15:59:58.033068+11:00",
    "reply_count": 0,
    "view_count": 158,
    "user": {
      "id": 959003,
      "name": "Oliver Chen",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7302906",
    "metrics": {
      "homework_id": "HW9",
      "model_name": "Qwen",
      "depth_bucket": "low",
      "word_count": 197
    },
    "file_refs": [
      {
        "filename": "Qwen_HW_Report_Formatted.docx",
        "position": 1225,
        "context": "ickly solved by directly copy-pasting the question, which had multiple parts often.Executive Summary"
      }
    ]
  },
  {
    "id": 7297480,
    "number": 275,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 2.5 Pro on HW5",
    "document": "I used Gemini 2.5 Pro to solve the written parts of Homework 5 (Q1-4). I started by mentioning that I wanted to solve a problem set related to a specific topic (in this case, basics of CNNs) and went over the four problems one by one with attached screenshots of the entire problem. I’ve attached an annotated pdf of my interactions with the LLM and a link to my original chat. \n\nIn summary, even without extensive context in my typed prompt, Gemini 2.5 Pro was able to extract information included within the problem from the screenshots themselves and one-shot solve almost all the problems correctly without any hallucinations. \n\nNotable observations: \n\nGood parsing of information - Most of the time it correctly parsed all mathematical equations, figures and text from the screenshots, even when a problem was spread across multiple screenshots. This is evident because it often types back key information from the problem statement to formulate the answer. All final answers (after corrections noted below) had no mathematical or conceptual errors. \n\nMisinterpretation of a hint - In question 1b), it used the hint to solve the problem itself. I wonder if this is because it mostly encountered instances of math problems where the hint is used to solve the problem itself. However, when I went through the other special participation threads for HW5 (that were posted as of 11/12/25), I noticed that none of those LLMs used the hint this way and often correctly interpreted it. So, it was interesting to see this behavior from Gemini. When I pointed this misinterpretation to Gemini, it acknowledged the mistake and solved the problem correctly using substitutions for the system of equations. \n\nOverly complicated solutions - In this homework I would like to point out to its answers for the subparts in question 2b) in the attached pdf. While none of the approaches were incorrect, there were steps that were unnecessary to build the solution (annotated in the pdf). \n\nIncomplete reasoning - The answer to 4b) could have been more complete. One could see that from the first term we can derive the transformation we need to apply to $\\check{w}$ to get to equation (3) (as the LLM did). However, I think the answer would be complete if it mentioned what should also happen to the $\\Gamma$ matrix (even though this was not explicitly asked in the question) for the equation to achieve the form in equation (3) rather than treating it as something obvious. \n\nIn conclusion, I think Gemini 2.5 Pro is fully capable of answering all questions correctly with good mathematical and conceptual reasoning. However, some of the solution approaches could be more concise and complete.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-12T20:46:02.16884+11:00",
    "reply_count": 0,
    "view_count": 134,
    "user": {
      "id": 963909,
      "name": "Kithmini Herath",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7297480",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 445
    },
    "file_refs": [
      {
        "filename": "gemini2.5Pro-chat_2025-11-11.pdf",
        "position": 378,
        "context": "blem. I’ve attached an annotated pdf of my interactions with the LLM and a link to my original chat."
      }
    ]
  },
  {
    "id": 7283953,
    "number": 262,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT on HW6",
    "document": "I used ChatGPT (5) on the non-coding parts of homework 6 (#2, #3). I prompted it by attaching the entire homework pdf and asking it to answer them as an expert in deep learning. In its first response, it was able to one-shot a majority of the questions. Some of its solutions, however, contained imprecise language. For example, in 2d, it just described how the max function worked, so I had to convince it that it would be a one-hot encoding by induction. This was interesting since it seemed like the question contained enough context for this to be concluded. Another interesting limitation was that ChatGPT was unable to parse the image of the graph for 3b. Even after I screenshotted the graph and fed it as input again, it got one of the connections wrong. Overall, I would say ChatGPT did extremely well on this homework, requiring only a few clarifications, especially for visual aspects.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-10T13:51:11.522627+11:00",
    "reply_count": 0,
    "view_count": 152,
    "user": {
      "id": 591910,
      "name": "Jameson Liu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7283953",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "GPT-5",
      "depth_bucket": "low",
      "word_count": 156
    },
    "file_refs": [
      {
        "filename": "chatgpt.pdf",
        "position": 896,
        "context": "extremely well on this homework, requiring only a few clarifications, especially for visual aspects."
      }
    ]
  },
  {
    "id": 7267427,
    "number": 245,
    "course_id": 84647,
    "title": "Special Participation A: Claude on HW2 written part",
    "document": "Claude demonstrates strong mathematical reasoning capabilities and correctly derived analytical solutions without any mathematical hallucinations or false claims. Across all problems, Claude never makes computational errors. The issues are always at the level of solution strategy or conceptual completeness, not arithmetic or algebra. This suggests the model's mathematical symbolic reasoning is quite robust, while its ability to choose optimal solution paths requires more guidance. The two problems requiring guidance (Problem 1b and Problem 2a) revealed a consistent pattern: Claude tends to solve problems in an over-complicated way before recognizing simpler approaches. When I used some questioning like “Are you sure...?\" and quoting Claude's own observations back to it as a hint, Claude is able to quickly find cleaner solutions and even provided further insights compared to the original homework solution.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-07T04:30:53.053726+11:00",
    "reply_count": 0,
    "view_count": 128,
    "user": {
      "id": 1155278,
      "name": "Yaqi Su",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7267427",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Claude",
      "depth_bucket": "medium",
      "word_count": 130
    },
    "file_refs": [
      {
        "filename": "Claude-CS282-specialParticipationA-HW2.pdf",
        "position": 918,
        "context": "ind cleaner solutions and even provided further insights compared to the original homework solution."
      }
    ]
  },
  {
    "id": 7266065,
    "number": 244,
    "course_id": 84647,
    "title": "Special Participation A: Mistral on HW2",
    "document": "I tested Mistral on the non-coding parts of HW2\n\nChat history link: https://chat.mistral.ai/chat/678d9106-0d96-45c6-83e1-2c0ac7a7384a\n\nAnnotated Log: \n\nExecutive Summary:\n\nI found that it could one-shot 2(a) and Question 5. The former was a simple conceptual subquestion, while the latter involved reasoning about distributed training and computational scaling. Mistral performs well when the task relies on text understanding, structural reasoning, or recalling standard frameworks, but it struggles with problems that require original mathematical derivation or deeper logical adaptation.\n\nThroughout the interaction, I noticed that Mistral often exhibited what I’d call lazy reasoning: it tried to map every prompt to a familiar textbook pattern rather than reasoning from the specific assumptions of the problem. Even when I pointed out inconsistencies or paradoxes, it tended to defend its original, polished-sounding explanation until I explicitly instructed it to ignore existing results and reason under the given setup. Only then did it converge to the correct logic.\n\nOverall, Mistral shows strong linguistic fluency and confidence, but limited flexibility and self-correction. It can one-shot straightforward or pattern-based questions, yet it fails to generalize to novel problem settings without direct intervention.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-06T18:22:12.750338+11:00",
    "reply_count": 0,
    "view_count": 102,
    "user": {
      "id": 1294618,
      "name": "Xi Cheng",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7266065",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Mistral",
      "depth_bucket": "medium",
      "word_count": 182
    },
    "file_refs": [
      {
        "filename": "mistral_hw2_annotated_log.pdf",
        "position": 1329,
        "context": "6-83e1-2c0ac7a7384a\">https://chat.mistral.ai/chat/678d9106-0d96-45c6-83e1-2c0ac7a7384aAnnotated Log:"
      }
    ]
  },
  {
    "id": 7265693,
    "number": 243,
    "course_id": 84647,
    "title": "Special Participation A: Gemini on Homework 4",
    "document": "Annotated Transcript:\n\nhttps://drive.google.com/file/d/1ZOIMXval6EtWYyoBE6H13fS0I7d58Fmd/view?usp=sharing\n\nFor this special participation, I used Gemini Pro 2.5 to solve the written portions of homework 4. I began by giving Gemini the assignment and telling it that I was trying to evaluate how well it could solve the problems, and then went through the problems with it one by one. Overall, it did a good job of solving the problems. \n\nAll the mistakes it made, except one, were due to the fact that it read the problems in the homework wrong. Oftentimes, these misreadings were very blatant, like when it just misread the entries of the matrices for problem 3. It only made one reasoning mistake, that wasn’t due to it misreading the question, in part 2g, but was able to correct itself after being told what step in the derivation it made a mistake on. \n\nOne notable part of the interaction was when it solved problem 2e. Its answer differed from what was given in the solutions, even though it was correct as there seems to be a mistake in the solutions, so I tried prompting it to fix its answer, at which point it arrived at the same solution. Afterwards, I gave it what was given in the solutions, and asked it to try to arrive at the answer in the solutions. After trying to arrive at the given answer, it eventually gave up, and said that it didn’t know why the answer was correct. I thought that this was pretty impressive because it shows that Gemini is actually critiquing itself as it goes. Instead of hallucinating to match the answer I gave it, it pushed back and gave what it thought was correct. I thought this was really impressive, as it isn’t just being “agreeable” and taking what the prompter says to be the truth, like other LLMs I’ve used like ChatGPT.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-06T16:19:43.889302+11:00",
    "reply_count": 0,
    "view_count": 134,
    "user": {
      "id": 647625,
      "name": "Jason Guo",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7265693",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 309
    },
    "file_refs": null
  },
  {
    "id": 7263386,
    "number": 239,
    "course_id": 84647,
    "title": "Special Participation A: HW6, gpt-oss-120b",
    "document": "I used gpt-oss-120b with thinking on HW 6 non coding questions. The performance was surprisingly good for a open-source model from a company with flagship proprietary models. It was able to one-shot almost all the questions. This model was also surprisingly fast (although that depends on the hosting provider). The latency was near 0 seconds and token generation was very fast. A more detailed summary is provided in the pdf.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-06T10:05:54.778256+11:00",
    "reply_count": 0,
    "view_count": 128,
    "user": {
      "id": 617558,
      "name": "Alex Luu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7263386",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "GPT-Oss",
      "depth_bucket": "medium",
      "word_count": 70
    },
    "file_refs": [
      {
        "filename": "Special Participation A, HW6, gpt-oss-120b.pdf",
        "position": 426,
        "context": ""
      }
    ]
  },
  {
    "id": 7258633,
    "number": 230,
    "course_id": 84647,
    "title": "Special Participation A: Kimi on HW7 Written Questions",
    "document": "I had Moonshot AI's Kimi K2 model answer HW7's written questions. For each problem, my goal was to get the model to the correct answer while giving it as little outside assistance as possible. I only attempted to steer the model when it gave a clearly incorrect answer or when its response diverged significantly from the staff solution.\n\nHere are some of my key findings:\n\nThe model was able to answer most questions in one shot. Laying aside minor formatting and notational differences, the final answers were almost always the same as the staff solution's. \n\nHowever, in reasoning through the problems, the model sometimes made logical leaps that, while correct, were not sufficiently justified in my opinion.\n\nFor example, for Q3b, the model goes straight from stating the loss function to computing its gradient WRT $W_2$ in just one step.\n\n(For context in the below equation, $\\hat{X} = W_2 W_1 X$)\n\nMeanwhile, the staff solution is much more thorough, walking us through every step of the derivation and explicitly citing the matrix calculus identities that were employed.\n\nSo to fix that issue, when necessary, I ask the model to justify in more detail how it reached a given step.\n\nI saw some evidence of hallucination; for question 4a, I accidentally forgot to give the model some information needed to answer the question correctly. But rather than state, \"not enough information provided\", it gave me an incorrect response.\n\nFurthermore, when I gave it a URL containing the information needed to answer the question correctly, it again answered the question wrong. \n\nUpon further questioning, the model revealed that in fact, it does not have live browsing capabilities.\n\nThat is a reasonable limitation, but what is concerning is that it didn't admit it couldn't read the URL until after it had already hallucinated. This decreases my overall trust in the model.\n\nThat said, the model did a pretty good job on the \"summarize a blog post\" question (apart from the hallucination).\n\nI feel like these sorts of \"summarize a technical blog\" questions are where LLMs' abilities really shine. \n\nThey are able to pick out the key details and \"big picture\" insights without getting too lost in the technical details (a mistake that I often make when summarizing blog posts / papers)\n\nA full, annotated transcript of the conversation can be found here: https://drive.google.com/file/d/1p6f0AXpDIMW-9v7vZMJVj2J7aqjPkWQe/view?usp=sharing. \n\nI will also add a comment to this post with screenshots of the conversation to make it easy to read.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-05T14:34:52.365835+11:00",
    "reply_count": 1,
    "view_count": 119,
    "user": {
      "id": 921495,
      "name": "Vijay Kethanaboyina",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7258633",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "Kimi",
      "depth_bucket": "medium",
      "word_count": 410
    },
    "file_refs": null
  },
  {
    "id": 7250623,
    "number": 226,
    "course_id": 84647,
    "title": "Special Participation A: HW7 with Grok",
    "document": "I used Grok to complete the written part of HW7. I begin by clearly stating Grok’s role and the assistance I require, then provide whole HW7 file to Grok and ask it to understand the problem setups ONLY, so it does not skip ahead to solving the problem.\n\nHW7 consists of proofs, multiple choice questions, and open-ended free response problems. Grok performs very well on the multiple-choice questions, often answering correctly on the first attempt. For the open-ended free response problems, Grok provides reasonable points with clear explanations. However, on the proof problems, Grok typically does not match the official solution on the first attempt and requires additional hints and guidance, for example, specifying which variables or equations to use.\n\n\n\nOverall Grok performs descently on the written part of the homework, with great performance on MCQ problems and open-ended free response problems, but difficulty with the proofs.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-04T09:47:19.742032+11:00",
    "reply_count": 0,
    "view_count": 130,
    "user": {
      "id": 647575,
      "name": "Ender Ji",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7250623",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "Grok",
      "depth_bucket": "low",
      "word_count": 147
    },
    "file_refs": [
      {
        "filename": "grok_hw7.pdf",
        "position": 942,
        "context": ""
      }
    ]
  },
  {
    "id": 7250482,
    "number": 225,
    "course_id": 84647,
    "title": "Special Participation A: Mistral on HW6",
    "document": "Link: https://chat.mistral.ai/chat/6cd62931-f284-4c4b-9ba3-c5b97943fd28\n\nAnnotated Log: https://drive.google.com/file/d/1mUgllouQGxVA_m5tmlVrq6Dt46HSf0hd/view?usp=sharing\n\nExecutive Summary:\n\nI observed that the model fails to extract information from diagrams like tables, images, and graphs. Assignments that include filling out information or extracting information from diagrams to complete the assignment are very likely to be incorrect, as the model will hallucinate information.\n\nAlso, it didn't list the explanation for its reasoning behind its own analysis of the graphs in one of the problems. I suggest being careful with problems involving visualization.\n\nIt did well in demonstrating its mathematical analysis, making it clear to figure out any mishap in its derivation. There were times when the model outputted answers that I, myself, hadn't seen, though they were mathematically equivalent. The model considers other options when solving problems, allowing for a broader understanding of mathematics.\n\nMistral also did a good job of allowing for an open interaction with the user by making sure to end with, ‘Ready for any further questions or clarification!’",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-04T09:28:36.310964+11:00",
    "reply_count": 0,
    "view_count": 104,
    "user": {
      "id": 970764,
      "name": "Heidy Hernandez Juan",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7250482",
    "metrics": {
      "homework_id": "HW6",
      "model_name": "Mistral",
      "depth_bucket": "medium",
      "word_count": 159
    },
    "file_refs": null
  },
  {
    "id": 7250444,
    "number": 224,
    "course_id": 84647,
    "title": "Special Participation A: Gemini Pro on HW 3",
    "document": "What I did:\n\nI ran an interactive, non-coding walkthrough of HW 3 using Gemini Pro. Full trace (screenshots + chat excerpts) is in my doc: https://docs.google.com/document/d/1P6yTAFO4GR4W4a_l02kAFN2mYwKtmgsmPLecVg9gAGY/edit?tab=t.0\n\nI used Gemini to sanity-check my derivations, clarify notation, and cross-validate with my own solutions\n\nWorkflow:\n\n1) Try solo first (≤30 min). If blocked, paste the exact sub-question into Gemini.\n\n2) Compare & probe. Check Gemini’s output against my derivation; ask for clarifications or for the missing step.\n\n3) Tighten. Clarify equation statements, symbol mismatches, and ask for the key identity or rules explicitly.\n\nSpecific Example (Analogy + Clarification):\n\nTopic: Interpreting pθ​:X→Δ(X) in Q4.\n\nMy Ask: “What does the arrow to Δ(X) mean in pθ​:X→Δ(X)?”\n\nGemini’s Analogy (helpful): Sound equalizer.\n\npθ​ = the equalizer; θ = slider positions;\n\nX = the set of audible frequencies;\n\nΔ(X) = all possible sound profiles (distributions over X).\n Changing θ selects a particular distribution in Δ(X).\n\nMy Follow-up (to make it precise): “Please restate without analogy: define X, Δ(X), and what an ‘element’ of Δ(X) is.”\n\nGemini (clarified): X is the sample space; Δ(X) is the set of all probability measures on X; for each θ, pθ​(⋅)∈Δ(X).\n\nPattern used: take the first pass (analogy/intuition), then demand the formal mapping with symbols.\n\nObservations:\n\nUses analogies to simplify (e.g., equalizer for pθ​:X→Δ(X)). Helpful as a first pass.\n\nInconsistent symbols at times, likely from generic training patterns; needs nudging to match the homework question’s notation.\n\nPositives (What Worked Well):\n\nOne-shot on standard rewrites: Correctly gave correct answers straight from around 30% of the questions\n\nRight methodology even when imperfect: When wrong/incomplete, it still pointed in the correct direction\n\nNegatives (Limitations and Effort Required):\n\nNotation slippage: Misread or drifted symbols\n\nOver/under-explanation: Wrapped easy steps in prose but skimmed tough steps unless I asked “elaborate on this step.”\n\nPrompting Strategies That Helped:\n\n“Derive further.” Ask for the exact algebraic substitution (e.g., show ∇p=p∇logp).\n\n“Elaborate / Why this?” Request the chain-rule Jacobian or the gradient wrt input vs. parameters explicitly.\n\n“Is my thought correct? If not, why?” Good for catching subtle mistakes and forcing a counterexample or fix.\n\n“Show this equation more clearly” Keeps it concise and reduces misunderstanding\n\n“Match the HW notation.” Prevents variable chaos.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-04T09:23:05.224039+11:00",
    "reply_count": 2,
    "view_count": 145,
    "user": {
      "id": 906474,
      "name": "Ben Yu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7250444",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "Gemini Pro",
      "depth_bucket": "medium",
      "word_count": 362
    },
    "file_refs": null
  },
  {
    "id": 7246769,
    "number": 219,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT on HW7",
    "document": "I evaluated ChatGPT‑5's capabilities for HW7 non‑coding parts: 3(b), 4, 7, 8, using the hw7 questions and staff‑solutions as ground truth.\n\nBottom line. ChatGPT‑5’s answers are correct. \n\nPer‑problem highlights.\n\n3(b) PCA & linear autoencoders. Derived the first‑order conditions exactly as in the key—∇W2​​L=2(W2​W1​−I)XX⊤W1⊤​, ∇W1​​L=2W2⊤​(W2​W1​−I)XX⊤—and correctly verified that W2​=Uk​,W1​=Uk⊤​ satisfies them. Verdict: correct. \n\n4 “How to train your ResNet.” Reported 341 s to 94% baseline and 26 s to ≈94.1% final; reflections (b, c) are on‑point (data‑pipeline bottlenecks, batch size/LR scaling, BN precision, “catastrophic forgetting” vs curvature). Verdict: correct (with extra but accurate context). \n\n7 Machine translation. (a) Correctly explains why “vertical stacking” is flawed (breaks variable‑length handling and global conditioning). (b) Teacher forcing tokens enumerated explicitly: <SOS>, I, see, a, dog. (c) Evaluation: <SOS>, then model’s outputs (I, saw, a, dog). Verdict: correct. \n\n8 Self‑supervised linear AEs. (a) Exactly matches the objective: two Linear layers (encoder/decoder), MSELoss, SGD + weight decay; no Dropout/LN/BN. (b) Clear SVD argument that the λ‑regularized optimum favors orthonormal columns in W2​ (minimizing σ2+1/σ2 at σ=1). Verdict: correct. \n\nOverall assessment. The model’s responses align with the staff solutions in substance and notation, with small stylistic differences only.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-03T18:16:17.084933+11:00",
    "reply_count": 0,
    "view_count": 127,
    "user": {
      "id": 607238,
      "name": "Faiaz Khan",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7246769",
    "metrics": {
      "homework_id": "HW7",
      "model_name": "GPT-5",
      "depth_bucket": "low",
      "word_count": 191
    },
    "file_refs": [
      {
        "filename": "hw7 report.pdf",
        "position": 1433,
        "context": ""
      }
    ]
  },
  {
    "id": 7244375,
    "number": 217,
    "course_id": 84647,
    "title": "Special Participation A: Gemini 2.5 Flash on HW2",
    "document": "I interactively engaged Gemini 2.5 Flash on the non-coding parts of Homework 2. Overall, the model was able to arrive at the correct answers in most cases, though several notable issues were observed.\n\nStrategies: I first clarified the main role that Gemini was expected to perform and illustrated the evaluation rubrics for its answers. Then, I do the following steps:\n\nStep 1: Provide Gemini with one homework problem.\n\nStep 2: Collect its initial (one-shot) answer.\n\nStep 3: If the answer was incorrect, provide hints to guide Gemini to fix the original answer, and repeat until the response was correct or it seems to have no chance of fixing it.\n\nStep 4: Repeat Steps 1–3 for all homework problems.\n\nNotable observations and flaws. Although Gemini was able to correctly solve most of the questions in the first answer, it occasionally made logical or mathematical errors that led to incorrect final answers, even when the reasoning process is flawless. In some cases, the model relied on specific examples provided in the prompt, which resulted in partially correct answers. Additionally, Gemini sometimes produced wrong-formed LaTeX code, making its output less readable. This issue may have been partly influenced by the formatting of the input prompts.\n\nHere's the full interaction log with my notations and comments, highlighting notable phenomenons happens in Gemini's answer:",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-11-03T10:24:52.673823+11:00",
    "reply_count": 0,
    "view_count": 111,
    "user": {
      "id": 1762083,
      "name": "Ruizhe Song",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7244375",
    "metrics": {
      "homework_id": "HW2",
      "model_name": "Gemini Flash",
      "depth_bucket": "medium",
      "word_count": 219
    },
    "file_refs": [
      {
        "filename": "Special_Participation_A.pdf",
        "position": 1386,
        "context": "ion log with my notations and comments, highlighting notable phenomenons happens in Gemini's answer:"
      }
    ]
  },
  {
    "id": 7243310,
    "number": 216,
    "course_id": 84647,
    "title": "Special Participation A: HW5 With the Help of Claude AI",
    "document": "Hi, I just made a script documenting how I guided Claude AI to walk through homework 5. I have to acknowledge that Claude AI is a very powerful tool that can help us walk through the homework. I rarely see any hallucinations or misconceptions. Most of the arguments it made are consistent with the right solution especially when the questions are fairly straightforward. However, when it is asked to derive something that requires many intermediate steps, it will sometimes fail to recognize the most obvious thing to do at some point. That is to say, for long-horizon tasks, it tends to make its solutions more complex even though such complex reasoning is not ideal.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-11-03T07:26:53.861759+11:00",
    "reply_count": 0,
    "view_count": 101,
    "user": {
      "id": 970785,
      "name": "Yuxiang Liu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7243310",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "Claude",
      "depth_bucket": "medium",
      "word_count": 114
    },
    "file_refs": [
      {
        "filename": "HW5_walkthrough.pdf",
        "position": 668,
        "context": " tasks, it tends to make its solutions more complex even though such complex reasoning is not ideal."
      }
    ]
  },
  {
    "id": 7227387,
    "number": 199,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek Chat on HW3",
    "document": "Conclusion:\n\nThe evaluation of DeepSeek's capabilities for homework 3 has demonstrated :\n\nStrong Mathematical Problem-Solving: DeepSeek reliably handles the mathematical problems, including linear algebra & calculus & probability , providing both solutions and clear, step-by-step explanations. Both the calculation and proof problems were well done. It can retrieve and understand the formulas from papers and analyze them well.\n\nEffective Information Retrieval and Synthesis: A key strength is its ability to process academic papers, accurately identify core arguments and results, and summarize them concisely, demonstrating strong comprehension and distillation skills.\n\nDeepSeek's integration of these capabilities makes it a remarkably efficient for navigating the challenges of advanced math & algorithm & deep learning studies.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-10-30T16:12:13.694906+11:00",
    "reply_count": 0,
    "view_count": 121,
    "user": {
      "id": 1763610,
      "name": "Zhuangzhe Wu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7227387",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "DeepSeek",
      "depth_bucket": "low",
      "word_count": 112
    },
    "file_refs": [
      {
        "filename": "deepseek_hw3_log.pdf",
        "position": 835,
        "context": ""
      }
    ]
  },
  {
    "id": 7219478,
    "number": 190,
    "course_id": 84647,
    "title": "Special Participation A: ChatGPT on HW1",
    "document": "I used ChatGPT on HW1 no-cording parts(Special Participation A).\nI posted the results, my findings about them, and my summary on the attached pdf.\n\nThis is summary from the pdf.\nOverall, ChatGPT achieved fully correct answers for all problems, demonstrating strong mathematical reasoning and consistency across sequential tasks. However, the interaction also revealed distinctive behavioral patterns in how the model approached problem solving.\n\nAccuracy and One-Shot Performance\n\nFor straightforward conceptual or definitional questions (e.g., explaining optimizer structures or interpreting results), ChatGPT often produced the correct answer immediately (“one-shot”).\n\nFor more complex derivations or matrix manipulations, it sometimes made conceptual simplification errors rather than arithmetic ones. Nevertheless, this resulted in only one error out of seven big problems.\n\nExample of Conceptual Oversimplification\n\nIn one problem, the model incorrectly dropped the matrix when simplifying to .\n\nThis error propagated through subsequent parts, showing that ChatGPT solved sequentially, building upon its previous reasoning.\n\nOnce provided with a hint, it immediately corrected the mistake—illustrating that the model is particularly strong at conditional reasoning with scaffolding.\n\nStyle of Reasoning\n\nChatGPT consistently produced mathematically formal and symbol-heavy responses, sometimes more rigorous than the problem required.\n\nEspecially for “explanation” questions, it tended to over-mathematize rather than summarize intuitively. This pattern was observed across multiple sub-questions.\n\nResponse Latency and Thoughtfulness\n\nThe model typically took 2–5 minutes for moderately complex derivations, suggesting that its “Thinking” mode encouraged stepwise symbolic reasoning rather than rapid generation.\n\nNotation and Presentation\n\nAll responses were logically consistent with the official solutions, though small notational differences (e.g., transposition order or matrix symbols) occasionally appeared.\n\nWhen a matrix identity was mentioned but not proven in the problem statement, ChatGPT sometimes derived it explicitly, reflecting uncertainty about whether it could assume the result.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-10-29T11:02:12.9722+11:00",
    "reply_count": 0,
    "view_count": 104,
    "user": {
      "id": 1308098,
      "name": "Junya Tsuneishi",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7219478",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "GPT-4o",
      "depth_bucket": "medium",
      "word_count": 286
    },
    "file_refs": [
      {
        "filename": "Special Participation A ChatGPT on HW1.pdf",
        "position": 2206,
        "context": ""
      }
    ]
  },
  {
    "id": 7212198,
    "number": 184,
    "course_id": 84647,
    "title": "Special Participation A HW5: ChatGPT 5",
    "document": "Link to the discussion: https://chatgpt.com/share/68ffefde-4c64-800f-9d99-5417df7eb2b8\n\nAcross this project, GPT accuracy was strongest when my prompts were concrete about goals, constraints, and style, and I consistently drove that clarity. I asked questions in a structured way—first “understand → summarize → derive → implement,” then “complete the code” with strict guardrails (no new helpers, inverted dropout, 0.5 L2 factor, BN modes, minimal diffs, NumPy-only where required). GPT answered by laying out the full math (OLS, BN, dropout expected-risk, conv/transpose-conv) and then mirroring it in code that matched the exected methods in different files. When GPT drifted—e.g., offering high-level guidance instead of finished cells/files, or being imprecise about BN mode keys, I nudged the GPT (“complete the notebook/file,” “set bn_param['mode'],” “use inverted dropout and scale by 1/(1-p),” “don’t regularize gamma/beta”), which corrected the approach and aligned outputs the expected result. That feedback loop precise nudges on the few inaccuracies and my immediate revisions in different branches of the same chat , tightened both the math explanations and the implementations until they matched my expected results and coding style.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-10-28T09:29:12.904783+11:00",
    "reply_count": 1,
    "view_count": 177,
    "user": {
      "id": 228039,
      "name": "Mehul Jaiswal",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7212198",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "GPT-5",
      "depth_bucket": "medium",
      "word_count": 175
    },
    "file_refs": null
  },
  {
    "id": 7212131,
    "number": 183,
    "course_id": 84647,
    "title": "Special Participation A: Mistral AI's Le Chat on HW3",
    "document": "Here is the online link: https://chat.mistral.ai/chat/8c72d241-dd44-41a0-b8fc-a0469d84ff1d\n\nHere is the annotated log:\n\nExecutive Summary:\n\nFrom my observation, Le Chat was able to answer most written questions correctly on one shot. However, for questions that reference an external research paper, it could misunderstand the problem statement and draw something tangent to what the question is asking. In particular, it could reference a table on a different page or a formula in a different section. Doing some prompt engineering helps the model to reference the correct table/figure.\n\nIn addition, for questions that involve numerical counting, it could mistake the computation by a small margin, even after engineering the prompt. For instance, it could count something twice and mess up with the calculation.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-28T09:17:55.632059+11:00",
    "reply_count": 0,
    "view_count": 149,
    "user": {
      "id": 975971,
      "name": "Jeffrey Cheng",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7212131",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "Mistral",
      "depth_bucket": "low",
      "word_count": 119
    },
    "file_refs": [
      {
        "filename": "Annotated log of conversation with Mistral AI.pdf",
        "position": 814,
        "context": "d84ff1d\">https://chat.mistral.ai/chat/8c72d241-dd44-41a0-b8fc-a0469d84ff1dHere is the annotated log:"
      }
    ]
  },
  {
    "id": 7202422,
    "number": 182,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek on HW5",
    "document": "For the HW5, I try to use Deepseek to solve the problem sets and all the questions and answers are documented in the above files. The strategies I am using are simple, just putting the transcription of the problem to Deepseek and see how it can solve the problems. There are some traits of the Deepseek AI:\n\n1. Deepseek does not support Multi-modal input, therefore when the problems rely on image input, then the model cannot really give a proper answer.\n2. Ambiguous question prompts will lead to longer thinking time.\n3. Deepseek is better at explanation than real calculation. My recommendations for using Deepseek as a learning tool to make it help with conceptual understanding.\n\nThis is observed from some of the facts:\n\n1. Of the 11 questions, Deepseek gets 9 of them correct first shot, while one of the question relies on vision input and the other is a calculation problem. It gets the calculation problem correct after another prompt of asking itself to self examine the calculation process. \n2. I try to ask Deepseek examines its own mistake after I finish all the problems and it still can directly and precisely locate where might go wrong, which demonstrate its capable long context understanding capability.\n3. For problem 4, Deepseek not only provides very clear demonstration on how to derive the answer correctly and can show the direct relation between batchnorm and dropout by directly deriving the scaling factor.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-26T16:13:16.614618+11:00",
    "reply_count": 0,
    "view_count": 140,
    "user": {
      "id": 1751442,
      "name": "Jerry Xiao",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7202422",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 241
    },
    "file_refs": [
      {
        "filename": "report.pdf",
        "position": 1436,
        "context": ""
      }
    ]
  },
  {
    "id": 7162279,
    "number": 166,
    "course_id": 84647,
    "title": "Special Participation A: Grok on HW0",
    "document": "For the special participation A on HW0, I use Grok to address the non-coding analytical components (problems 2–5). The performance of Grok really impressed me, almost all questions are one-shot correct except for 5(b)(iii).\n\nSummary: Grok demonstrated reliable analytical reasoning for most problems, with clear and accurate derivations for problems 2, 3, and 4. The answer of them are all fully correct. However, the error on problem 5(b)(iii) suggests its sensitivity to complex or ambiguous problem structures. \n\nHere is the link https://grok.com/share/c2hhcmQtMg%3D%3D_935edffc-4c41-4eb5-931a-aaf30ccdf737",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-20T18:33:07.202349+11:00",
    "reply_count": 0,
    "view_count": 233,
    "user": {
      "id": 1753326,
      "name": "Tianyu Gu",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7162279",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Grok",
      "depth_bucket": "medium",
      "word_count": 82
    },
    "file_refs": [
      {
        "filename": "Special participation A.pdf",
        "position": 609,
        "context": ""
      }
    ]
  },
  {
    "id": 7151370,
    "number": 163,
    "course_id": 84647,
    "title": "Special Participation A: GPT-Oss on HW5",
    "document": "For this special participation, I used gpt-oss-120b (Reasoning = High) to solve all non-coding analytical parts of HW5 (Q1–Q4).\nThe model was tested on symbolic derivations and conceptual reasoning without code execution.\n\nAccuracy: 9 / 11 one-shot (82 %), 2 / 11 minor-nudge (18 %)\n\nMain errors: one ASCII matrix mis-parse and one BatchNorm vs LayerNorm confusion\n\nNo hallucinations: all final answers matched the official HW5 solution key\n\nTakeaway: once prompts were clearly formatted in LaTeX, the model produced correct and interpretable derivations, showing reliable analytical reasoning but high sensitivity to input structure.\n\nBit slow(90-180 seconds)\n\n(Logs and annotations are attached in the full PDF report.)",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-10-18T11:31:49.638758+11:00",
    "reply_count": 0,
    "view_count": 248,
    "user": {
      "id": 1770934,
      "name": "Noah Lund Syrdal",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7151370",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "GPT-Oss",
      "depth_bucket": "medium",
      "word_count": 106
    },
    "file_refs": [
      {
        "filename": "SpecialParticipationA-GPTOssHW5.pdf",
        "position": 721,
        "context": ""
      }
    ]
  },
  {
    "id": 7148413,
    "number": 162,
    "course_id": 84647,
    "title": "Special Participation A: Grok on HW5",
    "document": "This document is my report for the HW5 - written part. I evaluate the performance of Grok. It is tested across a series of theoretical deep learning questions covering topics such as convolutional networks, batch normalization, dropout, depthwise separable convolutions, and regularization. The report includes:\n\nAn overview of Grok’s performance, where I categorized each task into one of four levels: One-Shot Correct, Minor Misconception, Larger Error, or Did Not Solve.\n\nA reflection on Grok’s strengths and weaknesses, where I highlight its strong conceptual intuition, clarity, and adaptability, while noting occasional over-explanation and moments where certain steps could have been emphasized more clearly.\n\nMy recommendations for effectively using Grok as a learning tool, emphasizing the importance of clear prompting, iterative clarification, and maintaining focus on conceptual understanding.\n\nFull transcripts of my Q&A exchanges with Grok, including its solutions, reasoning, and my reflections after each subproblem. \n\nOverall, I conclude that Grok performs really well as a conceptual teaching and problem-solving assistant for theoretical deep learning. It has only minor inefficiencies related to verbosity or explicitness.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-18T02:48:56.469848+11:00",
    "reply_count": 0,
    "view_count": 199,
    "user": {
      "id": 1765812,
      "name": "Anders Vestrum",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7148413",
    "metrics": {
      "homework_id": "HW5",
      "model_name": "Grok",
      "depth_bucket": "medium",
      "word_count": 173
    },
    "file_refs": [
      {
        "filename": "special_participation_A.pdf",
        "position": 1242,
        "context": "or theoretical deep learning. It has only minor inefficiencies related to verbosity or explicitness."
      }
    ]
  },
  {
    "id": 7132324,
    "number": 155,
    "course_id": 84647,
    "title": "Special Participation A: HW 4 using DeepSeek",
    "document": "Problem Context\n\nThere are 5 non-coding questions in this homework. Two questions on optimization (specifically Newton-Schultz iteration and MuP scaling) and three questions on CNNs. I took a special interest in how DeepSeek performs when solving these computationally heavy problems when prompted in different formats: purely text, purely images, a PDF and a hybrid of text and images. I also tested DeepSeek's ability to understand and retain multi-part instructions in between these different modalities. The idea was to evaluate DeepSeek's overall problem-solving performance while also attempting to understand which prompt type yielded the best results. \n\nIn this experiment:\n\nDeepSeek solved four out of five questions correctly in a single attempt.\n\nQuestion 2 was the only one it got wrong, though it could solve it with further prompting.\n\nQuestion 3 required a brief nudge from me to complete the final computation, likely because the context limit was reached mid-session.\n\nExecutive Summary\n\nFrom testing out different formats of the prompts, I observed the following:\n\nWhen only text was provided (Question 2), DeepSeek tended to lose context more easily. it sometimes failed to connect earlier parts of the questions or instructions.\n\nWith images or PDFs, it demonstrated significant better context retention and reasoning continuity. It appeared to process the questions more holistically, possibly due to the visual cues.\n\nA hybrid approach (text +images) also performed better. It was able to mix information from images and text efficiently to provide the correct solution (see Question 4).\n\nIn essence, DeepSeek performs impressively well on conceptual and numerical reasoning when the sufficient structured context is provided. Its performance, however, degraded slightly in purely textual prompts, indicating that context formatting plays a role in achieving accurate results.\n\nNote: This analysis is based on a limited sample of five questions, and is therefore not exhaustive or conclusive.\n\nHere is a link to the conversation I had with DeepSeek: https://chat.deepseek.com/share/jsuxr35m8bhnxyzz7b\n\nHere is the file containing the conversation, with question-wise comments (annotated in the PDF):",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-15T10:18:16.802913+11:00",
    "reply_count": 0,
    "view_count": 223,
    "user": {
      "id": 1691230,
      "name": "Srikar Babu Gadipudi",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7132324",
    "metrics": {
      "homework_id": "HW4",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 325
    },
    "file_refs": [
      {
        "filename": "participationA_HW4_DeepSeek.pdf",
        "position": 2221,
        "context": "z7bHere is the file containing the conversation, with question-wise comments (annotated in the PDF):"
      }
    ]
  },
  {
    "id": 7111658,
    "number": 141,
    "course_id": 84647,
    "title": "Special Participation A - HW 3 ChatGPT 5",
    "document": "Executive Summary\n\nI used ChatGPT 5 to interactively engage with HW 3's problems and get it to get to the correct answer. While this wasn't the first time I've interacted with ChatGPT 5 for Deep Learning material, I was still impressed with its performance. The majority of questions were one-shotted correctly.\n\n1: Maximal Update Parameterization: \n\nSuccessfully one shotted, reasoning good.\n\n3: Maximal Update Parameterization Research:\n\nWhen interpreting figure 1, ChatGPT 5's explanation of the visual figure was actually more informative and explanatory than the official solutions were. It critically highlighted that \"each layer’s updates remain of the same order regardless of width\"\n\nRather than using the RMS-to-RMS matrix norm that we covered in lecture (scaled version of spectral norm), it hallucinated the definition of the RMS matrix norm. It defined it as if the matrix was unrolled into a vector and then the RMS norm of that flattened vector was taken, \n\n$$\\frac{\\left\\lVert W\\right\\rVert_F}{\\sqrt{n_{l-1}n_l}}$$\n\nAs a result, it used this relation (that it claimed applies to random matrices)\n\n\n$$∥W_l∥_2\\approx\\frac{∥W_l∥_F}{\\sqrt{\\left\\{\\min(n_l,n_{l-1})\\right\\}}}$$\n\nto derive the incorrect order for the weight matrix and update's RMS norms.\n\nWhile it made this error, ChatGPT 5 was still able to correctly derive the upper bounds in Desideratum 1 (because it didn't use the RMS norms of hidden layer vector/update or weight matrix/update for its derivation). This is not that surprising given that I fed in the \"A Spectral Condition for Feature Learning\" pdf at the beginning of the chat and ChatGPT produced a very similar derivation. Additionally, it was still able to correctly identified the key assumption that allowed for the derivation of the lower bound.\n\nAdditionally, after providing the definition of the induced norm, it was able to correctly derive the  order for the weight matrix and update's RMS norms.\n\n4: Policy Gradient and the Reparameterization Gradient Estimator:\n\n Overall, ChatGPT5 seemed to provide comprehensive reasoning (and more additional information/context) and got to the correct solutions. \n\n5: Tensor Rematerialization:\nChatGPT 5 at first guessed 2 loadmems instead of 10 (thinking once a layer was loaded, it would stay in memory for the next layer). After I reiterated the problem statement, it was easily able to answer and explain the total number of loadmems as 10.\n\nOther than that, it was able to provide sound reasoning and the correct answers for the rest of the problem\n\nOverall:\nChatGPT seemed to provide comprehensive reasoning for every problem (sometimes an unnecessary amount), and seemed to be able to course-correct with a little bit of feedback. It seemed to falter more on topics or ideas that had less public material available online (e.g. RMS-RMS matrix norm)",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-10-11T14:07:05.580704+11:00",
    "reply_count": 0,
    "view_count": 194,
    "user": {
      "id": 1288128,
      "name": "Iana Lin",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7111658",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "GPT-5",
      "depth_bucket": "medium",
      "word_count": 434
    },
    "file_refs": [
      {
        "filename": "ParticipationA-HW3.pdf",
        "position": 2840,
        "context": "er more on topics or ideas that had less public material available online (e.g. RMS-RMS matrix norm)"
      }
    ]
  },
  {
    "id": 7095749,
    "number": 134,
    "course_id": 84647,
    "title": "Special Participation A: - Deepseek on HW1",
    "document": "Intro:\nI'm using Deepseek to solve HW1, including 7 problems.\n\nMy prompts:\n\n1. Please help me deal with these problems about deep neural networks. Think it step by step.\n\n<answer> (Given the solution of Problem 1)\n\n2. You have done a good job! What about the remaining problems? Think it step by step.\n\n<answer> (Given the solution of Problem 2)\n\n3.  Yes, proceed with Problem 3 about momentum.\n\n<answer>(Given the solution of Problem 3)\n\n4. Plz, continue.\n<answer>(Given the solution of Problem 4-7)\n\nMy observation:\n\n1. When the thinking and answer content was too long, DeepSeek only answered a part of the problems, probably due to the chunking technique and LLM's inclination to output shorter contents. So that's why there're 4 prompts for HW1.\n\n2. The accuracy is very high, especially for the computational tasks. However, If format of the question is not ordinary, it will be ignored sometimes, such as 4(a).\n\nI don't know whether it can be called \"hallucination\", but actually in the thinking content, it indeed notices this task but considers 4(a) internally without giving the solution: \n\n3. A serious problem occurs that at times, Deepseek will unnoticeably make an assumption, even if there is no information about it. This problem fails the COT(Chain of Thought) if the assumption is not expected, leading to the wrong answer. More seriously, if the users don't look through the process of thinking, they may heartily accept the answer, but the conflict is that the thinking content is too long to read.\n\n4. Deepseek fails to perfectly answer ambiguous questions, such as 2(c).  It seems that it tends to give a conceptual, verbal description, if possible. It can be seen as a kind of laziness from my perspective.\n\nLet's first compare the standard answer and Deepseek's answer:\n\nStandard： \n\nDeepseek:\n\nAlthough the two solutions points to the same conclusion, Deepseek's answer is quite short. I guessed that Deepseek mixed up the identity of reader. Therefore, I asked it to rewrite the solution with different identity prompting:\n\n--------------\n\nPrompt1:\n\nYou are a student doing the assignment. Please rewrite Question 2(c).\n\nPrompt2:\n\nThe professor will read your solution! Please rewrite Question 2(c).\n\n--------------\n\nBut the result shows that Deepseek still gives conceptual answers, only adding more description. Only when I mentioned 'calculation' externally, It does work. \n\n\n\nRecommendation:\n\nFor Observation 1: \n\nIf there's no question number, the chunking will be unpredictable. Therefore , I recommend managing long problems with explicit chunking by yourself.\n\nFor Observation 2:\n\nI don't know how to solve this problem. I think reducing the use of table may help.\n\nFor Observation 3:\n\nRequire DeepSeek to state assumptions explicitly. But sometimes, the reasoning chain will be too long so that perhaps looking through it by yourself is also another good choice. \n\nFor Observation 4:\n\nHandle ambiguity with targeted instruction.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-09T03:33:14.844297+11:00",
    "reply_count": 2,
    "view_count": 218,
    "user": {
      "id": 1751485,
      "name": "Tianhao Qian",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7095749",
    "metrics": {
      "homework_id": "HW1",
      "model_name": "DeepSeek",
      "depth_bucket": "high",
      "word_count": 467
    },
    "file_refs": [
      {
        "filename": "Deepseek4HW1.pdf",
        "position": 2962,
        "context": ""
      }
    ]
  },
  {
    "id": 7083805,
    "number": 124,
    "course_id": 84647,
    "title": "Special Participation A: Qwen3-Max on HW0",
    "document": "Here is the online link: https://chat.qwen.ai/s/6240e96b-585c-4943-870d-3af47859ec5f?fev=0.0.222 Here is annotated log: https://drive.google.com/file/d/1vVCY_yCtDPNHagaoSDGR6BSnjG78wVpI/view?usp=sharing Executive Summary: Qwen3-Max is generally able to one-shot nearly all answers and does not have many misconceptions/hallucinations. The main issue is that it often skips steps, which is especially problematic for problems where you ask the model to show that A = B rather than just solve a problem, since the key is in the detailed steps. This was especially the case for “4. The 5 Interpretations of Ridge Regression” where you want to show how the concepts tie together. Prompting it repeatedly to show more detailed steps helped.\n\nIt’s also important to be careful about pasting in from the PDF as the copy paste can lead the model to parse text incorrectly, where I needed to correct the model to consider “||w||^2 not ||w||_2”.\n\nFinally, it struggled greatly with “5. ReLU Elbow Update under SGD”. In particular, it attempted to draw and label using ascii art, but the plots were messed up and even after repeated prompting, they are still not presentable.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-07T08:03:27.722243+11:00",
    "reply_count": 0,
    "view_count": 145,
    "user": {
      "id": 1683657,
      "name": "Andy Zhang",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7083805",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "Qwen",
      "depth_bucket": "medium",
      "word_count": 174
    },
    "file_refs": null
  },
  {
    "id": 7074543,
    "number": 116,
    "course_id": 84647,
    "title": "Special Participation A: Deepseek with Deep Thinking on HW0",
    "document": "Here is the online link: https://chat.deepseek.com/share/hcxrv1b7tn9s8c3lo0\n\nHere is my annotated version of the log: https://drive.google.com/file/d/18ZU3GgmdtP_u84GnVxof8Kh2yrbpOotN/view?usp=sharing\n\nExecutive Summary:\nFrom my observations, at least for DeepSeek, the model tends to capture most of the details of the problem-solving process within its internal reasoning. However, even after some targeted prompt engineering, it often fails to provide a detailed explanation of its reasoning and solution in the actual response to the user. At times, it even skips crucial steps in deriving mathematical expressions. For example, when asked for the minimum norm solution of ridge regression, it outputs the correct expression but provides little explanation of how it was derived.\n\nMost of the time, the model answers in a single step rather than engaging in a back-and-forth process to verify whether its solution is fully correct. This one-shot approach can be acceptable for straightforward problems, but it does cause errors in more complex cases. This issue is visible later in the annotated log, where the model fails to consider multiple possible cases for a single question.\n\nOn the positive side, I did not notice any hallucinations so far.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "post",
    "created_at": "2025-10-05T19:56:43.327041+11:00",
    "reply_count": 1,
    "view_count": 217,
    "user": {
      "id": 647731,
      "name": "Wesley Kai Zheng",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7074543",
    "metrics": {
      "homework_id": "HW0",
      "model_name": "DeepSeek",
      "depth_bucket": "medium",
      "word_count": 183
    },
    "file_refs": null
  },
  {
    "id": 7049136,
    "number": 106,
    "course_id": 84647,
    "title": "Special Participation A - Grok HW3",
    "document": "REFLECTION\n\n​​In completing the non-coding parts of the homework with Grok, I found that it could one-shot questions about 70–80% of the time. Hints proved extremely useful, as providing a small nudge almost always led to a correct answer, while leaving them out sometimes still allowed Grok to succeed, such as on questions 4C and 4D. This showed that while hints can significantly improve accuracy, the model is capable of reasoning independently in some cases. I also noticed that the model tended to give very long and verbose responses, often providing explanations two to three pages long, even when a short answer would have sufficed. This happened especially when I asked follow-up questions, and Grok would completely lose focus.\n\nOne particularly striking behavior was how Grok interacted over the course of the chat. When I gave feedback—acknowledging correct answers or asking for more details—it seemed to “learn” from the previous interaction, adjusting its responses accordingly. However, this sometimes caused it to overextend or go beyond the original scope of a question. For instance, in question five, Grok spent over seven minutes thinking about an answer before I had even formally asked the question, showing both its ambition to solve problems and its tendency to act preemptively. Similarly, when I asked multiple follow-up questions, it occasionally lost focus on the original prompt, producing responses that were less relevant or harder to parse.\n\nOverall, my experience with Grok highlighted both its strengths and its quirks. It consistently produced correct answers for most questions, and strategic prompting and providing feedback helped guide it effectively. At the same time, its verbosity, occasional hallucinations, and overzealous problem-solving emphasized the need for careful interaction and moderation. Despite these limitations, Grok was a highly capable tool for this homework, and the process offered valuable insights into how modern LLMs balance reasoning, responsiveness, and conciseness in an interactive problem-solving setting.",
    "category": "Curiosity",
    "subcategory": "",
    "type": "question",
    "created_at": "2025-10-01T09:44:26.995833+10:00",
    "reply_count": 1,
    "view_count": 166,
    "user": {
      "id": 647432,
      "name": "Bruno Vieira",
      "course_role": "student"
    },
    "ed_url": "https://edstem.org/us/courses/84647/discussion/7049136",
    "metrics": {
      "homework_id": "HW3",
      "model_name": "Grok",
      "depth_bucket": "medium",
      "word_count": 312
    },
    "file_refs": [
      {
        "filename": "CS_182_Participation_A_HW3_Grok.pdf",
        "position": 2078,
        "context": ""
      }
    ]
  }
]