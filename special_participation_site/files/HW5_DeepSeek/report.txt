--- Page 1 ---
EECS 182 Homework 5 Report
Question 1
Hi Deepseek I am working on a problem on Deep Learning with regards to CNN practice, will
you be able to help me with that?
Answer
Of course! I'd be happy to help you with your Convolutional Neural Network (CNN) problem.
Please go ahead and share the specific problem you're working on. You can:
â€¢ Paste the problem statement
â€¢ Show me your code and describe where you're stuck
â€¢ Ask a conceptual question about how CNNs work
â€¢ Describe the error message you're receiving
I can assist with a wide range of CNN topics, including:
â€¢ Architecture Design: Choosing layers, filter sizes, pooling, etc.
â€¢ Implementation: Coding in frameworks like TensorFlow/Keras or PyTorch.
â€¢ Debugging: Fixing errors, dealing with overfitting/underfitting, vanishing gradients.
â€¢ Data Preparation: Image preprocessing, augmentation, data loaders.
â€¢ Theory: How convolutions, pooling, and backpropagation work in CNNs.
â€¢ Model Interpretation: Understanding what your model has learned.
Just paste your problem or code below, and we'll start working on it together.
Looking forward to helping you deepen your deep learning knowledge!
Question 2
We are now having the first problem, throughout this problem, we will use the convention of
NOT flipping the filter before dragging it over the signal. This is the standard notation with


--- Page 2 ---
neural networks (ie, we assume the filter given to us is already flipped)
1. List two reasons we typically prefer convolutional layers instead of fully connected layers
when working with image data.
2. Consider the following 1D signal: [1,4,0,âˆ’2,3]. After convolution with a length-3 filter, no
padding, stride=1, we get the following sequence: [âˆ’2,2,11]. What was the filter?
3. Transpose convolution is an operation to help us upsample a signal (increase the
resolution). For example, if our original signal were [a,b,c] and we perform transpose
convolution with pad=0 and stride=2, with the filter [x,y,z], the output would be
[ax,ay,az+bx,by,bz+cx,cy,cz]. Notice that the entries of the input are multiplied by each of
the entries of the filter. Overlaps are summed. Also notice how for a fixed filter size and
stride, the dimensions of the input and output are swapped compared to standard
convolution. (For example, if we did standard convolution on a length-7 sequence with
filter size of 3 and stride=2, we would output a length-3 sequence). If our 2D input is [âˆ’1
2; 3 1] and the 2D filter is [+1âˆ’1; 0 +1]. What is the output of transpose convolution with
pad=0 and stride=1?
Answer
Of course! Let's work through each part of this problem step by step.
1. Two reasons we typically prefer convolutional
layers over fully connected layers for image data:
Reason 1: Parameter Efficiency & Spatial Locality
Convolutional layers use weight sharing - the same filter is applied across all spatial positions
of the input. This dramatically reduces the number of parameters compared to fully connected
layers where each input pixel would connect to each neuron. For example, a 1000Ã—1000
image would require 1 billion parameters in a single fully connected layer, while a
convolutional layer might use only thousands.
Reason 2: Translation Equivariance
Convolutional layers naturally capture the property that features (like edges, corners, textures)
are important regardless of their position in the image. A filter that detects vertical edges will
detect them anywhere in the image, making CNNs inherently translation-equivariant.


--- Page 3 ---
2. Finding the filter:
Given:
â€¢ Input signal: [1, 4, 0, -2, 3]
â€¢ Output after convolution: [-2, 2, 11]
â€¢ No padding, stride=1, filter length=3
Let the filter be [wâ‚, wâ‚‚, wâ‚ƒ]. The convolution operation (without flipping) gives:
Position 1: 1Â·wâ‚ + 4Â·wâ‚‚ + 0Â·wâ‚ƒ = -2
Position 2: 4Â·wâ‚ + 0Â·wâ‚‚ + (-2)Â·wâ‚ƒ = 2
Position 3: 0Â·wâ‚ + (-2)Â·wâ‚‚ + 3Â·wâ‚ƒ = 11
Solving this system:
From equation 1: wâ‚ + 4wâ‚‚ = -2
From equation 2: 4wâ‚ - 2wâ‚ƒ = 2
From equation 3: -2wâ‚‚ + 3wâ‚ƒ = 11
Solving: wâ‚ = 2, wâ‚‚ = -1, wâ‚ƒ = 3
Filter: [2, -1, 3]
3. Transpose convolution:
Given:
â€¢ Input:  [-1 2; 3 1] (2Ã—2 matrix)
â€¢ Filter:  [+1 -1; 0 +1] (2Ã—2 matrix)
â€¢ Pad=0, Stride=1
For transpose convolution with stride=1 and no padding on a 2Ã—2 input with 2Ã—2 filter, the
output size is:
Output size = (Input size - 1) Ã— stride + filter size = (2-1)Ã—1 + 2 = 3
So we need a 3Ã—3 output.
Let's compute step by step. Each input element gets multiplied by the entire filter and placed


--- Page 4 ---
in the output with overlaps summed:
From input element (-1) at position (0,0):
[-1Ã—1  -1Ã—(-1)   0 ]   [ -1   1   0 ]
[-1Ã—0  -1Ã—1     0 ] = [  0  -1   0 ]
[  0     0      0 ]   [  0   0   0 ]
From input element (2) at position (0,1):
[  0   2Ã—1    2Ã—(-1)]   [  0   2  -2 ]
[  0   2Ã—0    2Ã—1   ] = [  0   0   2 ]
[  0    0      0    ]   [  0   0   0 ]
From input element (3) at position (1,0):
[ 3Ã—0   3Ã—1    0 ]   [  0   3   0 ]
[ 3Ã—0   3Ã—(-1) 0 ] = [  0  -3   0 ]
[  0     0     0 ]   [  0   0   0 ]
As we can see here, the inference is incorrect, thus leading to false result in the end.
From input element (1) at position (1,1):
[  0     0      0    ]   [  0   0   0 ]
[  0   1Ã—1    1Ã—(-1)] = [  0   1  -1 ]
[  0   1Ã—0    1Ã—1   ]   [  0   0   1 ]
Now sum all contributions:


--- Page 5 ---
Position (0,0): -1 + 0 + 0 + 0 = -1
Position (0,1): 1 + 2 + 3 + 0 = 6
Position (0,2): 0 + (-2) + 0 + 0 = -2
Position (1,0): 0 + 0 + 0 + 0 = 0
Position (1,1): -1 + 0 + (-3) + 1 = -3
Position (1,2): 0 + 2 + 0 + (-1) = 1
Position (2,0): 0 + 0 + 0 + 0 = 0
Position (2,1): 0 + 0 + 0 + 0 = 0
Position (2,2): 0 + 0 + 0 + 1 = 1
Final output:
[-1  6 -2]
[ 0 -3  1]
[ 0  0  1]
Let me know if you'd like me to explain any of these steps in more detail!
The last calculation is incorrect! As the third input is incorrect.
Question 3
We will move on to the next question. Consider the following digram where the shaded blocks
are the entries participating in one normalization step for a CNN-type architecture. N
represents the mini-batch, H,W represent the different pixels of the â€œimageâ€ at this layer, and
C represents different channels. Which one denotes the process of batch normalization and
Which one denotes layer normalization?
Attached is the diagram for the question.
Answer
Based on the description of the diagram where shaded blocks represent the entries
participating in one normalization step, and given the dimensions N (mini-batch), H and W
(spatial dimensions), and C (channels), the identification of batch normalization and layer
normalization is as follows:


--- Page 6 ---
â€¢ Batch normalization normalizes across the batch dimension N for each feature (i.e., for
each fixed channel C and spatial location H, W). Therefore, in the diagram, batch
normalization corresponds to the option where the shaded blocks are along the N
dimension, meaning that for a single spatial location and channel, all batch elements are
included in the normalization step.
â€¢ Layer normalization normalizes across the feature dimensions C, H, and W for each
batch element N. Therefore, in the diagram, layer normalization corresponds to the option
where the shaded blocks are along the C, H, and W dimensions, meaning that for a single
batch element, all channels and spatial locations are included in the normalization step.
From the provided text, it is not explicitly stated which option (A, B, or C) corresponds to
which normalization type. However, based on the standard conventions:
â€¢ If an option shows shading over the N dimension, it denotes batch normalization.
â€¢ If an option shows shading over the C, H, and W dimensions, it denotes layer
normalization.
You should examine the diagram in the screenshot to identify which option has shading along
N for batch normalization and which has shading along C, H, W for layer normalization. If the
diagram does not clearly show one option with shading over C, H, W, it may be that layer
normalization is represented by an option where all channels and spatial locations are shaded
for a single batch element.
Deepseek model does not have multimodal capabilities, therefore it cannot read the
diagram and can only provide a general answer.
Question 4
Consider a simplified BN where we do not divide by the standard deviation of the data batch.
Instead, we just de-mean our data batch before applying the scaling factor Î³and shifting
factor Î². For simplicity, consider scalar data in an n-sized batch: [x1,x2,...,xn]. Specifically, we
let xi = xiâˆ’Âµ where Âµis the average across the batch and output [y1,y2,...,yn] where yi =
gamma xi + Î² to the next layer. Assume we have a final loss Lsomewhere downstream.
Calculate âˆ‚L in terms of âˆ‚L/âˆ‚xi and âˆ‚L/âˆ‚yj for j = 1,...,n as well as Î³ and Î² as needed.
Numerically, what is âˆ‚L/âˆ‚x1 when n = 1 and our input batch just consists of [x1] with an
output batch of [y1]? (Your answer should be a real number. No need to justify.) What happens


--- Page 7 ---
when nâ†’âˆ? (Feel free to assume here that all relevant quantities are bounded.)
Answer
The general expression for the gradient of the loss with respect to the input xi is:
For n = 1 and input batch [x1], output batch [y1], the numerical value of âˆ‚x1
âˆ‚L is:
As n â†’âˆ, assuming all relevant quantities are bounded, âˆ‚xi
âˆ‚L approaches Î³ âˆ‚yi
âˆ‚L .
Correct!
Question 5
We are moving on to question 3
Depthwise separable convolutions are a type of convolutional operation used in deep learning
for image processing tasks. Unlike traditional convolutional operations, which perform both
spatial and channel-wise convolutions simultaneously, depthwise separable convolutions
decompose the convolution operation into two separate operations: Depthwise convolution
and Pointwise convolution. This can be viewed as a low-rank approximation to a traditional
convolution. For simplicity, throughout this problem, we will ignore biases while counting
learnable parameters.
(a) Suppose the input is a three-channel 224 Ã—224-resolution image, the kernel size of the
convolutional layer is 3 Ã—3, and the number of output channels is 4. What is the number of
learnable parameters in the traditional convolution layer?
(b) Depthwise separable convolution consists of two parts: depthwise convolutions (Fig.2)
followed by pointwise convolutions. Suppose the input is still a three-channel 224
Ã—224-resolution image. The input first goes through depthwise convolutions, where the
number of output channels is the same as the number of input channels, and there is no
âˆ‚xi
âˆ‚L = Î³ (âˆ‚yi
âˆ‚L âˆ’n
1
j=1
âˆ‘
n
âˆ‚yj
âˆ‚L )
0


--- Page 8 ---
â€œcross talkâ€ between different channels. Then, this intermediate output goes through
pointwise convolutions, which is basically a traditional convolution with the filter size being 1
Ã—1. Assume that we have 4 output channels. What is the total number of learnable parameters
of the depthwise separable convolution layer which consists of both depthwise and pointwise
convolutions?
Answer
ğŸ” Step 1: Traditional Convolutional Layer
Given:
â€¢ Input channels: Cin = 3
â€¢ Output channels: Cout = 4
â€¢ Kernel size: 3 Ã— 3
Each filter has:
With 4 filters:
Answer:
ğŸ” Step 2: Depthwise Separable Convolution
(a) Depthwise Convolution
â€¢ Input channels: Cin = 3
â€¢ Output channels: Cout = 3
â€¢ Kernel size: 3 Ã— 3
3 Ã— 3 Ã— 3 = 27 parameters
4 Ã— 27 = 108
108


--- Page 9 ---
Each channel uses a separate filter:
(b) Pointwise Convolution
â€¢ Input channels: 3
â€¢ Output channels: 4
â€¢ Kernel size: 1 Ã— 1
Each filter has:
With 4 filters:
Total parameters:
Answer:
Correct!
Question 6
We are moving to the next question:
Recall that linear regression optimizes the following learning objective:
One way of using dropout during SGD on the ( d )-dimensional input features ( x_i ) involves
keeping each feature at random ( \sim i.i.d.\ Bernoulli(p) ) (and zeroing it out if not kept) and
then performing a traditional SGD step.
3 Ã— (3 Ã— 3) = 27 parameters
1 Ã— 1 Ã— 3 = 3 parameters
4 Ã— 3 = 12 parameters
27 + 12 = 39
39
L(w) = âˆ£y âˆ’Xwâˆ£2
2
(1)


--- Page 10 ---
It turns out that such dropout makes our learning objective effectively become
where ( \odot ) is the element-wise product and the random binary matrix ( R \in {0,1}^{n
\times d} ) is such that ( R_{i,j} \sim i.i.d.\ Bernoulli(p) ).
We use ( \tilde{w} ) to remind you that this is learned by dropout.
Recalling how Tikhonov-regularized (generalized ridge-regression) least-squares problems
involve solving:
for some suitable matrix ( \Gamma ), it turns out we can manipulate (2) to eliminate the
expectations and get:
with ( \tilde{\Gamma} ) being a diagonal matrix whose ( j )-th diagonal entry is the norm of the (
j )-th column of the training matrix ( X ).
(a) Show that we can manipulate (2) to eliminate the expectations and get:
with ( \tilde{\Gamma} ) being a diagonal matrix whose ( j )-th diagonal entry is the norm of the (
j )-th column of the training matrix ( X ).
(b) How should we transform the ( \tilde{w} ) we learn using (5) (i.e., with dropout) to get
something that looks like a solution to the traditionally regularized problem (3)?
Hint: This is related to how we adjust weights learned using dropout training for using
them at inference time. PyTorch by default does this adjustment during training itself, but
here, we are doing dropout slightly differently with no adjustments during training.
(c) With the understanding that the ( \Gamma ) in (3) is an invertible matrix, change variables in
(3) to make the problem look like classical ridge regression:
Explicitly, what is the changed data matrix ( \tilde{X} ) in terms of the original data matrix ( X )
L(w~ ) = ERâˆ¼Bernoulli(p) [âˆ£y âˆ’(R âŠ™X)w~ âˆ£2
2]
(2)
L(w) = âˆ£y âˆ’Xwâˆ£2
2 + âˆ£Î“wâˆ£2
2
(3)
L(w~ ) = âˆ£y âˆ’pXw~ âˆ£2
2 + p(1 âˆ’p)âˆ£Î“
~ w~ âˆ£2
2
(4)
L(w~ ) = âˆ£y âˆ’pXw~ âˆ£2
2 + p(1 âˆ’p)âˆ£Î“
~ w~ âˆ£2
2
(5)
L(w~ ) = âˆ£y âˆ’X~ w~ âˆ£2
2 + Î»âˆ£w~ âˆ£2
2
(6)


--- Page 11 ---
and ( \Gamma )?
(d) Continuing the previous part, with the further understanding that ( \Gamma ) is a diagonal
invertible matrix with the ( j )-th diagonal entry proportional to the norm of the ( j )-th column in
( X ), what can you say about the norms of the columns of the effective training matrix (
\tilde{X} ) and speculate briefly on the relationship between dropout and batch-normalization.
Answer
ğŸ§  Step 1: Problem Overview
We consider linear regression with dropout regularization. The standard least-squares
objective is:
With dropout applied during stochastic gradient descent (SGD), the objective becomes:
where R âˆˆ{0, 1}nÃ—d has i.i.d. Bernoulli(p) entries, and âŠ™denotes element-wise
multiplication.
We aim to show that this expectation simplifies to:
where Î“
~ is a diagonal matrix with the j-th diagonal entry equal to the norm of the j-th column
of X.
ğŸ” Step 2: Part (a) â€” Deriving the Simplified Objective
Let R be a random matrix with entries Ri,j âˆ¼Bernoulli(p). Then:
L(w) = âˆ¥y âˆ’Xwâˆ¥2
2
L(w~ ) = ERâˆ¼Bernoulli(p) [âˆ¥y âˆ’(R âŠ™X)w~ âˆ¥2
2]
L(w~ ) = âˆ¥y âˆ’pXw~ âˆ¥2
2 + p(1 âˆ’p)âˆ¥Î“
~ w~ âˆ¥2
2
(R âŠ™X)w~ =
j
âˆ‘Ri,jXi,jw~ j


--- Page 12 ---
The loss becomes:
For fixed i, let Ai = âˆ‘j Ri,jXi,jw~ j. Then:
â€¢ E[Ai] = p âˆ‘j Xi,jw~ j = p(Xw~ )i
â€¢ Var(Ai) = p(1 âˆ’p) âˆ‘j Xi,j
2 w~
j
2
Using the identity E[(yi âˆ’Ai)2] = (yi âˆ’E[Ai])2 + Var(Ai), we get:
Summing over i:
Note that:
â€¢ âˆ‘i(yi âˆ’p(Xw~ )i)2 = âˆ¥y âˆ’pXw~ âˆ¥2
2
â€¢ âˆ‘i âˆ‘j Xi,j
2 w~
j
2 = âˆ‘j (âˆ‘i Xi,j
2 ) w~
j
2 = âˆ‘j âˆ¥X:,jâˆ¥2
2w~
j
2
Define Î“
~ as the diagonal matrix with Î“
~
jj = âˆ¥X:,jâˆ¥2. Then:
Thus:
ğŸ” Step 3: Part (b) â€” Transforming the Learned Weights
The dropout objective is:
L(w~ ) = E
i
âˆ‘(yi âˆ’
j
âˆ‘Ri,jXi,jw~ j)
2
E[(yi âˆ’Ai)2] = (yi âˆ’p(Xw~ )i)2 + p(1 âˆ’p)
j
âˆ‘Xi,j
2 w~
j
2
L(w~ ) =
i
âˆ‘(yi âˆ’p(Xw~ )i)2 + p(1 âˆ’p)
i
âˆ‘
j
âˆ‘Xi,j
2 w~
j
2
j
âˆ‘âˆ¥X:,jâˆ¥2
2w~
j
2 = âˆ¥Î“
~ w~ âˆ¥2
2
L(w~ ) = âˆ¥y âˆ’pXw~ âˆ¥2
2 + p(1 âˆ’p)âˆ¥Î“
~ w~ âˆ¥2
2


--- Page 13 ---
The traditional regularized objective is:
Let w = pw~ . Then:
â€¢ âˆ¥y âˆ’pXw~ âˆ¥2
2 = âˆ¥y âˆ’Xwâˆ¥2
2
â€¢ p(1 âˆ’p)âˆ¥Î“
~ w~ âˆ¥2
2 =
p
1âˆ’pâˆ¥Î“
~ wâˆ¥2
2
So the transformed objective becomes:
This matches the traditional form with Î“ =
p
1âˆ’pÎ“
~ . Thus, the learned weights w~ should be
scaled by p for inference:
ğŸ”„ Step 4: Part (c) â€” Change of Variables for Ridge
Regression
The regularized objective is:
Let w = Î“âˆ’1w~ . Then:
â€¢ âˆ¥y âˆ’Xwâˆ¥2
2 = âˆ¥y âˆ’XÎ“âˆ’1w~ âˆ¥2
2
â€¢ âˆ¥Î“wâˆ¥2
2 = âˆ¥w~ âˆ¥2
2
So the objective becomes:
where X~ = XÎ“âˆ’1. This is classical ridge regression with Î» = 1.
L(w~ ) = âˆ¥y âˆ’pXw~ âˆ¥2
2 + p(1 âˆ’p)âˆ¥Î“
~ w~ âˆ¥2
2
L(w) = âˆ¥y âˆ’Xwâˆ¥2
2 + âˆ¥Î“wâˆ¥2
2
L(w) = âˆ¥y âˆ’Xwâˆ¥2
2 +
p
1 âˆ’pâˆ¥Î“
~ wâˆ¥2
2
p
L(w) = âˆ¥y âˆ’Xwâˆ¥2
2 + âˆ¥Î“wâˆ¥2
2
L(w~ ) = âˆ¥y âˆ’X~ w~ âˆ¥2
2 + âˆ¥w~ âˆ¥2
2


--- Page 14 ---
ğŸ“Š Step 5: Part (d) â€” Column Norms and Connection to
Batch Normalization
Assume Î“ is diagonal with Î“jj =
p
1âˆ’pâˆ¥X:,jâˆ¥2. Then:
The j-th column of X~ is:
So the norm of each column is:
All columns of X~ have the same norm. This is reminiscent of batch normalization, which
normalizes activations to have zero mean and unit variance. Dropout implicitly normalizes
feature scales, similar to batch normalization.
âœ… Final Answers
â€¢ (a) Derived above.
â€¢ (b) Scale w~ by p: p
â€¢ (c) Transformed data matrix: XÎ“âˆ’1
â€¢ (d) Column norms:
1 âˆ’p
p
XÎ“âˆ’1
X~ = XÎ“âˆ’1
X~
:,j = Î“jj
X:,j =
1 âˆ’p
p
â‹…âˆ¥X:,jâˆ¥2
X:,j
âˆ¥X~
:,jâˆ¥2 =
1 âˆ’p
p
1 âˆ’p
p


--- Page 15 ---
Follow up question
Can you go back to the first question and see that in the question 1(c), your calculation for the
third input seems a little bit wrong!
Answer
I apologize for the error in my previous calculation for question 1(c) regarding the transpose
convolution. Let me provide the corrected solution.
ğŸ” Corrected Calculation for Transpose Convolution
(Question 1(c))
Given:
â€¢ Input: [âˆ’1
3
2
1]
â€¢ Filter: [+1
0
âˆ’1
+1]
â€¢ Parameters: pad=0, stride=1
Output size calculation:
For transpose convolution with stride=1 and no padding, the output size is (input size âˆ’
1) Ã— stride + filter size = (2 âˆ’1) Ã— 1 + 2 = 3, so the output is a 3 Ã— 3 matrix.
Step-by-step computation:
Each input element is multiplied by the entire filter and placed in the output matrix starting at
position (i Ã— stride, j Ã— stride) with overlaps summed.
1. Input element âˆ’1 at (0,0):
Contribution: âˆ’1 Ã— [1
0
âˆ’1
1 ] = [âˆ’1
0
1
âˆ’1]
Placed at: (0,0) to (1,1)
Matrix:


--- Page 16 ---
2. Input element 2 at (0,1):
Contribution: 2 Ã— [1
0
âˆ’1
1 ] = [2
0
âˆ’2
2 ]
Placed at: (0,1) to (1,2)
Matrix:
3. Input element 3 at (1,0):
Contribution: 3 Ã— [1
0
âˆ’1
1 ] = [3
0
âˆ’3
3 ]
Placed at: (1,0) to (2,1)
Matrix:
4. Input element 1 at (1,1):
Contribution: 1 Ã— [1
0
âˆ’1
1 ] = [1
0
âˆ’1
1 ]
Placed at: (1,1) to (2,2)
Matrix:
Summing all contributions:
âˆ’1
0
0
1
âˆ’1
0
0
0
0
0
0
0
2
0
0
âˆ’2
2
0
0
3
0
0
âˆ’3
3
0
0
0
0
0
0
0
1
0
0
âˆ’1
1


--- Page 17 ---
Correct output:
Thank you for pointing out the error. If you have any further questions, feel free to ask!
CorrecT!
âˆ’1
0
0
1
âˆ’1
0
0
0
0
+
0
0
0
2
0
0
âˆ’2
2
0
+
0
3
0
0
âˆ’3
3
0
0
0
+
0
0
0
0
1
0
0
âˆ’1
1
=
âˆ’1
3
0
3
âˆ’3
3
âˆ’2
1
1
âˆ’1
3
0
3
âˆ’3
3
âˆ’2
1
1
