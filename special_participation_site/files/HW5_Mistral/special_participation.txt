--- Page 1 ---
CS 182, Fall 2025.
Kian Hekmatnejad
Special Participation A: HW5 Using Mistral AI
0
Introduction
I will be using Mistral to solve the non-coding parts of HW5. We will show our prompts for each question
and analyze the results. My annotations will be marked in blue and everything else will be generated by
Mistral
1
Problem 1
1.1
Prompt/Explanation
Here is my prompt for the first question. I aim to be as straightforward as possible with my prompting,
essentially repeating the questions in their form on the homework sheet and asking the model to solve them:
I need you to solve problems on deep neural networks. The first question is on convolutional neural networks.
Throughout this problem, we will use the convention of NOT flipping the filter before dragging it over the
signal. Here are the individual parts of the first question. List your solutions and work to get them:
a) List two reasons we typically prefer convolutional layers instead of fully connected layers when working
with image data.
b) Consider the following 1D signal: [1, 4, 0, -2, 3]. After convolution with a length-3 filter, no padding,
stride=1, we get the following sequence: [-2, 2, 11]. What was the filter?
c) If our 2D input is the 2x2 matrix [-1, 2: 3, 1] and the 2D filter is the 2x2 matrix [1, -1; 0, 1], what is the
output of transpose convolution with pad=0 and stride=1? Note that transpose convolution is an operation
to help us upsample a signal (increase the resolution). For example, if our original signal were [a, b, c] and
we perform transpose convolution with pad=0 and stride=2, with the filter [x, y, z], the output would be
[ax, ay, az +bx, by, bz +cx, cy, cz]. Notice that the entries of the input are multiplied by each of the entries
of the filter. Overlaps are summed. Also notice how for a fixed filter size and stride, the dimensions of the
input and output are swapped compared to standard convolution
1.2
Answer
Let’s tackle each part of your question step by step.
a) Two reasons we typically prefer convolutional layers instead of fully connected layers when working
with image data:**
1.
**Parameter Sharing:** Convolutional layers use the same filter (kernel) across the entire input,
drastically reducing the number of parameters compared to fully connected layers. This makes the model
more efficient and less prone to overfitting.
2. **Spatial Hierarchy Preservation:** Convolutional layers exploit the spatial locality of pixels in im-
ages. They capture local patterns (e.g., edges, textures) and hierarchically combine them into more complex
features, which is ideal for image data where nearby pixels are highly correlated.
These responses for part (a) are correct. It properly identifies the reduction in number of parameters and
identifies inductive bias in ”spatial hierarchy preservation”
b) Finding the length-3 filter for the 1D signal:**
**Given:**
- Input signal: [1, 4, 0, −2, 3]
- Output after convolution: [−2, 2, 11]
- Filter length: 3
1


--- Page 2 ---
- Stride: 1
- No padding
**Let the filter be [w1, w2, w3].**
The convolution operation (without flipping the filter) is:
output[i] = input[i] · w1 + input[i + 1] · w2 + input[i + 2] · w3
For the given output:
−2 = 1 · w1 + 4 · w2 + 0 · w32 = 4 · w1 + 0 · w2 + (−2) · w311 = 0 · w1 + (−2) · w2 + 3 · w3
This gives the system of equations:





w1 + 4w2 = −2
4w1 −2w3 = 2
−2w2 + 3w3 = 11
Solving: From the first equation: w1 = −2 −4w2. From the third equation: w3 = 11+2w2
3
.
Substitute into the second equation:
4(−2 −4w2) −2
11 + 2w2
3

= 2
−8 −16w2 −22 + 4w2
3
= 2
Multiply by 3:
−24 −48w2 −22 −4w2 = 6
−52w2 = 52 =⇒w2 = −1
Now, w1 = −2 −4(−1) = 2 and w3 = 11+2(−1)
3
= 3.
**The filter is [2, −1, 3].**
The model correctly identifies the filter using the proper system of equations. The methodology in the
system of equations is identical to the work done in the answer key
c) Transpose Convolution for the 2D Input:**
Given: - Input:

−1
2
3
1

- Filter:

1
−1
0
1

- Pad: 0 - Stride: 1
**Transpose convolution** (also called deconvolution) upsamples the input by ”spreading” each input
value according to the filter, summing overlaps.
For 2D transpose convolution with stride 1 and no padding, the output size is:
output size = input size + filter size −1 = 2 + 2 −1 = 3
So, the output is a 3 × 3 matrix.
**Step-by-step:** - Each input value is multiplied by the entire filter and placed in the output grid,
summing overlaps.
Let’s denote the input as I and the filter as F:
I =
−1
2
3
1

,
F =
1
−1
0
1

The output O is computed as:
O[i, j] =
1
X
m=0
1
X
n=0
I[m, n] · F[i −m, j −n]
2


--- Page 3 ---
where i, j range over the output indices.
**Calculating each element:** - O[0, 0] = I[0, 0] · F[0, 0] = −1 · 1 = −1 - O[0, 1] = I[0, 0] · F[0, 1] + I[0, 1] ·
F[0, 0] = −1·(−1)+2·1 = 1+2 = 3 - O[0, 2] = I[0, 1]·F[0, 1] = 2·(−1) = −2 - O[1, 0] = I[1, 0]·F[0, 0] = 3·1 = 3
- O[1, 1] = I[1, 0] · F[0, 1] + I[1, 1] · F[0, 0] + I[0, 0] · F[1, 0] + I[0, 1] · F[1, 1] = 3 · (−1) + 1 · 1 + (−1) · 0 +
2 · 1 = −3 + 1 + 0 + 2 = 0 - O[1, 2] = I[1, 1] · F[0, 1] + I[0, 1] · F[1, 1] = 1 · (−1) + 2 · 1 = −1 + 2 = 1
- O[2, 0] = I[1, 0] · F[1, 0] = 3 · 0 = 0 - O[2, 1] = I[1, 0] · F[1, 1] + I[1, 1] · F[1, 0] = 3 · 1 + 1 · 0 = 3 -
O[2, 2] = I[1, 1] · F[1, 1] = 1 · 1 = 1
**Final output:**
O =


−1
3
−2
3
0
1
0
3
1


This solution is incorrect, particularly in the middle entry of the output matrix. From the start, the model
uses a different approach from my own solution and the staff solution by summing over individual terms in
the input and filter matrices instead of adding scalar multiples of the shifted 3x3 filter matrix based on the
current input. This leads to an error in the calculation of O[1, 1]. It should have been
= I[0, 0] · F[1, 1] + I[0, 1] · F[1, 0] + I[1, 0] · F[0, 1] + I[1, 1] · F[0, 0] = 3
To correct this issue, I prompted Mistral to identify and alleviate the mistake with the following prompt:
you may have miscalculated the output of the transpose convolution. check your work. It answered:
Let’s carefully re-calculate the transpose convolution step-by-step for the 2D case.
—
**c) Transpose Convolution for the 2D Input (Revisited):**
**Given:** - Input:
−1
2
3
1

- Filter:
1
−1
0
1

- Pad: 0 - Stride: 1
**Transpose convolution** upsamples the input by ”spreading” each input value according to the filter,
summing overlaps. The output size is:
output size = input size + filter size −1 = 2 + 2 −1 = 3
So, the output is a 3 × 3 matrix.
**Step-by-step:** For each position in the output, we place the filter centered at that position (with
zero-padding outside the input) and multiply by the input values, summing the results.
Let’s denote the input as I and the filter as F:
I =
−1
2
3
1

,
F =
1
−1
0
1

The output O is computed as:
O[i, j] =
1
X
m=0
1
X
n=0
I[m, n] · F[i −m, j −n]
where i, j range over the output indices.
**Calculating each element:**
- O[0, 0] = I[0, 0]·F[0, 0] = −1·1 = −1 - O[0, 1] = I[0, 0]·F[0, 1]+I[0, 1]·F[0, 0] = −1·(−1)+2·1 = 1+2 = 3
- O[0, 2] = I[0, 1] · F[0, 1] = 2 · (−1) = −2
- O[1, 0] = I[1, 0] · F[0, 0] + I[0, 0] · F[1, 0] = 3 · 1 + (−1) · 0 = 3 + 0 = 3 - O[1, 1] = I[1, 0] · F[0, 1] +
I[1, 1] · F[0, 0] + I[0, 0] · F[1, 1] + I[0, 1] · F[1, 0] = 3 · (−1) + 1 · 1 + (−1) · 1 + 2 · 0 = −3 + 1 −1 + 0 = −3 -
O[1, 2] = I[1, 1] · F[0, 1] + I[0, 1] · F[1, 1] = 1 · (−1) + 2 · 1 = −1 + 2 = 1
- O[2, 0] = I[1, 0]·F[1, 0]+I[1, 1]·F[0, 0] = 3·0+1·1 = 0+1 = 1 - O[2, 1] = I[1, 1]·F[1, 1]+I[1, 0]·F[1, 0] =
1 · 1 + 3 · 0 = 1 + 0 = 1 - O[2, 2] = I[1, 1] · F[1, 1] = 1 · 1 = 1
3


--- Page 4 ---
**Final output:**
O =


−1
3
−2
3
−3
1
1
1
1


—
**Corrected Summary of Answers:** - **(a)** Parameter sharing and spatial hierarchy preservation. -
**(b)** The filter is [2, −1, 3]. - **(c)** The output of the transpose convolution is:


−1
3
−2
3
−3
1
1
1
1


This time, Mistral correctly calculated O[1, 1], still using the same element-wise operations but using the
proper elements this time. This misconception is difficult to understand - maybe it just miswrote some
numbers for the middle element or there is some kind of deeper confusion.
2
Problem 2.
2.1
Prompt
Here is my prompt for this question. Again, I pretty much copied over the questions from the homework,
also attaching the image and instructing Mistral to solve the problemes with justification:
Let’s do another problem, this time on batch normalization for Convolutional Neural Networks. Here are
the individual parts, please answer each one, showing your work:
a) Consider the diagram attached to this prompt, where the shaded blocks are the entries participating
in one normalization step for a CNN-type architecture. N represents the mini-batch, H, W represent the
different pixels of the “image” at this layer, and C represents different channels. Which of the images (A, B,
C), denotes the process for batch normalization and which one denotes the process of layer normalization
b) Consider a simplified BN where we do not divide by the standard deviation of the data batch. Instead,
we just de-mean our data batch before applying the scaling factor γ and shifting factor β. For simplicity,
consider scalar data in an n-sized batch: [x1, x2, ..., xn]. Specifically, we let xi = xi−µ where µ is the average
1/n Pn
j=1 xj across the batch and output [y1, y2, ..., yn] where yi = γxi + β to the next layer. Assume we
have a final loss L somewhere downstream. Calculate ∂L/∂xi in terms of ∂L/∂yj for j = 1, . . . , n as well
as γ and β as needed. Numerically, what is ∂L/∂x1 when n = 1 and our input batch just consists of [x1]
with an output batch of [y1]? (Your answer should be a real number. No need to justify.) What happens
when n →∞? (Feel free to assume here that all relevant quantities are bounded.)
2.2
Answer
Let’s address each part of your question in detail.
—
**a) Batch Normalization vs. Layer Normalization in the Diagram**
- **Batch Normalization (BatchNorm):** BatchNorm normalizes **across the batch dimension (N)** for
each channel and spatial location. In the diagram, this corresponds to **A**, where the shaded block spans
the batch dimension (N) for a fixed channel (C) and spatial location (H, W).
- **Layer Normalization (LayerNorm):** LayerNorm normalizes **across the channel dimension (C)**
for each spatial location and batch element. In the diagram, this corresponds to **B**, where the shaded
block spans the channel dimension (C) for a fixed batch element (N) and spatial location (H, W). *(Note: C is
sometimes used for channels in LayerNorm, but the key is that normalization is done across features/channels
for each data point, not across the batch.)*
4


--- Page 5 ---
**Summary:** - **A:** Batch Normalization - **B:** Layer Normalization - **C:** Neither (this would
be GroupNorm or InstanceNorm, depending on the exact grouping)
The model correctly identifies which diagram represents layer norm and batch norm.
**b) Simplified Batch Normalization Gradient**
**Given:** - Input batch: [x1, x2, . . . , xn] - Mean: µ = 1
n
Pn
j=1 xj - Centered data: ˆxi = xi −µ - Output:
yi = γˆxi + β - Loss: L
**Goal:** Find
∂L
∂xi in terms of
∂L
∂yj , γ, and β.
—
**Step 1: Express
∂L
∂xi using the chain rule:**
∂L
∂xi
=
n
X
j=1
∂L
∂yj
∂yj
∂xi
Since yj = γˆxj + β and ˆxj = xj −µ, we have:
∂yj
∂xi
= γ ∂ˆxj
∂xi
**Step 2: Compute ∂ˆxj
∂xi :**
ˆxj = xj −µ = xj −1
n
n
X
k=1
xk
So,
∂ˆxj
∂xi
= δij −1
n
where δij is the Kronecker delta (1 if i = j, else 0).
**Step 3: Substitute back:**
∂L
∂xi
=
n
X
j=1
∂L
∂yj
γ

δij −1
n

∂L
∂xi
= γ

∂L
∂yi
−1
n
n
X
j=1
∂L
∂yj


—
**Numerical Value for n = 1:**
For n = 1: - µ = x1 - ˆx1 = x1 −x1 = 0 - y1 = γ · 0 + β = β
Since y1 does not depend on x1 (because ˆx1 = 0):
∂L
∂x1
= 0
—
**Behavior as n →∞:**
As n →∞, the term
1
n
Pn
j=1
∂L
∂yj becomes negligible compared to
∂L
∂yi (assuming bounded gradients).
Thus:
∂L
∂xi
≈γ ∂L
∂yi
The gradient becomes **local** to each xi, and the dependence on the batch mean vanishes.
5


--- Page 6 ---
— The solution to part (b) is also entirely correct. The model correctly calculates the general gradient
and then uses the correct inputs to find it in the special 1 dimensional case. It also identifies the correct
behavior as n goes to ∞, decribing what happens to the gradients. This is an improvement on the error on
question 1.
**Summary of Answers:** - **(a)** A: Batch Normalization, B: Layer Normalization - **(b)** -
∂L
∂xi =
γ

∂L
∂yi −1
n
Pn
j=1
∂L
∂yj

- For n = 1:
∂L
∂x1 = 0 - As n →∞:
∂L
∂xi ≈γ ∂L
∂yi
3
Problem 3.
3.1
Prompt
This problem included some background information, which I did include my prompt, but I did not include
the figures that may have been helpful because I wanted to see how the model would behave without them.
Here is my prompt:
You will solve another question on depthwise separable convolutions.
Depthwise separable convolutions
are a type of convolutional operation used in deep learning for image processing tasks. Unlike traditional
convolutional operations, which perform both spatial and channel-wise convolutions simultaneously, depth-
wise separable convolutions decompose the convolution operation into two separate operations: Depthwise
convolution and Pointwise convolution. This can be viewed as a low-rank approximation to a traditional con-
volution. For simplicity, throughout this problem, we will ignore biases while counting learnable parameters.
Solve each of these parts:
a) Suppose the input is a three-channel 224 × 224-resolution image, the kernel size of the convolutional
layer is 3 × 3, and the number of output channels is 4. What is the number of learnable parameters in the
traditional convolution layer?
b) Depthwise separable convolution consists of two parts: depthwise convolutions followed by pointwise
convolutions. Suppose the input is still a three-channel 224 × 224-resolution image. The input first goes
through depthwise convolutions, where the number of output channels is the same as the number of input
channels, and there is no “cross talk” between different channels.
Then, this intermediate output goes
through pointwise convolutions, which is basically a traditional convolution with the filter size being 1 × 1.
Assume that we have 4 output channels. What is the total number of learnable parameters of the depthwise
separable convolution layer which consists of both depthwise and pointwise convolutions?
3.2
Answer
Let’s calculate the number of learnable parameters for both traditional convolution and depthwise separable
convolution.
—
˚a **a) Traditional Convolution Layer**
**Given:** - Input: 3-channel, 224×224 image - Kernel size: 3×3 - Number of output channels: 4
**Number of learnable parameters:** For a traditional convolution, the number of parameters is:
(kernel height × kernel width × input channels + 1) × output channels
*(We ignore the bias term as per the problem statement.)*
So,
(3 × 3 × 3) × 4 = 27 × 4 = 108
**Answer:** The traditional convolution layer has **108** learnable parameters.
—
**b) Depthwise Separable Convolution Layer**
**1. Depthwise Convolution:** - Input channels: 3 - Output channels: 3 (same as input) - Kernel size:
3×3
6


--- Page 7 ---
For depthwise convolution, each input channel is convolved with a single filter.
So, the number of
parameters is:
(kernel height × kernel width × input channels) = 3 × 3 × 3 = 27
**2. Pointwise Convolution:** - Input channels: 3 (from depthwise output) - Output channels: 4 - Kernel
size: 1×1
For pointwise convolution, the number of parameters is:
(1 × 1 × input channels) × output channels = (1 × 1 × 3) × 4 = 12
**Total Parameters:**
27 (depthwise) + 12 (pointwise) = 39
**Answer:** The depthwise separable convolution layer has **39** learnable parameters.
The model correctly calculates the number of learnable parameters in each case, using the proper formula
for the number of parameters in a traditional convolution and depthwise convolution
4
Problem 4.
4.1
Prompt
For this prompt, instead of writing over the whole question, I copied over a screenshot of the entire problem
and asked Mistral to solve it based on that. Here is my prompt: Now you will solve a question on regu-
larization and dropout. Instead of writing out the question, I have attached an image of all the questions.
Please solve them as you have done previously
4.2
Response
Let’s tackle each part of the problem step-by-step, using the notation and context from the image.
—
**(a) Derive the Dropout Objective**
**Given:**
L( ˜w) = ER∼Bernoulli(p)

∥y −(R ⊙X) ˜w∥2
2

where Ri,j ∼i.i.d Bernoulli(p).
**Goal:** Show that this can be rewritten as:
L( ˜w) = ∥y −pX ˜w∥2
2 + p(1 −p)∥˜Γ ˜w∥2
2
where ˜Γ is a diagonal matrix with the norm of the j-th column of X as its j-th diagonal entry.
—
**Step 1: Expand the Expectation**
ER

∥y −(R ⊙X) ˜w∥2
2

= ER

(y −(R ⊙X) ˜w)T (y −(R ⊙X) ˜w)

Expanding the squared norm:
= ER

yT y −2yT (R ⊙X) ˜w + ˜wT (R ⊙X)T (R ⊙X) ˜w

**Step 2: Linearity of Expectation**
= yT y −2ER

yT (R ⊙X)
 ˜w + ER
 ˜wT (R ⊙X)T (R ⊙X) ˜w

**Step 3: Compute ER[R ⊙X] and ER[(R ⊙X)T (R ⊙X)]**
- ER[R⊙X] = pX (since each entry of R is 1 with probability p and 0 otherwise). - ER[(R⊙X)T (R⊙X)] =
ER[XT (R ⊙R)X] = XT ER[R ⊙R]X = XT (pI)X + XT (diag(p(1 −p)1) ⊙XT X)
7


--- Page 8 ---
But more directly, since R is diagonal and independent for each entry:
ER[(R ⊙X)T (R ⊙X)] = p2XT X + p(1 −p)˜Γ2
where ˜Γ is a diagonal matrix with ˜Γjj = ∥Xj∥2 (the norm of the j-th column of X).
**Step 4: Substitute Back**
L( ˜w) = yT y −2yT pX ˜w + ˜wT 
p2XT X + p(1 −p)˜Γ2
˜w
**Step 5: Rewrite as a Norm**
L( ˜w) = ∥y −pX ˜w∥2
2 + p(1 −p)∥˜Γ ˜w∥2
2
While this solution is mathematically correct, it does skip some essential steps. Namely, the computation
of ER[R ⊙X] and ER[(R ⊙X)T (R ⊙X)] is somewhat unclear. Instead of showing the element-wise sim-
plification, the model seems to skip over a few steps to arrive at the expression for the expectation, citing
properties such as the independence of the entries of R. While this makes sense, it would be better for the
model to go more in-depth into this simplification. Perhaps I could have prompted it better to show more
detail.
**(b) Transforming ˜w for Traditional Regularization**
**Goal:** Transform ˜w to w so that the problem looks like:
L(w) = ∥y −Xw∥2
2 + λ∥w∥2
2
**Solution:** Let w = p ˜w. Then:
L(w) = ∥y −Xw∥2
2 + p(1 −p)∥˜Γw
p ∥2
2 = ∥y −Xw∥2
2 + p(1 −p)
p2
∥˜Γw∥2
2
This resembles ridge regression with λ = p(1−p)
p2
˜Γ2, but since ˜Γ is data-dependent, it’s not exactly ridge
regression unless ˜Γ is proportional to the identity matrix.
The model identifies the correct choice for w and the reasoning behind the choice is sound
**(c) Change of Variables for Classical Ridge Regression**
**Given:**
L( ˜w) = ∥y −˜X ˜w∥2
2 + λ∥˜w∥2
2
**Goal:** Express ˜X in terms of X and Γ.
**Solution:** Let ˜X = XΓ−1. Then:
∥y −˜X ˜w∥2
2 = ∥y −XΓ−1 ˜w∥2
2
Let w = Γ−1 ˜w. Then:
L(w) = ∥y −Xw∥2
2 + λ∥Γw∥2
2
This solution is also correct
**(d) Norms of Columns of ˜X and Relationship to Batch Normalization**
**Given:** Γ is diagonal and invertible, with Γjj ∝∥Xj∥2.
**Effective Training Matrix:** ˜X = XΓ−1
**Norms of Columns of ˜X:**
∥˜Xj∥2 = ∥Xj/Γjj∥2 = ∥Xj∥2
Γjj
∝∥Xj∥2
∥Xj∥2
= 1
8


--- Page 9 ---
**Relationship to Batch Normalization:** - Dropout (with this transformation) effectively normalizes the
columns of the input matrix X to have unit norm, similar to batch normalization, which also normalizes the
input features. - Both techniques aim to stabilize and speed up training by reducing internal covariate shift.
Batch normalization explicitly normalizes activations, while dropout with this transformation implicitly
normalizes the input features during training.
While the model does correctly identify that dropout normalizes the columns of X in an effect similar to
batch norm, it incorrectly claims that the columns have unit norms. In reality, they have a constant norm
of c. This error could be from the lack of the model’s capability to refer back to part 2c to understand why
the norm is c. It may only be thinking in the context of the problem at hand, and go straight to the thought
that any normalization is unit normalization.
5
Executive Summary
In general, Mistral AI performed averagely on the written portion of HW5. It incorrectly answered 2/11
questions, and required one follow-up question pointing out the mistake in each incorrect answer to fix the
issues. Otherwise, Mistral mostly correctly identified key patterns and was able to perform most compu-
tations and proofs. The errors could be attributed to weak prompting from me, but I believe a lack of
capability to refer to the history of the conversation was the root of the error in the last question. The error
in the first question is more puzzling, it seemed to be more of a hallucination where the model added terms
that were not necessary in a summation. It also used a slightly different approach for a couple problems
than what I would have done, namely the part in question 1 that it got wrong. Another potential issue was
the lack of depth in the model’s answer for the derivation of the dropout objective. It seemed to skip a few
steps while still arriving at the right answer - this is an example of a situation where I should have been
more clear about my expectations as the prompter. The speed of the model was excellent - all answers were
generated within 10 seconds.
9
