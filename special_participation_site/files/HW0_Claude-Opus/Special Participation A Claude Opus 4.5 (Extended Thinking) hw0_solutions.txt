--- Page 1 ---
EECS 182 Deep Neural Networks - Homework 0
Solutions
Math Sections (Questions 2, 3, 4, 5)
Question 2: Vector Calculus Review
Setup: Let x, c âˆˆâ„ğ‘›and ğ´âˆˆâ„ğ‘›Ã—ğ‘›.
Convention: Vector derivatives of a scalar with respect to a column vector are
expressed as a row vector:
ğœ•ğ‘“
ğœ•x = [ ğœ•ğ‘“
ğœ•ğ‘¥1
, ğœ•ğ‘“
ğœ•ğ‘¥2
, â€¦ , ğœ•ğ‘“
ğœ•ğ‘¥ğ‘›
]
Part (a): Show
ğœ•
ğœ•x(xğ‘‡c) = cğ‘‡
Step 1: Identify the type of derivative - xğ‘‡c is a scalar (inner product of
two vectors) - The derivative of a scalar w.r.t. a column vector is a row vector
of size 1 Ã— ğ‘›
Step 2: Expand the expression
xğ‘‡c =
ğ‘›
âˆ‘
ğ‘–=1
ğ‘¥ğ‘–ğ‘ğ‘–= ğ‘¥1ğ‘1 + ğ‘¥2ğ‘2 + â‹¯+ ğ‘¥ğ‘›ğ‘ğ‘›
Step 3: Compute an arbitrary entry of the derivative
ğœ•
ğœ•ğ‘¥ğ‘—
(xğ‘‡c) =
ğœ•
ğœ•ğ‘¥ğ‘—
(
ğ‘›
âˆ‘
ğ‘–=1
ğ‘¥ğ‘–ğ‘ğ‘–) = ğ‘ğ‘—
Since only the ğ‘—-th term in the sum contains ğ‘¥ğ‘—, and that term is ğ‘¥ğ‘—ğ‘ğ‘—.
Step 4: Stack all entries
ğœ•
ğœ•x(xğ‘‡c) = [ğ‘1, ğ‘2, â€¦ , ğ‘ğ‘›] = cğ‘‡
ğœ•
ğœ•x(xğ‘‡c) = cğ‘‡
1
Note that this file was generated using Claude Opus 4.5 with Extended Reasoning. 
Claude wrote it's solutions to a markdown file with embedded LaTex and then I compiled 
said file to a pdf with pandoc.
Comparing 2.a to the staff solutions, 
Claude was able to solve this problem 
without error. It's also worth noting 
that Claude in this question goes more 
in depth in it's solutions and steps.


--- Page 2 ---
Part (b): Show
ğœ•
ğœ•xâ€–xâ€–2
2 = 2xğ‘‡
Step 1: Identify the type of derivative - â€–xâ€–2
2 is a scalar - The derivative
is a row vector of size 1 Ã— ğ‘›
Step 2: Expand the expression
â€–xâ€–2
2 = xğ‘‡x =
ğ‘›
âˆ‘
ğ‘–=1
ğ‘¥2
ğ‘–= ğ‘¥2
1 + ğ‘¥2
2 + â‹¯+ ğ‘¥2
ğ‘›
Step 3: Compute an arbitrary entry of the derivative
ğœ•
ğœ•ğ‘¥ğ‘—
â€–xâ€–2
2 =
ğœ•
ğœ•ğ‘¥ğ‘—
(
ğ‘›
âˆ‘
ğ‘–=1
ğ‘¥2
ğ‘–) = 2ğ‘¥ğ‘—
Step 4: Stack all entries
ğœ•
ğœ•xâ€–xâ€–2
2 = [2ğ‘¥1, 2ğ‘¥2, â€¦ , 2ğ‘¥ğ‘›] = 2[ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘›] = 2xğ‘‡
ğœ•
ğœ•xâ€–xâ€–2
2 = 2xğ‘‡
Part (c): Show
ğœ•
ğœ•x(ğ´x) = ğ´
Step 1: Identify the type of derivative - ğ´x is a vector of size ğ‘›Ã— 1 - The
derivative of a vector w.r.t. a vector is a matrix - Specifically: ğœ•(ğ´x)
ğœ•x
has entry
(ğ‘–, ğ‘—) equal to ğœ•(ğ´x)ğ‘–
ğœ•ğ‘¥ğ‘—
- This gives an ğ‘›Ã— ğ‘›matrix
Step 2: Expand the ğ‘–-th component of ğ´x
(ğ´x)ğ‘–=
ğ‘›
âˆ‘
ğ‘˜=1
ğ´ğ‘–ğ‘˜ğ‘¥ğ‘˜
Step 3: Compute an arbitrary entry of the Jacobian
ğœ•(ğ´x)ğ‘–
ğœ•ğ‘¥ğ‘—
=
ğœ•
ğœ•ğ‘¥ğ‘—
(
ğ‘›
âˆ‘
ğ‘˜=1
ğ´ğ‘–ğ‘˜ğ‘¥ğ‘˜) = ğ´ğ‘–ğ‘—
Since only the term with ğ‘˜= ğ‘—contributes.
Step 4: Assemble the matrix The (ğ‘–, ğ‘—) entry of the Jacobian is ğ´ğ‘–ğ‘—, which
means the Jacobian is exactly ğ´.
ğœ•
ğœ•x(ğ´x) = ğ´
2
Comparing 2.b to the staff 
solutions, Claude was able to 
solve this problem without 
error.
Comparing 2.c to the 
staff solutions, Claude 
was able to solve this 
problem without error.


--- Page 3 ---
Part (d): Show
ğœ•
ğœ•x(xğ‘‡ğ´x) = xğ‘‡(ğ´+ ğ´ğ‘‡)
Step 1: Identify the type of derivative - xğ‘‡ğ´x is a scalar (quadratic form)
- The derivative is a row vector of size 1 Ã— ğ‘›
Step 2: Expand the expression
xğ‘‡ğ´x =
ğ‘›
âˆ‘
ğ‘–=1
ğ‘›
âˆ‘
ğ‘—=1
ğ‘¥ğ‘–ğ´ğ‘–ğ‘—ğ‘¥ğ‘—
Step 3: Compute an arbitrary entry of the derivative
ğœ•
ğœ•ğ‘¥ğ‘˜
(xğ‘‡ğ´x) =
ğœ•
ğœ•ğ‘¥ğ‘˜
(
ğ‘›
âˆ‘
ğ‘–=1
ğ‘›
âˆ‘
ğ‘—=1
ğ‘¥ğ‘–ğ´ğ‘–ğ‘—ğ‘¥ğ‘—)
We need to consider three cases for terms in the double sum: - Terms where
ğ‘–= ğ‘˜and ğ‘—â‰ ğ‘˜: ğ‘¥ğ‘˜ğ´ğ‘˜ğ‘—ğ‘¥ğ‘—â†’derivative is ğ´ğ‘˜ğ‘—ğ‘¥ğ‘—- Terms where ğ‘–â‰ ğ‘˜and ğ‘—= ğ‘˜:
ğ‘¥ğ‘–ğ´ğ‘–ğ‘˜ğ‘¥ğ‘˜â†’derivative is ğ‘¥ğ‘–ğ´ğ‘–ğ‘˜- Terms where ğ‘–= ğ‘˜and ğ‘—= ğ‘˜: ğ‘¥ğ‘˜ğ´ğ‘˜ğ‘˜ğ‘¥ğ‘˜= ğ´ğ‘˜ğ‘˜ğ‘¥2
ğ‘˜
â†’derivative is 2ğ´ğ‘˜ğ‘˜ğ‘¥ğ‘˜
Combining all terms:
ğœ•
ğœ•ğ‘¥ğ‘˜
(xğ‘‡ğ´x) = âˆ‘
ğ‘—â‰ ğ‘˜
ğ´ğ‘˜ğ‘—ğ‘¥ğ‘—+ âˆ‘
ğ‘–â‰ ğ‘˜
ğ‘¥ğ‘–ğ´ğ‘–ğ‘˜+ 2ğ´ğ‘˜ğ‘˜ğ‘¥ğ‘˜
=
ğ‘›
âˆ‘
ğ‘—=1
ğ´ğ‘˜ğ‘—ğ‘¥ğ‘—+
ğ‘›
âˆ‘
ğ‘–=1
ğ‘¥ğ‘–ğ´ğ‘–ğ‘˜= (ğ´x)ğ‘˜+ (ğ´ğ‘‡x)ğ‘˜
Step 4: Stack all entries
ğœ•
ğœ•x(xğ‘‡ğ´x) = (ğ´x)ğ‘‡+ (ğ´ğ‘‡x)ğ‘‡= xğ‘‡ğ´ğ‘‡+ xğ‘‡ğ´= xğ‘‡(ğ´ğ‘‡+ ğ´)
ğœ•
ğœ•x(xğ‘‡ğ´x) = xğ‘‡(ğ´+ ğ´ğ‘‡)
Part (e): Under what condition is the previous derivative equal to
2xğ‘‡ğ´?
From part (d):
ğœ•
ğœ•x(xğ‘‡ğ´x) = xğ‘‡(ğ´+ ğ´ğ‘‡)
For this to equal 2xğ‘‡ğ´, we need:
xğ‘‡(ğ´+ ğ´ğ‘‡) = 2xğ‘‡ğ´
3
Comparing 2.d to the staff 
solutions, Claude was able to 
solve this problem without error. 
Once again Claude provides 
more detail and explanation than 
staff solutions.
Comparing 2.d to the staff solutions, 
Claude was able to solve this problem 
without error.


--- Page 4 ---
This requires ğ´+ ğ´ğ‘‡= 2ğ´, which simplifies to:
ğ´ğ‘‡= ğ´
The condition is that ğ´is symmetric: ğ´= ğ´ğ‘‡
Question 3: Least Squares and Min-norm from SVD Per-
spective
Setup: ğ‘‹w = y where ğ‘‹âˆˆâ„ğ‘šÃ—ğ‘›, ğ‘‹= ğ‘ˆÎ£ğ‘‰ğ‘‡is the full SVD.
Î£â€  is ğ‘›Ã— ğ‘šwith
1
ğœğ‘–on the â€œdiagonal.â€
Part (a): Overdetermined case (ğ‘š> ğ‘›) - Finding least squares solu-
tion
When ğ‘š> ğ‘›, we have more equations than unknowns. We want to minimize:
min
w â€–ğ‘‹w âˆ’yâ€–2
Step 1: Expand the objective
â€–ğ‘‹w âˆ’yâ€–2 = (ğ‘‹w âˆ’y)ğ‘‡(ğ‘‹w âˆ’y)
= wğ‘‡ğ‘‹ğ‘‡ğ‘‹w âˆ’2yğ‘‡ğ‘‹w + yğ‘‡y
Step 2: Take the derivative and set to zero
ğœ•
ğœ•wâ€–ğ‘‹w âˆ’yâ€–2 = 2wğ‘‡ğ‘‹ğ‘‡ğ‘‹âˆ’2yğ‘‡ğ‘‹= 0
Transposing:
ğ‘‹ğ‘‡ğ‘‹w = ğ‘‹ğ‘‡y
Step 3: Solve for w
Assuming ğ‘‹ğ‘‡ğ‘‹is invertible (which happens when ğ‘‹has full column rank):
wâˆ—= (ğ‘‹ğ‘‡ğ‘‹)âˆ’1ğ‘‹ğ‘‡y
4
Comparing 3.a to the staff 
solutions, Claude was able to solve 
this problem without error. I also 
and quite pleased with the notation 
Claude uses, defining terms at the 
beginning of quesitons and using 
consistent notation such as bolded 
vectors.


--- Page 5 ---
Part (b): Plug in SVD and simplify
Step 1: Substitute ğ‘‹= ğ‘ˆÎ£ğ‘‰ğ‘‡
ğ‘‹ğ‘‡ğ‘‹= (ğ‘ˆÎ£ğ‘‰ğ‘‡)ğ‘‡(ğ‘ˆÎ£ğ‘‰ğ‘‡) = ğ‘‰Î£ğ‘‡ğ‘ˆğ‘‡ğ‘ˆÎ£ğ‘‰ğ‘‡= ğ‘‰Î£ğ‘‡Î£ğ‘‰ğ‘‡
Note: ğ‘ˆğ‘‡ğ‘ˆ= ğ¼since ğ‘ˆis orthonormal.
Step 2: Compute Î£ğ‘‡Î£ - Î£ is ğ‘šÃ— ğ‘›(with ğ‘š> ğ‘›) - Î£ğ‘‡is ğ‘›Ã— ğ‘š- Î£ğ‘‡Î£ is
ğ‘›Ã— ğ‘›diagonal with ğœ2
ğ‘–on the diagonal
Step 3: Compute (ğ‘‹ğ‘‡ğ‘‹)âˆ’1
(ğ‘‹ğ‘‡ğ‘‹)âˆ’1 = (ğ‘‰Î£ğ‘‡Î£ğ‘‰ğ‘‡)âˆ’1 = ğ‘‰(Î£ğ‘‡Î£)âˆ’1ğ‘‰ğ‘‡
where (Î£ğ‘‡Î£)âˆ’1 is diagonal with
1
ğœ2
ğ‘–entries.
Step 4: Compute ğ‘‹ğ‘‡
ğ‘‹ğ‘‡= ğ‘‰Î£ğ‘‡ğ‘ˆğ‘‡
Step 5: Combine
wâˆ—= (ğ‘‹ğ‘‡ğ‘‹)âˆ’1ğ‘‹ğ‘‡y = ğ‘‰(Î£ğ‘‡Î£)âˆ’1ğ‘‰ğ‘‡â‹…ğ‘‰Î£ğ‘‡ğ‘ˆğ‘‡y
= ğ‘‰(Î£ğ‘‡Î£)âˆ’1Î£ğ‘‡ğ‘ˆğ‘‡y
Since (Î£ğ‘‡Î£)âˆ’1Î£ğ‘‡= Î£â€  (the pseudoinverse of Î£):
wâˆ—= ğ‘‰Î£â€ ğ‘ˆğ‘‡y
Part (c): What happens when we left-multiply ğ‘‹by ğ´= (ğ‘‹ğ‘‡ğ‘‹)âˆ’1ğ‘‹ğ‘‡?
ğ´â‹…ğ‘‹= (ğ‘‹ğ‘‡ğ‘‹)âˆ’1ğ‘‹ğ‘‡â‹…ğ‘‹= (ğ‘‹ğ‘‡ğ‘‹)âˆ’1(ğ‘‹ğ‘‡ğ‘‹) = ğ¼ğ‘›
This gives the ğ‘›Ã— ğ‘›identity matrix.
ğ´ğ‘‹= ğ¼ğ‘›, which is why ğ´= (ğ‘‹ğ‘‡ğ‘‹)âˆ’1ğ‘‹ğ‘‡is called the left-inverse
The left-inverse satisfies ğ´ğ‘‹= ğ¼(left multiplication gives identity).
5
Comparing 3.b to the staff 
solutions, Claude was able to 
solve this problem without 
error.
Comparing 3.c to the staff 
solutions, Claude was able to 
solve this problem without 
error.


--- Page 6 ---
Part (d): Underdetermined case (ğ‘š< ğ‘›) - Finding min-norm solution
When ğ‘š< ğ‘›, we have more unknowns than equations. We want:
min â€–wâ€–2
s.t.
ğ‘‹w = y
Step 1: Set up Lagrangian
â„’(w, ğœ†) = â€–wâ€–2 + ğœ†ğ‘‡(ğ‘‹w âˆ’y)
Step 2: Take derivatives and set to zero
ğœ•â„’
ğœ•w = 2w + ğ‘‹ğ‘‡ğœ†= 0 âŸ¹w = âˆ’1
2ğ‘‹ğ‘‡ğœ†
ğœ•â„’
ğœ•ğœ†= ğ‘‹w âˆ’y = 0
Step 3: Substitute and solve From the first equation: w = âˆ’1
2ğ‘‹ğ‘‡ğœ†
Substitute into the constraint:
ğ‘‹(âˆ’1
2ğ‘‹ğ‘‡ğœ†) = y
âˆ’1
2ğ‘‹ğ‘‹ğ‘‡ğœ†= y
ğœ†= âˆ’2(ğ‘‹ğ‘‹ğ‘‡)âˆ’1y
Step 4: Find optimal w
wâˆ—= âˆ’1
2ğ‘‹ğ‘‡ğœ†= âˆ’1
2ğ‘‹ğ‘‡â‹…(âˆ’2)(ğ‘‹ğ‘‹ğ‘‡)âˆ’1y
wâˆ—= ğ‘‹ğ‘‡(ğ‘‹ğ‘‹ğ‘‡)âˆ’1y
Part (e): Plug in SVD and simplify
Step 1: Substitute ğ‘‹= ğ‘ˆÎ£ğ‘‰ğ‘‡
ğ‘‹ğ‘‹ğ‘‡= ğ‘ˆÎ£ğ‘‰ğ‘‡(ğ‘ˆÎ£ğ‘‰ğ‘‡)ğ‘‡= ğ‘ˆÎ£ğ‘‰ğ‘‡ğ‘‰Î£ğ‘‡ğ‘ˆğ‘‡= ğ‘ˆÎ£Î£ğ‘‡ğ‘ˆğ‘‡
Note: ğ‘‰ğ‘‡ğ‘‰= ğ¼since ğ‘‰is orthonormal.
Step 2: Compute Î£Î£ğ‘‡- Î£ is ğ‘šÃ— ğ‘›(with ğ‘š< ğ‘›) - Î£ğ‘‡is ğ‘›Ã— ğ‘š- Î£Î£ğ‘‡is
ğ‘šÃ— ğ‘šdiagonal with ğœ2
ğ‘–on the diagonal
6
Comparing 3.d to the staff solutions, 
Claude was able to solve this problem 
without error.


--- Page 7 ---
Step 3: Compute (ğ‘‹ğ‘‹ğ‘‡)âˆ’1
(ğ‘‹ğ‘‹ğ‘‡)âˆ’1 = (ğ‘ˆÎ£Î£ğ‘‡ğ‘ˆğ‘‡)âˆ’1 = ğ‘ˆ(Î£Î£ğ‘‡)âˆ’1ğ‘ˆğ‘‡
Step 4: Compute ğ‘‹ğ‘‡
ğ‘‹ğ‘‡= ğ‘‰Î£ğ‘‡ğ‘ˆğ‘‡
Step 5: Combine
wâˆ—= ğ‘‹ğ‘‡(ğ‘‹ğ‘‹ğ‘‡)âˆ’1y = ğ‘‰Î£ğ‘‡ğ‘ˆğ‘‡â‹…ğ‘ˆ(Î£Î£ğ‘‡)âˆ’1ğ‘ˆğ‘‡y
= ğ‘‰Î£ğ‘‡(Î£Î£ğ‘‡)âˆ’1ğ‘ˆğ‘‡y
Since Î£ğ‘‡(Î£Î£ğ‘‡)âˆ’1 = Î£â€ :
wâˆ—= ğ‘‰Î£â€ ğ‘ˆğ‘‡y
Part
(f):
What
happens
when
we
right-multiply
ğ‘‹
by
ğµ
=
ğ‘‹ğ‘‡(ğ‘‹ğ‘‹ğ‘‡)âˆ’1?
ğ‘‹â‹…ğµ= ğ‘‹â‹…ğ‘‹ğ‘‡(ğ‘‹ğ‘‹ğ‘‡)âˆ’1 = (ğ‘‹ğ‘‹ğ‘‡)(ğ‘‹ğ‘‹ğ‘‡)âˆ’1 = ğ¼ğ‘š
This gives the ğ‘šÃ— ğ‘šidentity matrix.
ğ‘‹ğµ= ğ¼ğ‘š, which is why ğµ= ğ‘‹ğ‘‡(ğ‘‹ğ‘‹ğ‘‡)âˆ’1 is called the right-inverse
The right-inverse satisfies ğ‘‹ğµ= ğ¼(right multiplication gives identity).
Question 4: The 5 Interpretations of Ridge Regression
Part (a): Perspective 1 - Optimization Problem
Problem: arg min
w â€–y âˆ’ğ‘‹wâ€–2
2 + ğœ†â€–wâ€–2
2
where ğ‘‹âˆˆâ„ğ‘›Ã—ğ‘‘, y âˆˆâ„ğ‘›.
Step 1: Expand the objective
ğ¿(w) = â€–y âˆ’ğ‘‹wâ€–2
2 + ğœ†â€–wâ€–2
2
= (y âˆ’ğ‘‹w)ğ‘‡(y âˆ’ğ‘‹w) + ğœ†wğ‘‡w
7
Comparing 3.3 to the staff 
solutions, Claude was able to 
solve this problem without error.
Comparing 3.f to the staff 
solutions, Claude was 
able to solve this problem 
without error.
Comparing 4.a to the staff 
solutions, Claude was able to 
solve this problem without 
error.


--- Page 8 ---
= yğ‘‡y âˆ’2yğ‘‡ğ‘‹w + wğ‘‡ğ‘‹ğ‘‡ğ‘‹w + ğœ†wğ‘‡w
Step 2: Take the derivative
ğœ•ğ¿
ğœ•w = âˆ’2ğ‘‹ğ‘‡y + 2ğ‘‹ğ‘‡ğ‘‹w + 2ğœ†w
Using our results from Q2:
ğœ•
ğœ•w(wğ‘‡ğ‘‹ğ‘‡ğ‘‹w) = 2wğ‘‡ğ‘‹ğ‘‡ğ‘‹(since ğ‘‹ğ‘‡ğ‘‹is sym-
metric).
Step 3: Set to zero and solve
âˆ’2ğ‘‹ğ‘‡y + 2ğ‘‹ğ‘‡ğ‘‹w + 2ğœ†w = 0
ğ‘‹ğ‘‡ğ‘‹w + ğœ†w = ğ‘‹ğ‘‡y
(ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼)w = ğ‘‹ğ‘‡y
wâˆ—= (ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼)âˆ’1ğ‘‹ğ‘‡y
Part (b): Perspective 2 - Shifting Singular Values
Goal: Plug SVD ğ‘‹= ğ‘ˆÎ£ğ‘‰ğ‘‡into the Ridge solution and analyze.
Step 1: Compute ğ‘‹ğ‘‡ğ‘‹
ğ‘‹ğ‘‡ğ‘‹= ğ‘‰Î£ğ‘‡ğ‘ˆğ‘‡ğ‘ˆÎ£ğ‘‰ğ‘‡= ğ‘‰Î£ğ‘‡Î£ğ‘‰ğ‘‡
Let ğ·= Î£ğ‘‡Î£, which is a ğ‘‘Ã— ğ‘‘diagonal matrix with ğœ2
ğ‘–on the diagonal.
Step 2: Compute ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼
ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼= ğ‘‰ğ·ğ‘‰ğ‘‡+ ğœ†ğ¼= ğ‘‰ğ·ğ‘‰ğ‘‡+ ğœ†ğ‘‰ğ‘‰ğ‘‡= ğ‘‰(ğ·+ ğœ†ğ¼)ğ‘‰ğ‘‡
(ğ·+ ğœ†ğ¼) is diagonal with entries (ğœ2
ğ‘–+ ğœ†).
Step 3: Compute (ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼)âˆ’1
(ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼)âˆ’1 = ğ‘‰(ğ·+ ğœ†ğ¼)âˆ’1ğ‘‰ğ‘‡
where (ğ·+ ğœ†ğ¼)âˆ’1 is diagonal with entries
1
ğœ2
ğ‘–+ğœ†.
Step 4: Compute the full expression
(ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼)âˆ’1ğ‘‹ğ‘‡= ğ‘‰(ğ·+ ğœ†ğ¼)âˆ’1ğ‘‰ğ‘‡â‹…ğ‘‰Î£ğ‘‡ğ‘ˆğ‘‡= ğ‘‰(ğ·+ ğœ†ğ¼)âˆ’1Î£ğ‘‡ğ‘ˆğ‘‡
8
Comparing 4.b to the staff 
solutions, Claude was able to 
solve this problem without error.


--- Page 9 ---
The effective â€œpseudo-inverseâ€ has entries
ğœğ‘–
ğœ2
ğ‘–+ğœ†along the diagonal (in the ap-
propriate positions).
Analysis:
When ğœğ‘–â‰ªğœ†:
ğœğ‘–
ğœ2
ğ‘–+ ğœ†â‰ˆğœğ‘–
ğœ†â‰ˆ0
Small singular values are suppressed to nearly zero.
When ğœğ‘–â‰«ğœ†:
ğœğ‘–
ğœ2
ğ‘–+ ğœ†â‰ˆğœğ‘–
ğœ2
ğ‘–
= 1
ğœğ‘–
Large singular values behave like the ordinary pseudo-inverse.
Ridge regression â€shrinksâ€ small singular values toward zero while leaving large ones mostly unchanged.
Part (c): Perspective 3 - MAP Estimation
Setup: - Prior: ğ‘Šâˆ¼ğ’©(0, ğ¼) - Likelihood: ğ‘Œ= ğ‘‹ğ‘Š+
âˆš
ğœ†ğ‘where ğ‘âˆ¼ğ’©(0, ğ¼)
Step 1: Write the posterior By Bayesâ€™ rule:
ğ‘ƒ(ğ‘Š|ğ‘Œ= y) âˆğ‘ƒ(ğ‘Œ= y|ğ‘Š) â‹…ğ‘ƒ(ğ‘Š)
Step 2: Compute the likelihood Given ğ‘Š, we have ğ‘Œ|ğ‘Šâˆ¼ğ’©(ğ‘‹ğ‘Š, ğœ†ğ¼)
ğ‘ƒ(y|ğ‘Š) âˆexp (âˆ’1
2ğœ†â€–y âˆ’ğ‘‹ğ‘Šâ€–2)
Step 3: Compute the prior
ğ‘ƒ(ğ‘Š) âˆexp (âˆ’1
2â€–ğ‘Šâ€–2)
Step 4: Compute the posterior
ğ‘ƒ(ğ‘Š|y) âˆexp (âˆ’1
2ğœ†â€–y âˆ’ğ‘‹ğ‘Šâ€–2 âˆ’1
2â€–ğ‘Šâ€–2)
Step 5: Find MAP estimate The MAP estimate maximizes the posterior,
which is equivalent to minimizing the negative log-posterior:
9
Comparing 4.c to the staff 
solutions, Claude was able to 
solve this problem without error.


--- Page 10 ---
Ì‚
ğ‘Šğ‘€ğ´ğ‘ƒ= arg min
ğ‘Š[ 1
2ğœ†â€–y âˆ’ğ‘‹ğ‘Šâ€–2 + 1
2â€–ğ‘Šâ€–2]
Multiplying by ğœ†:
Ì‚
ğ‘Šğ‘€ğ´ğ‘ƒ= arg min
ğ‘Š[1
2â€–y âˆ’ğ‘‹ğ‘Šâ€–2 + ğœ†
2 â€–ğ‘Šâ€–2]
This is equivalent to:
Ì‚
ğ‘Šğ‘€ğ´ğ‘ƒ= arg min
ğ‘Š[â€–y âˆ’ğ‘‹ğ‘Šâ€–2 + ğœ†â€–ğ‘Šâ€–2]
This is exactly the Ridge Regression objective (1).
Part (d): Perspective 4 - Fake Data
Setup:
Ì‚y = [ y
0ğ‘‘
] ,
Ì‚ğ‘‹= [ ğ‘‹
âˆš
ğœ†ğ¼ğ‘‘
]
where
Ì‚y âˆˆâ„ğ‘›+ğ‘‘and
Ì‚ğ‘‹âˆˆâ„(ğ‘›+ğ‘‘)Ã—ğ‘‘.
Step 1: Expand the OLS objective
â€– Ì‚y âˆ’
Ì‚ğ‘‹wâ€–2
2 = âˆ¥[ y
0ğ‘‘
] âˆ’[ ğ‘‹
âˆš
ğœ†ğ¼ğ‘‘
] wâˆ¥
2
2
= âˆ¥[y âˆ’ğ‘‹w
âˆ’
âˆš
ğœ†w ]âˆ¥
2
2
Step 2: Compute the norm
= â€–y âˆ’ğ‘‹wâ€–2
2 + â€– âˆ’
âˆš
ğœ†wâ€–2
2
= â€–y âˆ’ğ‘‹wâ€–2
2 + ğœ†â€–wâ€–2
2
This is exactly the Ridge Regression objective (1).
10
Comparing 4.d to the staff solutions, 
Claude was able to solve this problem 
without error.


--- Page 11 ---
Part (e): Perspective 5 - Fake Features
Setup:
ÌŒğ‘‹= [ğ‘‹
âˆš
ğœ†ğ¼ğ‘›] âˆˆâ„ğ‘›Ã—(ğ‘‘+ğ‘›)
We solve:
arg min
ğœ‚
â€–ğœ‚â€–2
2
s.t.
ÌŒğ‘‹ğœ‚= y
Step 1: Partition ğœ‚Let ğœ‚= [w
z ] where w âˆˆâ„ğ‘‘and z âˆˆâ„ğ‘›.
Step 2: Write the constraint
ÌŒğ‘‹ğœ‚= ğ‘‹w +
âˆš
ğœ†z = y
So z =
1
âˆš
ğœ†(y âˆ’ğ‘‹w).
Step 3: Substitute into the objective
â€–ğœ‚â€–2
2 = â€–wâ€–2
2 + â€–zâ€–2
2 = â€–wâ€–2
2 + 1
ğœ†â€–y âˆ’ğ‘‹wâ€–2
2
Step 4: Minimize over w
min
w [â€–wâ€–2
2 + 1
ğœ†â€–y âˆ’ğ‘‹wâ€–2
2]
Multiplying by ğœ†:
min
w [ğœ†â€–wâ€–2
2 + â€–y âˆ’ğ‘‹wâ€–2
2]
The first ğ‘‘coordinates of ğœ‚âˆ—minimize the Ridge objective (1).
Part (f): Showing equivalence of two Ridge formulas
Step 1: Use pseudo-inverse for the min-norm problem (2)
For
ÌŒğ‘‹ğœ‚= y, the min-norm solution is:
ğœ‚âˆ—=
ÌŒğ‘‹ğ‘‡( ÌŒğ‘‹
ÌŒğ‘‹ğ‘‡)âˆ’1y
Step 2: Compute
ÌŒğ‘‹
ÌŒğ‘‹ğ‘‡
ÌŒğ‘‹
ÌŒğ‘‹ğ‘‡= [ğ‘‹
âˆš
ğœ†ğ¼ğ‘›] [ ğ‘‹ğ‘‡
âˆš
ğœ†ğ¼ğ‘›
] = ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ¼ğ‘›
11
Comparing 4.e to the staff solutions, 
Claude was able to solve this problem 
without error.
Comparing 4.f to the staff solutions, 
Claude was able to solve this problem 
without error.


--- Page 12 ---
Step 3: Compute
ÌŒğ‘‹ğ‘‡
ÌŒğ‘‹ğ‘‡= [ ğ‘‹ğ‘‡
âˆš
ğœ†ğ¼ğ‘›
]
Step 4: Extract the first ğ‘‘coordinates of ğœ‚âˆ—
ğœ‚âˆ—= [ ğ‘‹ğ‘‡
âˆš
ğœ†ğ¼ğ‘›
] (ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ¼)âˆ’1y
The first ğ‘‘coordinates are:
Ì‚w = ğ‘‹ğ‘‡(ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ¼)âˆ’1y
Step 5: Show equivalence to standard formula
We need to show:
ğ‘‹ğ‘‡(ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ¼)âˆ’1 = (ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼)âˆ’1ğ‘‹ğ‘‡
Proof: We verify that both sides give the same result when multiplied on the
right by (ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ¼) and on the left by (ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼):
Claim: (ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼)âˆ’1ğ‘‹ğ‘‡= ğ‘‹ğ‘‡(ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ¼)âˆ’1
Multiply both sides on the left by (ğ‘‹ğ‘‡ğ‘‹+ğœ†ğ¼) and on the right by (ğ‘‹ğ‘‹ğ‘‡+ğœ†ğ¼):
LHS: (ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼) â‹…(ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼)âˆ’1ğ‘‹ğ‘‡â‹…(ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ¼) = ğ‘‹ğ‘‡(ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ¼)
= ğ‘‹ğ‘‡ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ‘‹ğ‘‡
RHS: (ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼) â‹…ğ‘‹ğ‘‡(ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ¼)âˆ’1 â‹…(ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ¼) = (ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼)ğ‘‹ğ‘‡
= ğ‘‹ğ‘‡ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ‘‹ğ‘‡
Both sides are equal, so:
Ì‚w = ğ‘‹ğ‘‡(ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ¼)âˆ’1y = (ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼)âˆ’1ğ‘‹ğ‘‡y
Part (g): What happens when ğœ†â†’âˆ?
From the solution
Ì‚wğ‘Ÿ= (ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼)âˆ’1ğ‘‹ğ‘‡y:
As ğœ†â†’âˆ:
(ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼)âˆ’1 â‰ˆ(ğœ†ğ¼)âˆ’1 = 1
ğœ†ğ¼
12
Comparing 4.g to the staff solutions, 
Claude was able to solve this problem 
without error.


--- Page 13 ---
Therefore:
Ì‚wğ‘Ÿâ‰ˆ1
ğœ†ğ‘‹ğ‘‡y â†’0
As ğœ†â†’âˆ,
Ì‚wğ‘Ÿâ†’0. This is why ridge is called â€shrinkageâ€ - large ğœ†shrinks weights to zero.
Part (h): What happens when ğœ†â†’0?
Case 1: Overdetermined system (ğ‘‹is tall, ğ‘›> ğ‘‘)
As ğœ†â†’0:
Ì‚wğ‘Ÿ= (ğ‘‹ğ‘‡ğ‘‹+ ğœ†ğ¼)âˆ’1ğ‘‹ğ‘‡y â†’(ğ‘‹ğ‘‡ğ‘‹)âˆ’1ğ‘‹ğ‘‡y
This is the ordinary least squares (OLS) solution.
Case 2: Underdetermined system (ğ‘‹is wide, ğ‘›< ğ‘‘)
Using the alternate form:
Ì‚wğ‘Ÿ= ğ‘‹ğ‘‡(ğ‘‹ğ‘‹ğ‘‡+ ğœ†ğ¼)âˆ’1y
As ğœ†â†’0:
Ì‚wğ‘Ÿâ†’ğ‘‹ğ‘‡(ğ‘‹ğ‘‹ğ‘‡)âˆ’1y
This is the minimum-norm solution from Question 3.
In both cases, as ğœ†â†’0, ridge regression converges to the Moore-Penrose pseu-
doinverse solution.
Question 5: ReLU Elbow Update under SGD
Network:
Ì‚ğ‘“(ğ‘¥) = ğ‘Š(2)Î¦(ğ‘Š(1)ğ‘¥+ b)
Loss: â„“= 1
2â€– Ì‚ğ‘“(ğ‘¥) âˆ’ğ‘¦â€–2
2
Part (a): Single ReLU Analysis
For ğœ™(ğ‘¥) = {ğ‘¤ğ‘¥+ ğ‘
if ğ‘¤ğ‘¥+ ğ‘> 0
0
otherwise
Loss: â„“(ğ‘¥, ğ‘¦, ğœ™) = 1
2â€–ğœ™(ğ‘¥) âˆ’ğ‘¦â€–2
2 = 1
2(ğœ™(ğ‘¥) âˆ’ğ‘¦)2
(i) Location of the elbow ğ‘’:
13
Comparing 4.h to the staff 
solutions, Claude was able to 
solve this problem without error.
Comparing 5.a to the staff 
solutions, Claude was able to 
solve this problem without 
error.


--- Page 14 ---
The elbow is where ğœ™transitions from 0 to non-zero, i.e., where ğ‘¤ğ‘¥+ ğ‘= 0:
ğ‘¥= âˆ’ğ‘
ğ‘¤
ğ‘’= âˆ’ğ‘
ğ‘¤
(ii) Derivative of loss w.r.t. ğœ™(ğ‘¥):
ğ‘‘â„“
ğ‘‘ğœ™= ğ‘‘
ğ‘‘ğœ™[1
2(ğœ™(ğ‘¥) âˆ’ğ‘¦)2] = ğœ™(ğ‘¥) âˆ’ğ‘¦
ğ‘‘â„“
ğ‘‘ğœ™= ğœ™(ğ‘¥) âˆ’ğ‘¦
(iii) Partial derivative of loss w.r.t. ğ‘¤:
Using chain rule:
ğœ•â„“
ğœ•ğ‘¤= ğ‘‘â„“
ğ‘‘ğœ™â‹…ğœ•ğœ™
ğœ•ğ‘¤
When ğœ™(ğ‘¥) > 0: ğœ™(ğ‘¥) = ğ‘¤ğ‘¥+ ğ‘, so ğœ•ğœ™
ğœ•ğ‘¤= ğ‘¥
When ğœ™(ğ‘¥) = 0:
ğœ•ğœ™
ğœ•ğ‘¤= 0 (ReLU is in the â€œoffâ€ region)
ğœ•â„“
ğœ•ğ‘¤= {(ğœ™(ğ‘¥) âˆ’ğ‘¦) â‹…ğ‘¥
if ğœ™(ğ‘¥) > 0
0
if ğœ™(ğ‘¥) = 0
(iv) Partial derivative of loss w.r.t. ğ‘:
Using chain rule:
ğœ•â„“
ğœ•ğ‘= ğ‘‘â„“
ğ‘‘ğœ™â‹…ğœ•ğœ™
ğœ•ğ‘
When ğœ™(ğ‘¥) > 0:
ğœ•ğœ™
ğœ•ğ‘= 1
When ğœ™(ğ‘¥) = 0:
ğœ•ğœ™
ğœ•ğ‘= 0
ğœ•â„“
ğœ•ğ‘= {ğœ™(ğ‘¥) âˆ’ğ‘¦
if ğœ™(ğ‘¥) > 0
0
if ğœ™(ğ‘¥) = 0
14


--- Page 15 ---
Part (b): Gradient descent when ğœ™(ğ‘¥) âˆ’ğ‘¦= 1
The prediction is 1 unit above target. Update rule: ğ‘â€² = ğ‘âˆ’ğœ†âˆ‡ğ‘â„“
(i) Case: ğœ™(ğ‘¥) = 0
When ğœ™(ğ‘¥) = 0, weâ€™re in the â€œoffâ€ region of the ReLU. -
ğœ•â„“
ğœ•ğ‘¤= 0 - ğœ•â„“
ğœ•ğ‘= 0
No change to slope or elbow. The gradients are zero when the ReLU is inactive.
(ii) Case: ğ‘¤> 0, ğ‘¥> 0, ğœ™(ğ‘¥) > 0
Given ğœ™(ğ‘¥) âˆ’ğ‘¦= 1: -
ğœ•â„“
ğœ•ğ‘¤= 1 â‹…ğ‘¥= ğ‘¥> 0 - ğœ•â„“
ğœ•ğ‘= 1
Updates: - ğ‘¤â€² = ğ‘¤âˆ’ğœ†ğ‘¥(slope decreases since ğ‘¥> 0) - ğ‘â€² = ğ‘âˆ’ğœ†(bias decreases)
New elbow location:
ğ‘’â€² = âˆ’ğ‘â€²
ğ‘¤â€² = âˆ’ğ‘âˆ’ğœ†
ğ‘¤âˆ’ğœ†ğ‘¥
Letâ€™s analyze: with ğ‘¤> 0, ğ‘¥> 0, and ğœ™(ğ‘¥) = ğ‘¤ğ‘¥+ ğ‘> 0, the elbow was at
ğ‘’= âˆ’ğ‘/ğ‘¤.
Since both ğ‘¤and ğ‘decrease, we need to compare the ratios. The elbow moves
based on the relative changes.
Slope decreases. The elbow location changes depending on ğ‘
ğ‘¤vs 1
ğ‘¥.
(iii) Case: ğ‘¤> 0, ğ‘¥< 0, ğœ™(ğ‘¥) > 0
â€¢
ğœ•â„“
ğœ•ğ‘¤= 1 â‹…ğ‘¥= ğ‘¥< 0
â€¢
ğœ•â„“
ğœ•ğ‘= 1
Updates: - ğ‘¤â€² = ğ‘¤âˆ’ğœ†ğ‘¥= ğ‘¤+ ğœ†|ğ‘¥| (slope increases since ğ‘¥< 0) - ğ‘â€² = ğ‘âˆ’ğœ†
(bias decreases)
New elbow:
ğ‘’â€² = âˆ’
ğ‘âˆ’ğœ†
ğ‘¤+ ğœ†|ğ‘¥|
Since weâ€™re at ğ‘¥< 0 but ğœ™(ğ‘¥) > 0, the elbow must be to the left of ğ‘¥.
Slope increases, bias decreases. Elbow moves right (toward more negative ğ‘¥values becoming inactive).
(iv) Case: ğ‘¤< 0, ğ‘¥> 0, ğœ™(ğ‘¥) > 0
For ğœ™(ğ‘¥) = ğ‘¤ğ‘¥+ ğ‘> 0 with ğ‘¤< 0 and ğ‘¥> 0: we need ğ‘> |ğ‘¤|ğ‘¥.
15
Comparing 5.b to the staff solutions, Claude was able to solve 
this problem without error.


--- Page 16 ---
â€¢
ğœ•â„“
ğœ•ğ‘¤= 1 â‹…ğ‘¥= ğ‘¥> 0
â€¢
ğœ•â„“
ğœ•ğ‘= 1
Updates: - ğ‘¤â€² = ğ‘¤âˆ’ğœ†ğ‘¥(becomes more negative, slope magnitude increases) -
ğ‘â€² = ğ‘âˆ’ğœ†(bias decreases)
Slope becomes more negative (steeper downward). Elbow moves based on relative changes in ğ‘and ğ‘¤.
Diagrams Description: For each case where ğœ™(ğ‘¥) > 0: - Draw the original
ReLU with elbow marked - Show the training point (ğ‘¥, ğ‘¦) with ğœ™(ğ‘¥) = ğ‘¦+ 1
(prediction above target) - Draw arrows showing the function moving downward
toward the target - Indicate elbow movement direction
Part (c): Elbow location for the ğ‘–-th ReLU in full network
The ğ‘–-th ReLU receives input: ğ‘§ğ‘–= ğ‘Š(1)
ğ‘–
ğ‘¥+ ğ‘ğ‘–
where ğ‘Š(1)
ğ‘–
is the ğ‘–-th row of ğ‘Š(1) (a scalar for 1D input).
The ReLU activates when ğ‘§ğ‘–> 0, i.e., when ğ‘Š(1)
ğ‘–
ğ‘¥+ ğ‘ğ‘–> 0.
The elbow (transition point) is where ğ‘Š(1)
ğ‘–
ğ‘¥+ ğ‘ğ‘–= 0:
ğ‘’ğ‘–= âˆ’ğ‘ğ‘–
ğ‘Š(1)
ğ‘–
Part (d): New elbow location after SGD update
Step 1: Compute gradients for the full network
Let ğ‘§ğ‘–= ğ‘Š(1)
ğ‘–
ğ‘¥+ ğ‘ğ‘–and ğ‘ğ‘–= ReLU(ğ‘§ğ‘–).
Output:
Ì‚ğ‘“(ğ‘¥) = âˆ‘ğ‘—ğ‘Š(2)
ğ‘—ğ‘ğ‘—
Loss: â„“= 1
2( Ì‚ğ‘“(ğ‘¥) âˆ’ğ‘¦)2
Step 2: Backpropagation
ğœ•â„“
ğœ•Ì‚ğ‘“
=
Ì‚ğ‘“(ğ‘¥) âˆ’ğ‘¦
ğœ•â„“
ğœ•ğ‘ğ‘–
= ( Ì‚ğ‘“(ğ‘¥) âˆ’ğ‘¦)ğ‘Š(2)
ğ‘–
16
Note that Claude did not or was 
unable to draw the necesary 
diagrams but instead describied 
them. 


--- Page 17 ---
ğœ•â„“
ğœ•ğ‘§ğ‘–
= ( Ì‚ğ‘“(ğ‘¥) âˆ’ğ‘¦)ğ‘Š(2)
ğ‘–
â‹…1ğ‘§ğ‘–>0
ğœ•â„“
ğœ•ğ‘Š(1)
ğ‘–
= ğœ•â„“
ğœ•ğ‘§ğ‘–
â‹…ğ‘¥= ( Ì‚ğ‘“(ğ‘¥) âˆ’ğ‘¦)ğ‘Š(2)
ğ‘–
â‹…1ğ‘§ğ‘–>0 â‹…ğ‘¥
ğœ•â„“
ğœ•ğ‘ğ‘–
= ğœ•â„“
ğœ•ğ‘§ğ‘–
= ( Ì‚ğ‘“(ğ‘¥) âˆ’ğ‘¦)ğ‘Š(2)
ğ‘–
â‹…1ğ‘§ğ‘–>0
Step 3: SGD updates
Let ğ›¿= ( Ì‚ğ‘“(ğ‘¥) âˆ’ğ‘¦)ğ‘Š(2)
ğ‘–
â‹…1ğ‘§ğ‘–>0
(ğ‘Š(1)
ğ‘–
)â€² = ğ‘Š(1)
ğ‘–
âˆ’ğœ†ğ›¿ğ‘¥
ğ‘â€²
ğ‘–= ğ‘ğ‘–âˆ’ğœ†ğ›¿
Step 4: New elbow location
ğ‘’â€²
ğ‘–= âˆ’
ğ‘â€²
ğ‘–
(ğ‘Š(1)
ğ‘–
)â€² = âˆ’
ğ‘ğ‘–âˆ’ğœ†ğ›¿
ğ‘Š(1)
ğ‘–
âˆ’ğœ†ğ›¿ğ‘¥
If ğ‘§ğ‘–â‰¤0 (ReLU inactive), then ğ›¿= 0 and ğ‘’â€²
ğ‘–= ğ‘’ğ‘–(no change).
If ğ‘§ğ‘–> 0 (ReLU active):
ğ‘’â€²
ğ‘–= âˆ’
ğ‘ğ‘–âˆ’ğœ†( Ì‚ğ‘“(ğ‘¥) âˆ’ğ‘¦)ğ‘Š(2)
ğ‘–
ğ‘Š(1)
ğ‘–
âˆ’ğœ†( Ì‚ğ‘“(ğ‘¥) âˆ’ğ‘¦)ğ‘Š(2)
ğ‘–
ğ‘¥
Or equivalently, with ğ›¿= ( Ì‚ğ‘“(ğ‘¥) âˆ’ğ‘¦)ğ‘Š(2)
ğ‘–
:
ğ‘’â€²
ğ‘–=
âˆ’ğ‘ğ‘–+ ğœ†ğ›¿
ğ‘Š(1)
ğ‘–
âˆ’ğœ†ğ›¿ğ‘¥
= ğ‘’ğ‘–ğ‘Š(1)
ğ‘–
+ ğœ†ğ›¿
ğ‘Š(1)
ğ‘–
âˆ’ğœ†ğ›¿ğ‘¥
Summary
This homework covers fundamental concepts essential for deep learning:
1. Vector Calculus (Q2): Forms the basis for understanding gradient com-
putation in neural networks.
2. SVD and Linear Algebra (Q3): Provides insight into how linear sys-
tems are solved, which underlies many optimization algorithms.
17


--- Page 18 ---
3. Ridge Regression (Q4): Shows multiple perspectives on regularization,
a critical concept for preventing overfitting.
4. ReLU and SGD (Q5): Demonstrates how gradient descent updates
affect the geometry of neural network activation functions.
18
Overall I was very impressed with Claude's work. It solved each question correctly in one-
shot and in many cases had more detailed explanations of it's work and steps than the staff 
solutions.
