--- Page 1 ---
Claude HW13 output
You will be answering a series of homework questions from a deep learning
class. I will input multiple subquestions as a screenshots, and questions with
the same number will build upon previous subquestions. Some questions may
take multiple screenshots so make sure to answer all subquestions. You will
need to use context from the first image. Avoid hallucinating or making up
definitions if information is missing; explicitly say so. Answer with clear math
notation and concise justification. Do not skip derivation steps. If the problem
contains multiple subparts (a), (b), (c), or (i),(ii), answer each clearly.
12/9/2025, 4:42:31 PM
I understand! I'm ready to help you with your deep learning homework questions.
I'll:
1. Answer each subquestion clearly and completely
2. Use proper mathematical notation
3. Provide concise justifications without skipping derivation steps
4. Build upon context from previous subquestions when needed
5. Explicitly state if information is missing rather than making assumptions
6. Address all parts (a), (b), (c) or (i), (ii), etc. separately
Please go ahead and share the screenshots of your homework questions, and I'll
work through them systematically.
12/9/2025, 4:42:35 PM
ğŸš€ Powered by Claude Exporter
1 / 18


--- Page 2 ---
ğŸš€ Powered by Claude Exporter
2 / 18


--- Page 3 ---
here is 1a-h
12/9/2025, 4:43:17 PM
I'll work through each part systematically.
Problem 1: DDPM/DDIM Fun - From a Gaussian
Setup: Target distribution is 
 where 
. Forward process starts at
 and adds independent Gaussian noise with variance 
 at
each step from 
 to 
.
(a) What is the distribution of 
?
N(0, Ïƒ )
2
Ïƒ â‰ª
2
1
X
â€‹ âˆ¼
0
N(0, Ïƒ )
2
Î”t â‰ªÏƒ2
t = 0
t = 1
X
â€‹1
ğŸš€ Powered by Claude Exporter
3 / 18


--- Page 4 ---
We can think of 
 as the result of 
 steps of adding independent Gaussian
noise to 
.
Starting with 
, after adding 
 independent Gaussian noises each with
variance 
:
where 
 are independent.
Since the sum of independent Gaussians is Gaussian:
Mean: 
Variance: 
(b) Marginal distributions of 
 and 
By the same reasoning as part (a), at any time  we've added 
 noise steps:
(c) Simplify the variance of the conditional distribution
Given: 
We need to simplify the variance 
 when 
.
X
â€‹1
T =
â€‹
Î”t
1
X
â€‹0
X
â€‹ âˆ¼
0
N(0, Ïƒ )
2
T
Î”t
X
â€‹ =
1
X
â€‹ +
0
â€‹Ïµ
â€‹
i=1
âˆ‘
T
i
Ïµ
â€‹ âˆ¼
i
N(0, Î”t)
0 + 0 = 0
Ïƒ +
2
T â‹…Î”t = Ïƒ +
2
â€‹ â‹…
Î”t
1
Î”t = Ïƒ +
2
1
â€‹
X
â€‹ âˆ¼N(0, Ïƒ + 1)
1
2
X
â€‹
tâˆ’Î”t
X
â€‹t
t
(t/Î”t)
â€‹
X
â€‹ âˆ¼N(0, Ïƒ + t)
t
2
â€‹
X
â€‹ âˆ¼N(0, Ïƒ + t âˆ’Î”t)
tâˆ’Î”t
2
(X
â€‹âˆ£X
â€‹ =
tâˆ’Î”t
t
x
â€‹) âˆ¼
t
N
â€‹x
â€‹,
â€‹
(
Ïƒ +t
2
Ïƒ +tâˆ’Î”t
2
t
Ïƒ +t
2
(Ïƒ +tâˆ’Î”t)Î”t
2
)
â€‹
Ïƒ +t
2
(Ïƒ +tâˆ’Î”t)Î”t
2
Î”t â‰ªÏƒ â‰ª
2
1
â€‹ =
Ïƒ + t
2
(Ïƒ + t âˆ’Î”t)Î”t
2
â€‹ =
Ïƒ + t
2
(Ïƒ + t)Î”t âˆ’(Î”t)
2
2
Î”t âˆ’
â€‹
Ïƒ + t
2
(Î”t)2
ğŸš€ Powered by Claude Exporter
4 / 18


--- Page 5 ---
Since 
 and 
, we have 
 and 
.
Therefore: 
(d) Naive reverse denoising without stochastic noise
Start with 
 and iteratively apply conditional mean mapping backward
with 
 steps.
At each step: 
After 
 steps from 
 to 
:
Since 
:
Telescoping: Notice the product telescopes - most terms cancel, leaving only
.
Since 
, the variance 
, which is far too small
compared to the desired 
.
(e) Variance with added stochastic noise
Î”t â‰ªÏƒ â‰ª
2
1
t âˆˆ[0, 1]
(Î”t) â‰ª
2
Î”t
Ïƒ +
2
t â‰¥Ïƒ >
2
0
â€‹ â‰ª
Ïƒ +t
2
(Î”t)2
Î”t
â€‹
Variance â‰ˆÎ”t
â€‹ âˆ¼
X^1
N(0, 1)
T =
â€‹
Î”t
1
â€‹ =
X^tâˆ’Î”t
â€‹
â€‹
Ïƒ +t
2
Ïƒ +tâˆ’Î”t
2
X^t
T
t = 1
t = 0
â€‹ =
X^0
â€‹
â€‹ â‹…
k=0
âˆ
Tâˆ’1
Ïƒ + 1 âˆ’kÎ”t
2
Ïƒ + 1 âˆ’(k + 1)Î”t
2
â€‹
X^1
=
â€‹
â€‹
Ïƒ + 1
2
Ïƒ2
X^1
â€‹ âˆ¼
X^1
N(0, 1)
â€‹
â€‹ âˆ¼N
0,
â€‹
X^0
(
(Ïƒ + 1
2
Ïƒ2
)
2
)
â€‹ =
Ïƒ +1
2
Ïƒ +0
2
â€‹
Ïƒ +1
2
Ïƒ2
Ïƒ â‰ª
2
1
â€‹
â‰ˆ
( Ïƒ +1
2
Ïƒ2 )
2
(Ïƒ ) =
2 2
Ïƒ â‰ª
4
Ïƒ2
Ïƒ2
ğŸš€ Powered by Claude Exporter
5 / 18


--- Page 6 ---
Given the variance formula: $$\text{Var}(\hat{X}0) = \left(\frac{\sigma^2}
{\sigma^2 + 1}\right)^2 + \Delta t \sum{k=0}^{T-1} \left(\frac{\sigma^2}
{\sigma^2 + k\Delta t}\right)^2$$
Taking 
 (so T \to \infty$), convert the sum to an integral with $k\Delta t \to s:
Evaluate the integral: $$\int_0^1 \frac{\sigma^4}{(\sigma^2 + s)^2} ds =
\sigma^4 \int_0^1 \frac{1}{(\sigma^2 + s)^2} ds$$
Using the hint 
:
Therefore: $$\text{Var}(\hat{X}_0) = \frac{\sigma^4}{(\sigma^2 + 1)^2} +
\frac{\sigma^2}{\sigma^2 + 1}$$
Since 
, the first term 
:
This matches the desired variance 
!
(f) Training a neural network for DDIM
Given dataset 
 drawn from 
, we want to train a neural net to
estimate 
.
Inputs to the neural net: 
 where 
 is a noisy sample and  is the
time/noise level.
Î”t â†’0
Var(
â€‹) =
X^0
â€‹
+
(Ïƒ + 1
2
Ïƒ2
)
2
â€‹
â€‹
ds
âˆ«
0
1
(Ïƒ + s
2
Ïƒ2
)
2
â€‹dt =
âˆ«(1+t)2
1
C âˆ’
â€‹
1+t
1
= Ïƒ
âˆ’
â€‹
â€‹ =
4 [
Ïƒ + s
2
1
]
0
1
Ïƒ
âˆ’
â€‹ +
â€‹
4 (
Ïƒ + 1
2
1
Ïƒ2
1 )
= Ïƒ â‹…
4
â€‹ =
Ïƒ (Ïƒ + 1)
2
2
(Ïƒ + 1) âˆ’Ïƒ
2
2
â€‹ =
Ïƒ (Ïƒ + 1)
2
2
Ïƒ4
Ïƒ + 1
2
Ïƒ2
Ïƒ â‰ª
2
1
â‰ˆÏƒ â‰ª
4
Ïƒ2
â€‹
Var(
â€‹) â‰ˆ
â€‹ â‰ˆÏƒ (1 âˆ’Ïƒ ) â‰ˆÏƒ
X^0
Ïƒ + 1
2
Ïƒ2
2
2
2
Ïƒ2
s
â€‹, s
â€‹, â€¦ , s
â€‹
1
2
n
N(0, Ïƒ )
2
g(x
â€‹, t) =
t
â€‹x
â€‹
Ïƒ +t
2Ïƒ2
t
(x
â€‹, t)
t
x
â€‹t
t
ğŸš€ Powered by Claude Exporter
6 / 18


--- Page 7 ---
Generating a batch:
1. Sample 
 from the dataset
2. Sample random time 
3. Generate noisy version: 
 where 
4. Batch consists of pairs 
 with targets 
Loss function: Mean squared error between network output 
 and target
:
(g) Approximate the DDIM step for 
Given 
 and the DDIM update with 
.
The DDIM step is: 
When 
, approximate 
:
Using 
 for small :
Therefore:
s
â€‹i
t âˆ¼Uniform(0, 1)
x
â€‹ =
t
s
â€‹ +
i
Ïµ
Ïµ âˆ¼N(0, t)
(x
â€‹, t)
t
g(x
â€‹, t) =
t
â€‹x
â€‹
Ïƒ +t
2
Ïƒ2
t
f
â€‹(x
â€‹, t)
Î¸
t
â€‹x
â€‹
Ïƒ +t
2Ïƒ2
t
â€‹
L = E
â€‹
â€‹f
â€‹(x
â€‹, t) âˆ’
â€‹x
â€‹
â€‹
s
â€‹,t,Ïµ
i
[
Î¸
t
Ïƒ + t
2
Ïƒ2
t
2
]
Î”t â‰ªt
g(x
â€‹, t) =
t
â€‹x
â€‹
Ïƒ +t
2
Ïƒ2
t
Î·(t, Î”t) =
â€‹
â€‹+
â€‹
tâˆ’Î”t
t
â€‹t
x
â€‹ =
tâˆ’Î”t
DDIM
x
â€‹ +
t
Î·(t, Î”t) â‹…(deterministicÂ DDPMÂ step)
Î”t â‰ªt
Î·(t, Î”t)
Î·(t, Î”t) =
â€‹ =
â€‹ +
â€‹
t âˆ’Î”t
t
â€‹t
â€‹
â€‹
â€‹ +
â€‹
t
1 âˆ’Î”t/t
t
â€‹t
â€‹ â‰ˆ
1 âˆ’x
1 âˆ’x/2
x
â‰ˆ
â€‹ =
â€‹(1 âˆ’Î”t/(2t)) +
â€‹
t
t
â€‹t
â€‹ â‰ˆ
2 âˆ’Î”t/(2t)
1
â€‹2
1
â€‹
x
â€‹ â‰ˆx
â€‹ âˆ’
â€‹x
â€‹
tâˆ’Î”t
DDIM
t
2(Ïƒ + t)
2
Î”t
t
ğŸš€ Powered by Claude Exporter
7 / 18


--- Page 8 ---
(h) Compute 
 from 
 via DDIM
Applying the DDIM step from part (g) iteratively:
After 
 steps, expressing as a product:
Taking logarithm and converting to sum:
Using 
 for small :
Taking 
, convert to integral with 
:
Therefore: $$\hat{X}_0 = \hat{X}_1 \cdot \exp\left(-\frac{1}{2}\ln\frac{\sigma^2
+ 1}{\sigma^2}\right) = \hat{X}_1 \cdot \sqrt{\frac{\sigma^2}{\sigma^2 +
1}}$$
Since 
:
This matches the desired distribution!
12/9/2025, 4:44:08 PM
â€‹
X^0
â€‹ âˆ¼
X^1
N(0, 1)
x
â€‹ =
tâˆ’Î”t
x
â€‹
1 âˆ’
â€‹
t (
2(Ïƒ + t)
2
Î”t
)
T
â€‹ =
X^0
â€‹
â€‹
1 âˆ’
â€‹
X^1
k=0
âˆ
Tâˆ’1
(
2(Ïƒ + 1 âˆ’kÎ”t)
2
Î”t
)
ln(
â€‹/
â€‹) =
X^0 X^1
â€‹ln
1 âˆ’
â€‹
k=0
âˆ‘
Tâˆ’1
(
2(Ïƒ + 1 âˆ’kÎ”t)
2
Î”t
)
ln(1 âˆ’x) â‰ˆâˆ’x
x
â‰ˆâˆ’
â€‹
â€‹
k=0
âˆ‘
Tâˆ’1
2(Ïƒ + 1 âˆ’kÎ”t)
2
Î”t
Î”t â†’0
s = 1 âˆ’kÎ”t
â†’âˆ’
â€‹
â€‹ds =
âˆ«
0
1
2(Ïƒ + s)
2
1
âˆ’
â€‹[ln(Ïƒ +
2
1
2
s)]
â€‹ =
0
1
âˆ’
â€‹ ln
â€‹
2
1
Ïƒ2
Ïƒ + 1
2
â€‹ âˆ¼
X^1
N(0, 1)
â€‹
â€‹ âˆ¼N
0,
â€‹
â‰ˆN(0, Ïƒ )
X^0
(
Ïƒ + 1
2
Ïƒ2
)
2
ğŸš€ Powered by Claude Exporter
8 / 18


--- Page 9 ---
ğŸš€ Powered by Claude Exporter
9 / 18


--- Page 10 ---
ğŸš€ Powered by Claude Exporter
10 / 18


--- Page 11 ---
ğŸš€ Powered by Claude Exporter
11 / 18


--- Page 12 ---
here is 2a-g
12/9/2025, 4:45:06 PM
I'll work through each part of Problem 2 on Direct Preference Optimization (DPO).
Problem 2: Honey, Where's My Reward Model?
ğŸš€ Powered by Claude Exporter
12 / 18


--- Page 13 ---
Context: RLHF pipeline has 3 phases: SFT â†’ Reward Modeling â†’ RL Fine-Tuning.
DPO bypasses the explicit reward model and RL loop.
(a) Find the optimal 
 for 
Given: $$\mathbb{D}{\text{KL}}(p | q) = \mathbb{E}{x \sim p(x)} \log \frac{p(x)}
{q(x)} = \sum_x p(x) \log \frac{p(x)}{q(x)} \geq 0$$
To minimize, we use the method of Lagrange multipliers with the constraint
:
Taking the derivative with respect to 
: $$\frac{\partial \mathcal{L}}{\partial
p(x)} = \log \frac{p(x)}{q(x)} + 1 - \lambda = 0$$
This gives: $$\log p(x) - \log q(x) = \lambda - 1$$ $$p(x) = e^{\lambda - 1} q(x)$$
Using the constraint 
: $$e^{\lambda - 1} \sum_x q(x) = e^{\lambda -
1} \cdot 1 = 1$$ $$e^{\lambda - 1} = 1$$
Therefore: $$\boxed{p^*(x) = q(x)}$$
The KL divergence is minimized when  equals , giving 
.
(b) Show that the optimal solution 
 takes the given
form
We want to maximize: $$\max_{\pi_\theta} \mathbb{E}{x \sim \mathcal{D}, y \sim
\pi\theta(y|x)} [r_\phi(x,y)] - \beta \mathbb{D}{\text{KL}}[\pi\theta(y|x) |
\pi_{\text{ref}}(y|x)]$$
pâˆ—
min
â€‹ D
â€‹(pâˆ¥q)
pâˆˆP
KL
â€‹ p(x) =
âˆ‘x
1
L =
â€‹p(x) log
â€‹ âˆ’
x
âˆ‘
q(x)
p(x)
Î»
â€‹p(x) âˆ’1
(
x
âˆ‘
)
p(x)
â€‹ p(x) =
âˆ‘x
1
p
q
D
â€‹(p âˆ¥q) =
KL
âˆ—
0
Ï€
â€‹(yâˆ£x)
Î¸âˆ—
ğŸš€ Powered by Claude Exporter
13 / 18


--- Page 14 ---
For fixed , this is: $$\max_{\pi_\theta(\cdot|x)} \sum_y \pi_\theta(y|x) r_\phi(x,y) -
\beta \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}$$
Rearranging: $$\max_{\pi_\theta(\cdot|x)} \sum_y \pi_\theta(y|x) \left[r_\phi(x,y) -
\beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}\right]$$
This is equivalent to minimizing: $$\mathbb{D}{\text{KL}}\left[\pi\theta(\cdot|x) |
\frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta}
r_\phi(x,y)\right)\right]$$
From part (a), the optimal 
 equals the target distribution:
where 
 is the partition function.
(c) Why is this representation hard to utilize in practice?
Answer: The output space 
 can be extremely large (or even infinite for text
generation).
For language models, 
 consists of all possible sequences of tokens, which grows
exponentially with sequence length. Computing the partition function 
 requires summing over all possible outputs, which is
computationally intractable.
(d) Express the reward function 
 in terms of 
, 
,
and 
x
=
â€‹
â€‹Ï€
â€‹(yâˆ£x)
â€‹r
â€‹(x, y) + log Ï€
â€‹(yâˆ£x) âˆ’
Ï€
â€‹(â‹…âˆ£x)
Î¸
max
y
âˆ‘
Î¸
[Î²
1
Ï•
ref
]
Î²
Ï€
â€‹(yâˆ£x) log Ï€
â€‹(yâˆ£x)
y
âˆ‘
Î¸
Î¸
Ï€
â€‹
Î¸âˆ—
â€‹
Ï€
â€‹(yâˆ£x) =
â€‹Ï€
â€‹(yâˆ£x) exp
â€‹r
â€‹(x, y)
Î¸âˆ—
Z(x)
1
ref
(Î²
1
Ï•
)
Z(x) =
â€‹ Ï€
â€‹(yâˆ£x) exp
â€‹r
â€‹(x, y)
âˆ‘y
ref
( Î²
1
Ï•
)
Y
Y
Z(x) =
â€‹ Ï€
â€‹(yâˆ£x) exp
â€‹r
â€‹(x, y)
âˆ‘y
ref
( Î²
1
Ï•
)
r
â€‹(x, y)
Ï•
Ï€
â€‹
Î¸âˆ—Ï€
â€‹
ref
Z(x)
ğŸš€ Powered by Claude Exporter
14 / 18


--- Page 15 ---
From part (b): $$\pi_{\theta^*}(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x)
\exp\left(\frac{1}{\beta} r_\phi(x,y)\right)$$
Taking logarithms: $$\log \pi_{\theta^*}(y|x) = -\log Z(x) + \log \pi_{\text{ref}}
(y|x) + \frac{1}{\beta} r_\phi(x,y)$$
Solving for 
: $$\frac{1}{\beta} r_\phi(x,y) = \log \pi_{\theta^*}(y|x) - \log
\pi_{\text{ref}}(y|x) + \log Z(x)$$
(e) Substitute into Bradley-Terry model and show that 
cancels
The Bradley-Terry model is: $$p^(y_w \succ y_l | x) = \frac{\exp(r^(x, y_w))}
{\exp(r^(x, y_w)) + \exp(r^(x, y_l))}$$
Substituting 
:
The 
 terms cancel:
Why is this desirable? We no longer need to compute the intractable partition
function 
! The preference probability depends only on the ratio of policies,
r
â€‹(x, y)
Ï•
â€‹
r
â€‹(x, y) = Î² log
â€‹ + Î² log Z(x)
Ï•
Ï€
â€‹(yâˆ£x)
ref
Ï€
â€‹(yâˆ£x)
Î¸âˆ—
Z(x)
r
â€‹(x, y) =
Ï•
Î² log
â€‹ +
Ï€
â€‹(yâˆ£x)
ref
Ï€
â€‹(yâˆ£x)
Î¸âˆ—
Î² log Z(x)
p (y
â€‹ â‰»
âˆ—
w
y
â€‹âˆ£x) =
l
â€‹
exp Î² log
â€‹ + Î² log Z(x)
+ exp Î² log
â€‹ + Î² log Z(x)
(
Ï€
â€‹(y
â€‹âˆ£x)
ref
w
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
w
)
(
Ï€
â€‹(y
â€‹âˆ£x)
ref
l
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
l
)
exp Î² log
â€‹ + Î² log Z(x)
(
Ï€
â€‹(y
â€‹âˆ£x)
ref
w
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
w
)
=
â€‹
Z(x)
â€‹
+ Z(x)
â€‹
Î² ( Ï€
â€‹(y
â€‹âˆ£x)
ref
w
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
w
)
Î²
Î² ( Ï€
(y
â€‹âˆ£x)
ref
l
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
l
)
Î²
Z(x)
â€‹
Î² ( Ï€
â€‹(y
â€‹âˆ£x)
ref
w
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
w
)
Î²
Z(x)Î²
â€‹
p (y
â€‹ â‰»y
â€‹âˆ£x) =
â€‹
âˆ—
w
l
â€‹
+
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
w
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
w
)
Î²
( Ï€
â€‹(y
â€‹âˆ£x)
ref
l
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
l
)
Î²
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
w
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
w
)
Î²
Z(x)
ğŸš€ Powered by Claude Exporter
15 / 18


--- Page 16 ---
which we can compute directly.
(f) Derive the gradient of DPO loss and interpret the
weighting term
Given implicit reward: 
The DPO loss is: $$\mathcal{L}{\text{DPO}}(\pi\theta; \pi_{\text{ref}}) = -
\mathbb{E}{(x, y_w, y_l) \sim \mathcal{D}} \left[\log \sigma\left(\beta \log
\frac{\pi\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}
{\pi_{\text{ref}}(y_l|x)}\right)\right]$$
Let 
Then 
.
Using 
 and the chain rule:
Now: $$\frac{\partial z}{\partial \theta} = \beta \frac{\nabla_\theta
\pi_\theta(y_w|x)}{\pi_\theta(y_w|x)} - \beta \frac{\nabla_\theta \pi_\theta(y_l|x)}
{\pi_\theta(y_l|x)}$$
Therefore: $$\boxed{\nabla_\theta \mathcal{L}{\text{DPO}} = -\beta \sigma(-z)
[\nabla\theta \log \pi_\theta(y_w|x) - \nabla_\theta \log \pi_\theta(y_l|x)]}$$
where 
.
Interpretation of weighting term 
:
When 
 (model is correct, strongly prefers winner):
, so gradient is small â†’ updates less
â€‹(x, y) =
r^Î¸
Î² log
â€‹
Ï€
â€‹(yâˆ£x)
ref
Ï€
â€‹(yâˆ£x)
Î¸
z = Î² log
â€‹ âˆ’
Ï€
(y
â€‹âˆ£x)
ref
w
Ï€
â€‹(y
â€‹âˆ£x)
Î¸
w
Î² log
â€‹ =
Ï€
â€‹(y
â€‹âˆ£x)
ref
l
Ï€
â€‹(y
â€‹âˆ£x)
Î¸
l
â€‹(x, y
â€‹) âˆ’
r^Î¸
w
â€‹(x, y
â€‹)
r^Î¸
l
L = âˆ’log Ïƒ(z)
Ïƒ (z) =
â€²
Ïƒ(z)(1 âˆ’Ïƒ(z)) = Ïƒ(z)Ïƒ(âˆ’z)
â€‹ =
âˆ‚Î¸
âˆ‚L
âˆ’
â€‹
â€‹ =
Ïƒ(z)
Ïƒ (z)
â€²
âˆ‚Î¸
âˆ‚z
âˆ’Ïƒ(âˆ’z)
â€‹
âˆ‚Î¸
âˆ‚z
= Î²âˆ‡
â€‹ log Ï€
â€‹(y
â€‹âˆ£x) âˆ’
Î¸
Î¸
w
Î²âˆ‡
â€‹ log Ï€
â€‹(y
â€‹âˆ£x)
Î¸
Î¸
l
z =
â€‹(x, y
â€‹) âˆ’
r^Î¸
w
â€‹(x, y
â€‹)
r^Î¸
l
Ïƒ(
â€‹(x, y
â€‹) âˆ’
r^Î¸
l
â€‹(x, y
â€‹))
r^Î¸
w
â€‹(x, y
â€‹) â‰«
r^Î¸
w
â€‹(x, y
â€‹)
r^Î¸
l
Ïƒ(âˆ’z) â‰ˆ0
ğŸš€ Powered by Claude Exporter
16 / 18


--- Page 17 ---
When 
 (model is incorrect, prefers loser): 
, so
gradient is large â†’ updates more
The gradient update is larger when the model is incorrect, providing adaptive
learning that focuses on mistakes.
(g) Plackett-Luce model - show 
 cancels
Given the Plackett-Luce probability for ranking : $$p_{\theta^*}(\tau | y_1, \ldots,
y_K, x) = \prod_{k=1}^{K} \frac{\exp(r_\phi(x, y_{\tau(k)}))}{\sum_{j=k}^{K}
\exp(r_\phi(x, y_{\tau(j)}))}$$
Using the implicit reward from part (d): 
Substituting: $$\exp(r_\phi(x,y)) = Z(x)^\beta \left(\frac{\pi_{\theta^*}(y|x)}
{\pi_{\text{ref}}(y|x)}\right)^\beta$$
Each term in the product becomes: $$\frac{\exp(r_\phi(x, y_{\tau(k)}))}
{\sum_{j=k}^{K} \exp(r_\phi(x, y_{\tau(j)}))} = \frac{Z(x)^\beta
\left(\frac{\pi_{\theta^}(y_{\tau(k)}|x)}{\pi_{\text{ref}}
(y_{\tau(k)}|x)}\right)^\beta}{\sum_{j=k}^{K} Z(x)^\beta
\left(\frac{\pi_{\theta^}(y_{\tau(j)}|x)}{\pi_{\text{ref}}
(y_{\tau(j)}|x)}\right)^\beta}$$
The 
 cancels from numerator and denominator:
Therefore, the final Plackett-Luce probability is:
â€‹(x, y
â€‹) â‰ª
r^Î¸
w
â€‹(x, y
â€‹)
r^Î¸
l
Ïƒ(âˆ’z) â‰ˆ1
Z(x)
Ï„
r
â€‹(x, y) =
Ï•
Î² log
â€‹ +
Ï€
â€‹(yâˆ£x)
ref
Ï€
â€‹(yâˆ£x)
Î¸âˆ—
Î² log Z(x)
Z(x)Î²
=
â€‹
â€‹
â€‹
âˆ‘j=k
K
( Ï€
â€‹(y
â€‹âˆ£x)
ref
Ï„(j)
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
Ï„(j)
)
Î²
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
Ï„(k)
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
Ï„(k)
)
Î²
â€‹
p
â€‹(Ï„âˆ£y
â€‹, â€¦ , y
â€‹, x) =
â€‹
â€‹
Î¸âˆ—
1
K
k=1
âˆ
K
â€‹
â€‹
âˆ‘j=k
K
( Ï€
â€‹(y
â€‹âˆ£x)
ref
Ï„(j)
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
Ï„(j)
)
Î²
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
Ï„(k)
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
Ï„(k)
)
Î²
ğŸš€ Powered by Claude Exporter
17 / 18


--- Page 18 ---
This is solely in terms of 
 and 
, with no partition function 
!
The corresponding DPO loss becomes: $$\mathcal{L}{\text{DPO}}(\pi\theta;
\pi_{\text{ref}}) = -\mathbb{E}{(\tau, y_1, \ldots, y_K, x\tau) \sim \mathcal{D}}
\left[\log \sigma\left(\beta \log \frac{\pi\theta(y_{\tau(k)}|x)}{\pi_{\text{ref}}
(y_{\tau(k)}|x)} - \beta \log \frac{\pi_\theta(y_{\tau(j)}|x)}{\pi_{\text{ref}}
(y_{\tau(j)}|x)}\right)\right]$$
for appropriate pairs in the ranking.
12/9/2025, 4:46:02 PM
Ï€
â€‹
Î¸âˆ—
Ï€
â€‹
ref
Z(x)
ğŸš€ Powered by Claude Exporter
18 / 18


--- Page 19 ---
â€‹Question 1â€‹
â€‹(a) Correctâ€‹
â€‹Claude correctly used the steps of adding independent Gaussians to solve the problem.â€‹
â€‹(b) Correctâ€‹
â€‹Claude correctly extended the logic from (a) to obtain X_{t-Î”t} ~ N(0, ÏƒÂ² + t - Î”t) and X_t ~ N(0,â€‹
â€‹ÏƒÂ² + t) by recognizing variance accumulates linearly with time in the forward diffusion process.â€‹
â€‹(c) Correctâ€‹
â€‹Claude properly simplified (ÏƒÂ² + t - Î”t)Î”t/(ÏƒÂ² + t) by factoring and recognizing that when Î”t â‰ª ÏƒÂ²â€‹
â€‹â‰ª 1, the term Î”t/(ÏƒÂ² + t) is negligible, yielding the approximation Var â‰ˆ Î”t. The algebraic stepsâ€‹
â€‹were clear and justified.â€‹
â€‹(d) Correctâ€‹
â€‹Claude successfully demonstrated the telescoping product XÌ‚ â‚€ = Xâ‚ Â· âˆ(ÏƒÂ² + 1 - kÎ”t)/(ÏƒÂ² + 1 -â€‹
â€‹(k+1)Î”t), correctly identified this telescopes to (ÏƒÂ²/(ÏƒÂ² + 1))Â·Xâ‚, and concluded Var(XÌ‚ â‚€) â‰ˆ Ïƒâ´ â‰ª ÏƒÂ².â€‹
â€‹The explanation clearly identified why this is problematic. The variance collapses too muchâ€‹
â€‹without stochastic noise.â€‹
â€‹(e) Correct, good derivation.â€‹
â€‹Claude set up the Riemann sum correctly, converted to the integral âˆ«â‚€Â¹ Ïƒâ´/(ÏƒÂ² + s)Â² ds, and thenâ€‹
â€‹performed the integration correctly and explicitly.â€‹
â€‹â—â€‹ â€‹Applied the hint âˆ« 1/(1+t)Â² dt = C - 1/(1+t)â€‹
â€‹â—â€‹ â€‹Evaluated Ïƒâ´[1/(ÏƒÂ² + s)]â‚€Â¹ = Ïƒâ´(1/ÏƒÂ² - 1/(ÏƒÂ² + 1))â€‹
â€‹â—â€‹ â€‹Simplified to ÏƒÂ²Â·[(ÏƒÂ² + 1) - ÏƒÂ²]/(ÏƒÂ² + 1) = ÏƒÂ²/(ÏƒÂ² + 1)â€‹
â€‹â—â€‹ â€‹Combined with the deterministic term (ÏƒÂ²/(ÏƒÂ² + 1))Â² to get total varianceâ€‹
â€‹â—â€‹ â€‹Showed the first term â‰ˆ Ïƒâ´ â‰ª ÏƒÂ² when ÏƒÂ² â‰ª 1â€‹
â€‹â—â€‹ â€‹Concluded Var(XÌ‚ â‚€) â‰ˆ ÏƒÂ²/(ÏƒÂ² + 1) â‰ˆ ÏƒÂ²â€‹
â€‹(f) Correct with good procedureâ€‹
â€‹1.â€‹ â€‹Specified inputs: (x_t, t) pairsâ€‹
â€‹2.â€‹ â€‹Detailed batch generation procedure:â€‹
â€‹â—‹â€‹ â€‹Sample s_i from datasetâ€‹
â€‹â—‹â€‹ â€‹Sample t ~ Uniform(0,1)â€‹
â€‹â—‹â€‹ â€‹Generate x_t = s_i + Îµ where Îµ ~ N(0,t)â€‹
â€‹â—‹â€‹ â€‹Form pairs (x_t, t) with targets g*(x_t, t) = ÏƒÂ²/(ÏƒÂ² + t)Â·x_tâ€‹
â€‹3.â€‹ â€‹Specified loss function: MSE between network output f_Î¸(x_t, t) and targetâ€‹


--- Page 20 ---
â€‹4.â€‹ â€‹Clear objective: L = E[|f_Î¸(x_t, t) - ÏƒÂ²/(ÏƒÂ² + t)Â·x_t|Â²]â€‹
â€‹Claude gave a good level of specificity (which is what the problem requested) and gaveâ€‹
â€‹actionable implementation details.â€‹
â€‹(g) Correctâ€‹
â€‹Claude approximated Î·(t, Î”t) = âˆšt/(âˆš(t-Î”t) + âˆšt) for Î”t â‰ª t:â€‹
â€‹â—â€‹ â€‹Factored as âˆšt/(âˆšt(âˆš(1-Î”t/t) + 1))â€‹
â€‹â—â€‹ â€‹Approximated âˆš(1-Î”t/t) â‰ˆ 1 - Î”t/(2t) using âˆš(1-x) â‰ˆ 1 - x/2â€‹
â€‹â—â€‹ â€‹Obtained Î· â‰ˆ 1/(2 - Î”t/(2t)) â‰ˆ 1/2â€‹
â€‹â—â€‹ â€‹Derived final form: x_{t-Î”t} â‰ˆ x_t(1 - Î”t/(2(ÏƒÂ² + t)))â€‹
â€‹The approximation was properly justified and the algebraic steps were shown clearly.â€‹
â€‹(h) Correctâ€‹
â€‹Claude provided the complete calculation:â€‹
â€‹1.â€‹ â€‹Started with the product: XÌ‚ â‚€ = XÌ‚ â‚ âˆ(1 - Î”t/(2(ÏƒÂ² + 1 - kÎ”t)))â€‹
â€‹2.â€‹ â€‹Took logarithms: ln(XÌ‚ â‚€/XÌ‚ â‚) = Î£ ln(1 - Î”t/(2(ÏƒÂ² + 1 - kÎ”t)))â€‹
â€‹3.â€‹ â€‹Applied ln(1-x) â‰ˆ -x: â‰ˆ -Î£ Î”t/(2(ÏƒÂ² + 1 - kÎ”t))â€‹
â€‹4.â€‹ â€‹Converted to Riemann integral: â†’ -âˆ«â‚€Â¹ ds/(2(ÏƒÂ² + s))â€‹
â€‹5.â€‹ â€‹Evaluated: = -(1/2)[ln(ÏƒÂ² + s)]â‚€Â¹ = -(1/2)ln((ÏƒÂ² + 1)/ÏƒÂ²)â€‹
â€‹6.â€‹ â€‹Exponentiated: XÌ‚ â‚€ = XÌ‚ â‚Â·âˆš(ÏƒÂ²/(ÏƒÂ² + 1))â€‹
â€‹7.â€‹ â€‹Concluded: XÌ‚ â‚€ ~ N(0, ÏƒÂ²/(ÏƒÂ² + 1)) â‰ˆ N(0, ÏƒÂ²)â€‹
â€‹Claude showed every step of converting the product to a sum, sum to integral, evaluating theâ€‹
â€‹integral, and exponentiating back.â€‹
â€‹Questions 2:â€‹
â€‹(a) Correctâ€‹
â€‹Claude correctly solved using Lagrange multipliers, took derivatives with respect to p(x), setâ€‹
â€‹equal to zero, and obtained p*(x) = q(x). The derivation was complete and properly showed thatâ€‹
â€‹e^(Î»-1) = 1 from the normalization constraint.â€‹
â€‹(b) Correctâ€‹
â€‹Claude was able to solve this correctly with explicit derivation.â€‹
â€‹1.â€‹ â€‹Wrote out the objective for fixed x: max Î£ Ï€_Î¸(y|x)[r_Ï†(x,y) - Î² log(Ï€_Î¸(y|x)/Ï€_ref(y|x))]â€‹
â€‹2.â€‹ â€‹Rearranged to show it's equivalent to minimizing KL divergence fromâ€‹
â€‹(1/Z(x))Ï€_ref(y|x)exp((1/Î²)r_Ï†(x,y))â€‹
â€‹3.â€‹ â€‹Applied result from part (a) to conclude the optimal policy equals this distributionâ€‹


--- Page 21 ---
â€‹4.â€‹ â€‹Identified Z(x) as the partition functionâ€‹
â€‹One thing I noticed is that Claude did not show every Lagrange multiplier calculation step but itâ€‹
â€‹did show the connection to KL minimization and showing how the optimal form emerges. Thisâ€‹
â€‹demonstrates understanding of the constrained optimization structure.â€‹
â€‹(c) Correctâ€‹
â€‹Claude correctly identified that computing Z(x) = Î£_y Ï€_ref(y|x)Â·exp(r_Ï†(x,y)/Î²) is intractable forâ€‹
â€‹large output spaces like text generation.â€‹
â€‹(d) Correctâ€‹
â€‹Claude correctly rearranged: log Ï€_Î¸*(y|x) = -log Z(x) + log Ï€_ref(y|x) + (1/Î²)r_Ï†(x,y), thenâ€‹
â€‹solved to get r_Ï†(x,y) = Î² log(Ï€_Î¸*(y|x)/Ï€_ref(y|x)) + Î² log Z(x). The algebra was shownâ€‹
â€‹step-by-step.â€‹
â€‹(e) Correctâ€‹
â€‹Claude substituted the reward parametrization into the Bradley-Terry formula, showed thatâ€‹
â€‹exp(r_Ï†(x,y)) = Z(x)^Î²Â·(Ï€_Î¸*(y|x)/Ï€_ref(y|x))^Î², demonstrated the Z(x)^Î² terms cancel fromâ€‹
â€‹numerator and denominator, and clearly explained why this is desirable.â€‹
â€‹(f) Correctâ€‹
â€‹1.â€‹ â€‹Defined z = rÌ‚_Î¸(x,y_w) - rÌ‚_Î¸(x,y_l) and L = -log Ïƒ(z)â€‹
â€‹2.â€‹ â€‹Applied chain rule: âˆ‚L/âˆ‚Î¸ = -(Ïƒ'(z)/Ïƒ(z))Â·âˆ‚z/âˆ‚Î¸â€‹
â€‹3.â€‹ â€‹Used Ïƒ'(z) = Ïƒ(z)Ïƒ(-z) to get âˆ‚L/âˆ‚Î¸ = -Ïƒ(-z)Â·âˆ‚z/âˆ‚Î¸â€‹
â€‹4.â€‹ â€‹Computed âˆ‚z/âˆ‚Î¸ = Î²âˆ‡_Î¸ log Ï€_Î¸(y_w|x) - Î²âˆ‡_Î¸ log Ï€_Î¸(y_l|x)â€‹
â€‹5.â€‹ â€‹Combined to get final form: âˆ‡_Î¸ L_DPO = -Î² Ïƒ(rÌ‚_Î¸(x,y_l) - rÌ‚_Î¸(x,y_w))Â·(âˆ‡_Î¸ logâ€‹
â€‹Ï€_Î¸(y_w|x) - âˆ‡_Î¸ log Ï€_Î¸(y_l|x))â€‹
â€‹Claude explicitly showed the chain rule application and intermediate steps. The interpretation ofâ€‹
â€‹the sigmoid weighting was also correct: large gradient when model is wrong (Ïƒ(-z) â‰ˆ 1), smallâ€‹
â€‹when correct (Ïƒ(-z) â‰ˆ 0).â€‹
â€‹(g) Correctâ€‹
â€‹Claude substituted r_Ï†(x,y) = Î² log(Ï€_Î¸*(y|x)/Ï€_ref(y|x)) + Î² log Z(x) into the Plackett-Luceâ€‹
â€‹formula, showed that exp(r_Ï†(x,y)) = Z(x)^Î²Â·(Ï€_Î¸*(y|x)/Ï€_ref(y|x))^Î², demonstrated that Z(x)^Î²â€‹
â€‹appears in both numerator and denominator of each term in the product and cancels, and wroteâ€‹
â€‹the final probability depending only on Ï€_Î¸* and Ï€_ref. The algebra was thorough and correct.â€‹


--- Page 22 ---
â€‹Summaryâ€‹
â€‹Overall, I am very impressed with how Claude did on this homework. I was expecting it to solveâ€‹
â€‹most of the earlier parts correctly (which it did) but I was pleasantly surprised it was able to doâ€‹
â€‹the full derivations for the later parts correctly and (mostly) in full. I think some of the solutionsâ€‹
â€‹could have used a little more detail, such as in 2b, but overall definitely very good. I would say itâ€‹
â€‹solved every question correctly. All the derivations were very explicit and easy to understand,â€‹
â€‹and Claude rarely skipped steps in the process even when the traces were getting long. All ofâ€‹
â€‹this was in one attempt.â€‹
â€‹For example In part 1(e), the integral evaluation required several non-trivial steps: recognizingâ€‹
â€‹the Riemann sum structure, applying the substitution correctly, evaluating the antiderivative atâ€‹
â€‹the bounds, and then combining terms. And claude was able to execute each step well.â€‹
â€‹Similarly, in part 1(h), the product-to-integral conversion involved taking logarithms, applying aâ€‹
â€‹first-order approximation, recognizing the Riemann sum, evaluating the integral, andâ€‹
â€‹exponentiating back. This is a long chain of reasoning where earlier mistakes could cascadeâ€‹
â€‹through the rest of the calculation, but Claude was able to maintain accuracy throughout.â€‹
â€‹Part 2(b) reveals an interesting aspect of Claude's approach. While it didn't write out everyâ€‹
â€‹single step of the Lagrange multiplier calculation (taking derivatives, setting to zero, solving theâ€‹
â€‹system), it showed the key conceptual insight: recognizing that the constrained optimizationâ€‹
â€‹problem is equivalent to KL minimization from a specific target distribution. This suggestsâ€‹
â€‹Claude prioritizes conveying the mathematical structure and reasoning over mechanical symbolâ€‹
â€‹manipulation.â€‹
