--- Page 1 ---
Hi Deepseek, You are my study partner for Homework 8. This part of 
the homework contains only non-coding problems (conceptual, 
mathematical, proofs, and reasoning questions).
For each question:
1. Read the question carefully and explain in your own words what it 
is asking.
2. Solve the problem step by step. Be explicit in reasoning and 
algebra. If there are multiple interpretations, note them and explain 
your choice.
3. If you are unsure at any step, clearly indicate your uncertainty.
4. After finishing, perform a self-check: does your answer directly 
address the question? Are there possible mistakes or gaps? Suggest 
ways to verify your result.
Format for each question can be flexible but should include your 
reasoning and self-checks.
First, I need to understand the format. I'm acting as a study partner for Homework 8, which
has non-coding problems. For each question, I must:
1. Explain the question in my own words.
2. Solve step by step with clear reasoning.
3. Indicate uncertainty if any.
4. Self-check after.
Since no specific questions are provided, I'll assume that the user will give me the questions
one by one. But since this is a response, I should wait for the questions. Looking back at the
user's message, it seems like they might have forgotten to include the actual questions. The
user says: "This part of the homework contains only non-coding problems" but doesn't list
them.
Perhaps I need to prompt the user to provide the questions. But since this is a simulation, I
should just proceed as if the questions will be provided in the conversation.
The user might be expecting me to generate questions or something, but that doesn't make
sense. Let me read carefully: "You are my study partner for Homework 8. This part of the
å·²æ€è€ƒï¼ˆç”¨æ—¶ 14 ç§’ï¼‰
I gave him a few rules to guide his thinking patte


--- Page 2 ---
Hello! I'm excited to be your study partner for Homework 8. I understand that this
section focuses on non-coding problems, such as conceptual, mathematical, proofs,
and reasoning questions. I'm ready to help you tackle each question step by step.
Since you haven't provided any specific questions yet, please share the first
question from your homework. Once you do, I'll:
1. Explain the question in my own words to ensure I understand what it's asking.
2. Solve the problem step by step with explicit reasoning and algebra, noting any
multiple interpretations and explaining my choices.
3. Indicate any uncertainties clearly if I encounter them.
4. Perform a self-check after solving to verify that my answer addresses the
question directly and suggest ways to catch possible mistakes or gaps.
Go ahead and paste the first question! I'm here to work through it with you.
homework contains only non-coding problems..."
I think the user intends to provide the questions later, so for now, I should just acknowledge
and wait. But since this is a text-based interaction, I'll respond accordingly.
To be helpful, I'll state my readiness to help with the questions once they are provided.
So, my response should be: I'm ready to help with your homework questions. Please provide
the first question, and I'll follow the format you specified.
But since the user might be testing me, I should demonstrate with a sample question if
possible. But I don't have any sample.
Another thought: perhaps the homework questions are implied, but no, the user didn't attach
anything.
I think I should just respond as per the instruction.
image.png
PNG 263.42KB
1. SSM Convolution Kernel
 Background and Setup: Consider a discrete-time State-Space 
Model (SSM) of the form
 xk+1 = Axk +Buk,
 yk = Cxk +Duk,
 (a) ConvolutionKernelandtheOutputEquation. 
GiventhatthesequencelengthisL(input: (u0,Â·Â·Â· ,uL),
 output: (y0,Â·Â·Â· ,yL)) and assume x0 = 0, show that the output yk can 
I provided him with both image and text titles to prevent omissio
during copy-pasting.


--- Page 3 ---
be expressed as a convolution
 of the input sequence {uâ„“}L
 0 with a kernel K = {Kâ„“}L
 0:
 L
 yk =
 â„“=0
 Kâ„“ukâˆ’â„“,
 where any uâ‰¤0 with a negative index is set to 0 (zero-padding). 
Also, find K.
 (b) Concrete Examples.
 i. Scalar Case: Let n = 1, and set A = Î±, B = Î², C = Î³, D = Î´. Use Î± = 
0.8,Î² = 1,Î³ = 1.5
 and compute the kernel up to L = 4.
 ii. 2D Case: Let A âˆˆ R2Ã—2 be, for instance,
 A= 0.7 0.1
 0.2 0.6 , B= 1
 0 , C= 1 0 , D=0
 Compute kernels up to L = 3 and briefly discuss how the kernel 
captures the â€œimpulse responseâ€.
 (c) Efficient Computation with Convolutions. If we already know the 
kernel K, how much can we
 parallelize the computation of the output sequence {yk} for an input 
sequence {uk} âˆˆ Rd of length L?
 What is the minimum critical path length of the computation? What 
about a naive, direct computation
 of yk from the unrolled recursion?
 (d) Efficient Kernel Computation. Given A,B,C, how can we compute 
the kernel, K, efficiently?
 What are some strategies to parallelize kernel computation? You 
may assume L = 2N for some N for
 simplicity.
 (e) Adding structure to A. Suppose A is a diagonal matrix. How can 
we leverage this structure to
 compute the kernel K more efficiently?
 (f) Diagonal-plus-low-rank (DPLR) structure Now if A has the 
following form:
 A=In+ppâŠ¤,
 where A âˆˆ RnÃ—n,p âˆˆ Rn. How can we leverage this structure to 
compute the kernel K more
 efficiently?
å·²æ€è€ƒï¼ˆç”¨æ—¶ 220 ç§’ï¼‰


--- Page 4 ---
ğŸ“˜ Solution for Problem 1: SSM Convolution Kernel
ğŸ”¹ Part (a): Convolution Kernel and Output Equation
â“ Question Interpretation
We are given a discrete-time State-Space Model (SSM):
State update: 
Output: 
with initial condition 
. We need to show that the output 
 can be
expressed as a convolution of the input sequence 
 with a kernel 
, and
explicitly find 
.
ğŸ§  Step-by-Step Solution
1. Unroll the state recurrence:
In general:
2. Substitute into the output equation:
3. Change the summation index:
Let 
. Then 
, and when 
, 
; when 
, 
.
So:
x
â€‹ =
k+1
Ax
â€‹ +
k
Bu
â€‹k
y
â€‹ =
k
Cx
â€‹ +
k
Du
â€‹k
x
â€‹ =
0
0
y
â€‹k
{u
â€‹}
â„“
K
K
x
â€‹ =
0
0
x
â€‹ =
1
Ax
â€‹ +
0
Bu
â€‹ =
0
Bu
â€‹0
x
â€‹ =
2
Ax
â€‹ +
1
Bu
â€‹ =
1
ABu
â€‹ +
0
Bu
â€‹1
x
â€‹ =
3
Ax
â€‹ +
2
Bu
â€‹ =
2
A Bu
â€‹ +
2
0
ABu
â€‹ +
1
Bu
â€‹2
x
â€‹ =
k
â€‹A
Bu
â€‹
j=0
âˆ‘
kâˆ’1
kâˆ’1âˆ’j
j
y
â€‹ =
k
Cx
â€‹ +
k
Du
â€‹ =
k
C
â€‹A
Bu
â€‹
+
(
j=0
âˆ‘
kâˆ’1
kâˆ’1âˆ’j
j)
Du
â€‹k
â„“= k âˆ’j
j = k âˆ’â„“
j = 0 â„“= k
j = k âˆ’1 â„“= 1


--- Page 5 ---
4. Combine into a single sum:
Define the kernel 
 as:
 for 
Then:
where 
 for 
.
âœ… Self-Check
For 
: 
, matches the original output.
For 
: 
, matches the original output.
The kernel 
 is explicitly defined and depends only on 
.
ğŸ”¹ Part (b): Concrete Examples
â“ Question Interpretation
We compute the kernel 
 for two specific SSMs and discuss how it represents the
impulse response.
ğŸ§  Step-by-Step Solution
(i) Scalar Case
, 
, 
, 
Kernel:
y
â€‹ =
k
â€‹CA
Bu
â€‹ +
â„“=1
âˆ‘
k
â„“âˆ’1
kâˆ’â„“
Du
â€‹k
K = {K
â€‹}
â„“
K
â€‹ =
0
D
K
â€‹ =
â„“
CA
B
â„“âˆ’1
â„“= 1, 2, â€¦ , L
y
â€‹ =
k
â€‹K
â€‹u
â€‹
â„“=0
âˆ‘
L
â„“
kâˆ’â„“
u
â€‹ =
kâˆ’â„“
0
k âˆ’â„“< 0
k = 0 y
â€‹ =
0
K
â€‹u
â€‹ =
0
0
Du
â€‹0
k = 1 y
â€‹ =
1
K
â€‹u
â€‹ +
0
1
K
â€‹u
â€‹ =
1
0
Du
â€‹ +
1
CBu
â€‹0
K
A, B, C, D
K
A = Î± = 0.8 B = Î² = 1 C = Î³ = 1.5 D = Î´ = 0
K
â€‹ =
0
D = 0
K
â€‹ =
1
CB = 1.5


--- Page 6 ---
Result: 
(ii) 2D Case
, 
, 
, 
Kernel:
Result: 
ğŸ’¡ Discussion on Impulse Response
If the input is an impulse 
, 
 for 
, then:
...
So the kernel 
 is exactly the impulse response of the system.
âœ… Self-Check
Computations are straightforward matrix multiplications.
The kernel decays as  increases, reflecting the stability of 
.
ğŸ”¹ Part (c): Efficient Computation with Convolutions
K
â€‹ =
2
CAB = 1.5 Ã— 0.8 = 1.2
K
â€‹ =
3
CA B =
2
1.5 Ã— 0.8 =
2
0.96
K
â€‹ =
4
CA B =
3
1.5 Ã— 0.8 =
3
0.768
K = [0, 1.5, 1.2, 0.96, 0.768]
A =
â€‹
â€‹
(0.7
0.2
0.1
0.6) B =
â€‹
(1
0) C =
â€‹
â€‹
(1
0) D = 0
K
â€‹ =
0
D = 0
K
â€‹ =
1
CB = 1
K
â€‹ =
2
CAB =
â€‹
â€‹
â€‹
=
(1
0) (0.7
0.2)
0.7
K
â€‹ =
3
CA B =
2
CA(AB) =
â€‹
â€‹
â€‹
=
(1
0) (0.51
0.26)
0.51
K = [0, 1, 0.7, 0.51]
u
â€‹ =
0
1 u
â€‹ =
k
0
k > 0
y
â€‹ =
0
K
â€‹0
y
â€‹ =
1
K
â€‹1
y
â€‹ =
2
K
â€‹2
K
k
A


--- Page 7 ---
â“ Question Interpretation
We analyze the parallelization potential and critical path length when computing the
output via convolution versus the naive recurrent method.
ğŸ§  Step-by-Step Solution
1. Using Convolution:
Each 
 is a dot product of length 
, which can be computed in parallel for
all .
Using tree reduction, each dot product has critical path length 
.
Overall critical path: 
2. Naive Recurrent Computation:
Compute 
 sequentially: 
Then compute 
 in parallel: 
Overall critical path: 
âœ… Self-Check
Convolution enables massive parallelization.
Critical path reduces from 
 to 
.
ğŸ”¹ Part (d): Efficient Kernel Computation
â“ Question Interpretation
We seek efficient methods to compute the kernel 
 for 
,
and how to parallelize this.
ğŸ§  Step-by-Step Solution
1. Direct Sequential Method:
Compute 
, 
, 
, ..., 
y
â€‹ =
k
â€‹ K
â€‹u
â€‹
âˆ‘â„“=0
L
â„“
kâˆ’â„“
y
â€‹k
L + 1
k
O(log L)
O(log L)
x
â€‹, x
â€‹, â€¦ , x
â€‹
1
2
L
O(L)
y
â€‹ =
k
Cx
â€‹ +
k
Du
â€‹k
O(1)
O(L)
O(L)
O(log L)
K
â€‹ =
â„“
CA
B
â„“âˆ’1
â„“= 1, â€¦ , L
v
â€‹ =
0
B v
â€‹ =
1
Av
â€‹0 v
â€‹ =
2
Av
â€‹1
v
â€‹ =
Lâˆ’1
Av
â€‹
Lâˆ’2
The model overlooks the 
computational complexity of 
matrix multiplication.


--- Page 8 ---
Then 
This takes 
 steps.
2. Parallel Strategy Using Doubling:
Compute powers of 
 up to 
 using repeated squaring.
Compute 
 recursively:
Critical path: 
Then compute 
 in parallel.
âœ… Self-Check
Doubling method reduces critical path to 
.
Requires 
 processors but is highly parallel.
ğŸ”¹ Part (e): Diagonal Matrix 
â“ Question Interpretation
We leverage the diagonal structure of 
 to compute the kernel more efficiently.
ğŸ§  Step-by-Step Solution
Let 
Then 
So:
Compute the geometric sequences 
 in parallel using doubling.
Then sum over  for each .
âœ… Self-Check
K
â€‹ =
â„“
Cv
â€‹
â„“âˆ’1
O(L)
A
L âˆ’1
V
â€‹ =
k
[v
â€‹, v
â€‹, â€¦ , v
â€‹]
0
1
2 âˆ’1
k
V
â€‹ =
k+1
[V
â€‹, A V
â€‹]
k
2k
k
O(log L)
K
â€‹ =
â„“
Cv
â€‹
â„“âˆ’1
O(log L)
O(L)
A
A
A = diag(Î»
â€‹, â€¦ , Î»
â€‹)
1
n
A
=
â„“âˆ’1
diag(Î»
â€‹, â€¦ , Î»
â€‹)
1
â„“âˆ’1
n
â„“âˆ’1
K
â€‹ =
â„“
CA
B =
â„“âˆ’1
â€‹C
â€‹B
â€‹Î»
â€‹
i=1
âˆ‘
n
i
i
i
â„“âˆ’1
Î»
â€‹
i
â„“âˆ’1
i
â„“


--- Page 9 ---
Avoids matrix multiplications.
Critical path: 
ğŸ”¹ Part (f): Diagonal-plus-low-rank (DPLR) Structure
â“ Question Interpretation
We exploit the structure 
 to compute the kernel efficiently.
ğŸ§  Step-by-Step Solution
1. Eigenvalue Decomposition:
 is symmetric with eigenvalues:
Let 
 be the orthogonal matrix diagonalizing 
.
2. Kernel Expression:
Let 
, 
Then:
where:
3. Final Formula:
and 
.
âœ… Self-Check
O(log L)
A = I
â€‹ +
n
ppâŠ¤
A
Î»
â€‹ =
1
1 + âˆ¥pâˆ¥2
Î»
â€‹ =
2
â‹¯= Î»
â€‹ =
n
1
U
A
C =
â€²
CU B =
â€²
U B
âŠ¤
K
â€‹ =
â„“
C Î›
B =
â€²
â„“âˆ’1
â€²
Î±Î²
+
â„“âˆ’1
S
Î² = 1 + âˆ¥pâˆ¥2
Î± =
âˆ¥pâˆ¥2
(Cp)(p B)
âŠ¤
S = CB âˆ’Î±
K
â€‹ =
â„“
CB + Î± Î²
âˆ’1
forÂ â„“â‰¥
(
â„“âˆ’1
)
1
K
â€‹ =
0
D
The model proposed a different 
approach than the answer, which 
was quite interesting.


--- Page 10 ---
Verified for 
.
Only scalar exponentials needed â†’ compute in 
 via doubling.
âœ… Overall Self-Check
All answers directly address the questions.
Mathematical derivations are explicit and logical.
Parallelization strategies are clearly explained.
Assumptions (e.g., 
 in examples) are stated.
This completes the solution to Problem 1. If you have any questions or need further
clarification, feel free to ask! ğŸ˜Š
â„“= 1, 2
O(log L)
D = 0
For part (c), did you consider the computation of matrix mult?, same 
for part(d), rethink it and double check that
ğŸ” Revised Analysis for Parts (c) and (d)
I'll re-examine parts (c) and (d) with special attention to matrix multiplication
complexities and parallelization strategies.
ğŸ”¹ Part (c): Efficient Computation with Convolutions
(Revised)
â“ Question Interpretation
Given the kernel 
, we want to compute the output sequence 
 for 
 via convolution:
K
{y
â€‹}
k
k =
0, â€¦ , L
y
â€‹ =
k
â€‹K
â€‹u
â€‹
â„“=0
âˆ‘
L
â„“
kâˆ’â„“
å·²æ€è€ƒï¼ˆç”¨æ—¶ 212 ç§’ï¼‰
I gave him a hint to let him 
correct it.


--- Page 11 ---
We analyze parallelization potential and critical path length, considering matrix-
vector multiplications.
ğŸ§  Step-by-Step Solution
1. Convolution Structure:
Each 
 is a sum of matrix-vector products: 
.
If 
 and 
, then 
 is 
.
The computation for each 
 is a dot product between the kernel and a shifted
input sequence.
2. Parallelization Strategy:
Option 1: Independent Dot Products: Compute each 
 in parallel. Each dot
product of length 
 can be done with tree reduction in 
 time.
Option 2: FFT-Based Convolution:
Convert to frequency domain using FFT (parallel FFT has 
 critical
path).
Perform element-wise multiplication (parallel in 
).
Inverse FFT (another 
 step).
Total critical path: 
.
3. Critical Path Analysis:
Using FFT: 
 (dominant term from FFT steps).
Naive recurrence:
Sequential state updates: 
 requires 
 matrix-vector
multiplications.
Each matrix-vector multiplication has cost 
 if 
 is 
, but the
critical path length is 
 due to sequential dependency.
4. Comparison:
Convolution method: Critical path = 
 (assuming FFT).
Naive recurrence: Critical path = 
.
âœ… Self-Check
FFT-based convolution is highly parallel and achieves 
 critical path.
y
â€‹k
K
â€‹u
â€‹
â„“
kâˆ’â„“
u
â€‹ âˆˆ
k
Rd
y
â€‹ âˆˆ
k
Rm
K
â€‹â„“
m Ã— d
y
â€‹k
y
â€‹k
L + 1
O(log L)
O(log L)
O(1)
O(log L)
O(log L)
O(log L)
x
â€‹ =
k+1
Ax
â€‹ +
k
Bu
â€‹k
O(L)
O(n )
2
A
n Ã— n
O(L)
O(log L)
O(L)
O(log L)


--- Page 12 ---
Matrix multiplications are handled efficiently in frequency domain.
The naive method's sequential dependency cannot be broken.
ğŸ”¹ Part (d): Efficient Kernel Computation (Revised)
â“ Question Interpretation
Compute 
 for 
 efficiently and in parallel, considering
matrix multiplication costs.
ğŸ§  Step-by-Step Solution
1. Direct Sequential Method:
Compute 
, 
, ..., 
.
Then 
.
Cost: 
 matrix multiplications, critical path 
.
2. Parallel Doubling Method:
Step 1: Compute Powers of 
:
Compute 
 using repeated squaring.
Each matrix multiplication: 
 time (where 
 for Strassen).
Critical path: 
 steps.
Step 2: Compute 
 via Binary Expansion:
For each , express 
 as product of powers from step 1.
Multiply these powers by 
 using tree reduction.
Critical path: 
 matrix multiplications.
Step 3: Multiply by 
:
Compute 
 in parallel for all .
Critical path: 
 if parallelized.
3. Optimized Doubling for 
:
Initialize 
.
For 
 to 
:
K
â€‹ =
â„“
CA
B
â„“âˆ’1
â„“= 1, â€¦ , L
V =
0
B V
â€‹ =
1
AV0
V
â€‹ =
Lâˆ’1
AV
â€‹
Lâˆ’2
K
â€‹ =
â„“
CV
â€‹
â„“âˆ’1
O(L)
O(L)
A
A , A , A , â€¦ , A
1
2
4
2N
O(n )
Ï‰
Ï‰ â‰ˆ2.8
O(log L)
A B
j
j
Aj
B
O(log L)
C
K
â€‹ =
â„“
C(A
B)
â„“âˆ’1
â„“
O(1)
A B
j
Y
â€‹ =
0
B
k = 0
N âˆ’1


--- Page 13 ---
In parallel for all 
 to 
:
This computes 
 for 
.
Critical path: 
 steps (each step involves one matrix multiplication).
4. Overall Critical Path:
Power computation: 
Doubling steps: 
Multiplication by 
: 
Total: 
âœ… Self-Check
The doubling method efficiently parallelizes the sequence 
.
Matrix multiplication costs are absorbed into the 
 critical path.
This method leverages parallel tree reductions and is optimal for 
.
ğŸ”¹ Final Answers for Parts (c) and (d)
(c) Efficient Computation with Convolutions
Parallelization: Full parallelization is possible using FFT-based convolution.
Critical Path: 
 using FFT.
Naive Recurrence: 
 due to sequential state updates.
(d) Efficient Kernel Computation
Strategy: Use doubling method to compute 
 in 
 steps.
Parallelization:
Precompute powers of 
 via repeated squaring.
Compute 
 using parallel assignments.
Multiply by 
 in parallel.
Critical Path: 
.
j = 0
2 âˆ’
k
1
Y
â€‹ =
j+2k
A Y
â€‹
2k
j
Y
â€‹ =
j
A B
j
j = 0, â€¦ , L âˆ’1
O(log L)
O(log L)
O(log L)
C O(1)
O(log L)
A B
j
O(log L)
L = 2N
O(log L)
O(L)
A B
j
O(log L)
A
A B
j
C
O(log L)
still missed


--- Page 14 ---
These revised answers explicitly account for matrix multiplication complexities and
confirm that logarithmic critical paths are achievable through careful parallelization.
think of the matrix mult, n*n @ n*n matrix, what's the critical path? 
Then rethink about c and d
ğŸ” Revised Analysis for Parts (c) and (d) with Matrix
Multiplication Considerations
I'll re-examine parts (c) and (d) with careful attention to matrix multiplication
complexities and parallelization strategies.
ğŸ”¹ Part (c): Efficient Computation with Convolutions
(Revised)
â“ Question Interpretation
Given the kernel 
, we want to compute the output sequence 
 for 
 via convolution:
where 
, 
, and 
 is 
. We analyze parallelization potential and
critical path length, considering matrix-vector multiplications.
ğŸ§  Step-by-Step Solution
1. Convolution Structure:
Each 
 requires computing 
 matrix-vector products 
 and
summing the results.
Matrix-vector product for fixed  and :
Each element of the result is a dot product of length .
With tree reduction, critical path is 
.
K
{y
â€‹}
k
k =
0, â€¦ , L
y
â€‹ =
k
â€‹K
â€‹u
â€‹
â„“=0
âˆ‘
L
â„“
kâˆ’â„“
u
â€‹ âˆˆ
k
Rd y
â€‹ âˆˆ
k
Rm
K
â€‹â„“
m Ã— d
y
â€‹k
L + 1
K
â€‹u
â€‹
â„“
kâˆ’â„“
â„“
k
d
O(log d)
å·²æ€è€ƒï¼ˆç”¨æ—¶ 169 ç§’ï¼‰
This time I gave him a 
more detailed hint.


--- Page 15 ---
Summing 
 vectors of size 
:
Tree reduction gives critical path 
.
Total critical path per 
: 
.
All 
 can be computed in parallel, so overall critical path remains 
.
2. FFT-Based Convolution:
Convert kernel and input to frequency domain using FFT:
Parallel FFT has critical path 
.
Pointwise multiplication in frequency domain:
Each pointwise product is 
 matrix-vector multiplication? Actually,
careful:
In frequency domain, the convolution becomes pointwise multiplication of
transforms.
But since 
 and 
 are sequences of matrices and vectors, we need to
consider multi-channel FFT.
Critical path for pointwise multiplication: 
 per frequency bin.
Inverse FFT: critical path 
.
Total critical path: 
.
3. Naive Recurrent Computation:
Compute states sequentially: 
.
Each state update: matrix-vector product 
 (critical path 
) and
vector addition.
Sequential dependency: 
 steps.
Then compute 
 in parallel: critical path 
.
Total critical path: 
.
âœ… Self-Check
Convolution method critical path: 
 vs. naive: 
.
Matrix-vector products are efficiently parallelized via tree reductions.
FFT method achieves same asymptotic critical path.
L + 1
m
O(log L)
y
â€‹k O(log d + log L) = O(log(dL))
y
â€‹k
O(log(dL))
O(log L)
m Ã— d
K
â€‹â„“
u
â€‹â„“
O(log d)
O(log L)
O(log L + log d) = O(log(dL))
x
â€‹ =
k+1
Ax
â€‹ +
k
Bu
â€‹k
Ax
â€‹k
O(log n)
O(L)
y
â€‹ =
k
Cx
â€‹ +
k
Du
â€‹k
O(log n)
O(L â‹…log n)
O(log(dL))
O(L log n)


--- Page 16 ---
ğŸ”¹ Part (d): Efficient Kernel Computation (Revised)
â“ Question Interpretation
Compute 
 for 
 efficiently and in parallel, considering
matrix multiplication costs. Assume 
.
ğŸ§  Step-by-Step Solution
1. Matrix Multiplication Critical Path:
Multiplying two 
 matrices:
Each element of the product is a dot product of length .
With tree reduction, critical path is 
.
So each matrix multiplication has critical path 
.
2. Parallel Doubling Method:
Step 1: Compute Powers of 
:
Compute 
 via repeated squaring.
Number of steps: 
.
Critical path per step: 
.
Total critical path: 
.
Step 2: Compute 
 for 
:
For each , express 
 as product of powers from Step 1.
Multiply these powers by 
 using tree reduction:
Each multiplication: 
 matrix with 
 matrix â†’ critical path
.
Tree depth: 
.
Critical path per : 
.
All  can be computed in parallel.
Step 3: Compute 
:
Multiply 
 ( 
 ) by 
 ( 
 ):
Critical path: 
 per .
All  can be computed in parallel.
K
â€‹ =
â„“
CA
B
â„“âˆ’1
â„“= 1, â€¦ , L
L = 2N
n Ã— n
n
O(log n)
O(log n)
A
A , A , A , â€¦ , A
1
2
4
2N
O(log L)
O(log n)
O(log L â‹…log n)
A B
j
j = 0, â€¦ , L âˆ’1
j
Aj
B
n Ã— n
n Ã— d
O(log n)
O(log L)
j O(log L â‹…log n)
j
K
â€‹ =
â„“
CA
B
â„“âˆ’1
C
m Ã— n
A
B
â„“âˆ’1
n Ã— d
O(log n)
â„“
â„“


--- Page 17 ---
Overall critical path: 
.
3. Alternative Parallel Prefix Method:
Use parallel prefix algorithm to compute 
 for 
:
Critical path: 
 steps, each step a matrix multiplication (
).
Total: 
.
Then compute 
 and 
 as above.
âœ… Self-Check
Critical path dominated by matrix multiplications in doubling steps.
Achieves 
 critical path, which is efficient for parallel computation.
Precomputation of powers allows reuse for multiple inputs.
ğŸ”¹ Final Answers for Parts (c) and (d)
(c) Efficient Computation with Convolutions
Parallelization: Full parallelization is possible using direct convolution or FFT.
Critical Path: 
 using either method.
Naive Recurrence: 
 due to sequential state updates.
Key Insight: Convolution breaks the sequential dependency, allowing logarithmic
critical path in 
.
(d) Efficient Kernel Computation
Strategy: Use doubling or parallel prefix to compute powers of 
, then multiply by
 and 
.
Critical Path: 
.
Parallelization:
Compute matrix powers via repeated squaring.
Compute 
 using tree reductions over binary expansions of .
Multiply by 
 in parallel.
O(log L â‹…log n)
Aj
j = 0, â€¦ , L âˆ’1
O(log L)
O(log n)
O(log L â‹…log n)
A B
j
CA B
j
O(log L â‹…log n)
O(log(dL))
O(L â‹…log n)
L
A
B
C
O(log L â‹…log n)
A B
j
j
C
Correct now


--- Page 18 ---
Key Insight: Matrix multiplication critical path 
 multiplies with 
steps from doubling.
These revised answers explicitly account for matrix multiplication complexities and
confirm that logarithmic critical paths are achievable through careful parallelization.
The convolution method significantly outperforms the naive recurrence for long
sequences.
O(log n)
O(log L)
image.png
PNG 162.18KB
image.png
PNG 191.88KB
Good job! Now let's do problem3. The requirements are the same as 
before. 3. Self-Supervised Linear Purification
 Consider a linear encoder â€” square weight matrix W âˆˆ RmÃ—m â€” 
that we want to be a â€œpurificationâ€
 operation on mâˆ’dimensional feature vectors from a particular 
problem domain. We do this by using self
supervised learning to reconstruct n points of training data X âˆˆ 
RmÃ—n by minimizing the loss:
 L1(W;X) = âˆ¥Xâˆ’WXâˆ¥2
 F
 (1)
 While the trivial solution W = I can minimize the reconstruction loss 
(1), we will now see how weight
decay (or equivalently in this case, ridge-style regularization) can 
help us achieve non-trivial purification.
 L2(W;X,Î») = âˆ¥Xâˆ’WXâˆ¥2
 F
 Reconstruction Loss
 +Î» âˆ¥Wâˆ¥2
 F
 Regularization Loss
 Note above that Î» controls the relative weighting of the two losses 
in the optimization.
 Homework 8, Â© Faculty teaching EECS 182, Fall 2025. 
AllRightsReserved. Thismaynotbepubliclysharedwithoutexplicit 
permission.
 (2)
 2
Homework8@2025-10-2508:44:19Z
 
(a)Considerthesimplifiedcaseform=2withthefollowingtwocandidatew
eightmatrices:


--- Page 19 ---
 W(Î±)= 1 0
 0 1 W(Î²)= 1 0
 0 0 (3)
 ThetrainingdatamatrixXisalsogiventoyouasfollows:
 X= âˆ’2.17 1.98 2.41 âˆ’2.03
 0.02 âˆ’0.01 0.01 âˆ’0.02 (4)
 i. 
Computethereconstructionlossandtheregularizationlossforthetwoen
coders,andfillin
 themissingentriesinthetablebelow.
 Encoder ReconstructionLoss RegularizationLoss Î± __________ 
__________
 Î² 0.001 __________
 ii. 
ForwhatvaluesoftheregularizationparameterÎ»istheidentitymatrixW(Î±)
gethigherloss
 L2in(2),ascomparedtoW(Î²)?
 
(b)NowconsideragenericsquarelinearencoderWâˆˆRmÃ—mandtheregul
arizedobjectiveL2reproduced
 belowforyourconvenience:
 L2(W;X,Î»)=âˆ¥Xâˆ’WXâˆ¥2
 F
 ReconstructionLoss
 +Î» âˆ¥Wâˆ¥2
 F
 RegularizationLoss
 AssumeÏƒ1>Â·Â·Â·>Ïƒmâ‰¥0arethemsingularvaluesinX, that 
thenumberoftrainingpointsnis
 
largerthanthenumberoffeaturesm,andthatXcanbeexpressedinSVDco
ordinatesasX=UÎ£VâŠ¤.
 i. Youaregiventhat 
theoptimizingweightmatrixfortheregularizedobjectiveL2abovetakesth
e
 followingform.Fillintheemptymatricesbelow.
 W=
     ï£°
 ï£¹
 ï£º ï£»Â·
               ï£°


--- Page 20 ---
 Ïƒ2
 1
 Ïƒ2
 1+Î»
 Ïƒ2
 2
 Ïƒ2
 2+Î»
 . . .
 Ïƒ2m
 Ïƒ2m+Î»
 ï£¹
 ï£º ï£º ï£º ï£º ï£º ï£º ï£»
 Â·
     ï£°
 ï£¹
 ï£º ï£» (5)
 ii. Derivetheaboveexpression.
 
(Hint:CanyouunderstandL2(W;X,Î»)asasumofmcompletelydecoupledri
dge-regression
 problems?)
 (Hint:TheFrobeniusnormisequal toâˆ¥Aâˆ¥2
 F :=tr(AAT),andit isinvariantunderorthogonal
 transform.Thatis,âˆ¥Aâˆ¥2
 F=âˆ¥UAVTâˆ¥2
 F foranyorthogonalmatricesU,V,andanyrectangular
 matrixA,aslongasU,A,Vhavecompatibleshapes.)
 
(c)YouaregiventhatthedatamatrixXâˆˆR8Ã—nhasthefollowingsingularval
ues:
 {Ïƒi}={10,8,4,1,0.5,0.36,0.16,0.01}
 
Homework8,Â©FacultyteachingEECS182,Fall2025.AllRightsReserved.
Thismaynotbepubliclysharedwithoutexplicitpermission. 3
Homework 8 @2025-10-25 08:44:19Z
 For what set of hyperparameter values Î» can we guarantee that the 
learned purifier W will
 preserve at least 80% of the feature directions corresponding to the 
first 3 singular vectors of
 X, while attenuating components in the remaining directions to at 
most 50% of their original


--- Page 21 ---
 strength?
 (Hint: What are the two critical singular values to focus on?) The 
picture attached also demonstrate the question , in case of you don't 
understand it, and you can have a double check for this question.
ğŸ“˜ Solution for Problem 3: Self-Supervised Linear Purification
ğŸ”¹ Part (a)
â“ Question Interpretation
We are given two candidate weight matrices 
 (identity) and 
 (partial
identity), and a data matrix 
. We need to compute the reconstruction loss 
 and regularization loss 
 for each, then determine for which  values
 has higher total loss than 
.
ğŸ§  Step-by-Step Solution
(i) Compute Losses
Given:
For 
:
Reconstruction loss: 
Regularization loss: 
 (since 
)
For 
:
Reconstruction error: 
Reconstruction loss: 
Regularization loss: 
W (Î±)
W (Î²)
X
âˆ¥X âˆ’
WXâˆ¥
â€‹
F
2
âˆ¥Wâˆ¥
â€‹
F
2
Î»
W (Î±)
W (Î²)
X =
â€‹
â€‹
â€‹
â€‹
[âˆ’2.17
0.02
1.98
âˆ’0.01
2.41
0.01
âˆ’2.03
âˆ’0.02]
W
=
(Î±)
I
âˆ¥X âˆ’IXâˆ¥
â€‹ =
F
2
âˆ¥0âˆ¥
â€‹ =
F
2
0
âˆ¥Iâˆ¥
=
F
2
2
1 +
2
0 +
2
0 +
2
1 =
2
2
W
=
(Î²)
â€‹
â€‹
[1
0
0
0]
W
X =
(Î²)
â€‹
â€‹
â€‹
â€‹
[âˆ’2.17
0
1.98
0
2.41
0
âˆ’2.03
0
]
X âˆ’W
X =
(Î²)
â€‹
â€‹
â€‹
â€‹
[ 0
0.02
0
âˆ’0.01
0
0.01
0
âˆ’0.02]
0.02 +
2
(âˆ’0.01) +
2
0.01 +
2
(âˆ’0.02) =
2
0.001
âˆ¥W
âˆ¥
â€‹ =
(Î²)
F
2
1 +
2
0 +
2
0 +
2
0 =
2
1
å·²æ€è€ƒï¼ˆç”¨æ—¶ 99 ç§’ï¼‰


--- Page 22 ---
Completed Table:
(ii) Compare Total Losses
Set 
:
âœ… Self-Check
Computations match given reconstruction loss for .
Inequality derivation is straightforward.
ğŸ”¹ Part (b)
â“ Question Interpretation
We derive the optimal 
 for the regularized loss 
 using SVD and orthogonal
invariance properties.
ğŸ§  Step-by-Step Solution
(i) Fill in Matrices
Given 
, the optimal weight matrix is:
(ii) Derivation
1. Orthogonal Invariance:
â€‹
â€‹
â€‹
â€‹
Encoder
Î±
Î²
ReconstructionÂ Loss
0
0.001
RegularizationÂ Loss
2
1
L
â€‹ =
2
(Î±)
0 + 2Î» = 2Î»
L
â€‹ =
2
(Î²)
0.001 + 1Î» = 0.001 + Î»
L
â€‹ >
2
(Î±)
L
â€‹
2
(Î²)
2Î» > 0.001 + Î» âŸ¹Î» > 0.001
Î²
W
L
â€‹2
X = UÎ£V âŠ¤
=
W^
U â‹…
â€‹
â€‹
â€‹
â€‹
â€‹ â‹…
â€‹
Ïƒ
â€‹+Î»
1
2
Ïƒ
â€‹1
2
â‹±
â€‹
Ïƒ
â€‹+Î»
m
2
Ïƒ
â€‹
m
2
U âŠ¤


--- Page 23 ---
2. Let 
:
Loss becomes: 
3. Decouple via Diagonal Structure:
Since 
 is diagonal, optimal 
 is diagonal: 
Each 
 minimizes: 
Derivative: 
Solution: 
4. Recover 
:
âœ… Self-Check
Derivation uses Frobenius norm invariance and decoupling.
Solution matches given form.
ğŸ”¹ Part (c)
â“ Question Interpretation
Given singular values, find  such that:
First 3 components are preserved â‰¥80%: 
Remaining components are attenuated â‰¤50%: 
ğŸ§  Step-by-Step Solution
Singular values: 
Conditions:
1. Preservation (i=1,2,3):
âˆ¥X âˆ’WXâˆ¥
â€‹ =
F
2
âˆ¥U (X âˆ’
âŠ¤
WX)V âˆ¥
â€‹ =
F
2
âˆ¥Î£ âˆ’U WUÎ£âˆ¥
â€‹
âŠ¤
F
2
âˆ¥Wâˆ¥
â€‹ =
F
2
âˆ¥U WUâˆ¥
â€‹
âŠ¤
F
2
A = U WU
âŠ¤
âˆ¥Î£ âˆ’AÎ£âˆ¥
â€‹ +
F
2
Î»âˆ¥Aâˆ¥
â€‹
F
2
Î£
A
A = diag(a
â€‹, â€¦ , a
â€‹)
1
m
a
â€‹i
Ïƒ
â€‹(1 âˆ’
i
2
a
â€‹) +
i 2
Î»a
â€‹i
2
âˆ’2Ïƒ
â€‹(1 âˆ’
i
2
a
â€‹) +
i
2Î»a
â€‹ =
i
0
a
â€‹ =
i
â€‹
Ïƒ
â€‹+Î»
i
2
Ïƒ
â€‹i
2
W
W = UAU âŠ¤
Î»
â€‹ â‰¥
Ïƒ
â€‹+Î»
i
2
Ïƒ
â€‹i
2
0.8
â€‹ â‰¤
Ïƒ
â€‹+Î»
i
2
Ïƒ
â€‹i
2
0.5
{10, 8, 4, 1, 0.5, 0.36, 0.16, 0.01}
The model's own checks are largely 
useless; they almost never detect errors.


--- Page 24 ---
Most restrictive: 
2. Attenuation (i=4,\dots,8):
Most restrictive: 
Combined: 
âœ… Self-Check
Boundaries checked: at 
, component 4 is exactly 50%; at 
,
component 3 is exactly 80%.
All conditions are satisfied within this range.
âœ… Overall Self-Check
All answers directly address the questions.
Computations are explicit and verified.
Derivation in (b) uses orthogonal invariance and decoupling correctly.
Range in (c) satisfies all constraints.
â€‹ â‰¥
Ïƒ
â€‹ + Î»
i
2
Ïƒ
â€‹i
2
0.8 âŸ¹Î» â‰¤
â€‹
4
Ïƒ
â€‹i
2
Ïƒ
â€‹ =
1
10 âŸ¹Î» â‰¤25
Ïƒ
â€‹ =
2
8 âŸ¹Î» â‰¤16
Ïƒ
â€‹ =
3
4 âŸ¹Î» â‰¤4
Î» â‰¤4
â€‹ â‰¤
Ïƒ
â€‹ + Î»
i
2
Ïƒ
â€‹i
2
0.5 âŸ¹Î» â‰¥Ïƒ
â€‹i
2
Ïƒ
â€‹ =
4
1 âŸ¹Î» â‰¥1
Ïƒ
â€‹ =
5
0.5 âŸ¹Î» â‰¥0.25
Ïƒ
â€‹ =
6
0.36 âŸ¹Î» â‰¥0.1296
Ïƒ
â€‹ =
7
0.16 âŸ¹Î» â‰¥0.0256
Ïƒ
â€‹ =
8
0.01 âŸ¹Î» â‰¥0.0001
Î» â‰¥1
1 â‰¤Î» â‰¤4
Î» = 1
Î» = 4
Correct!


--- Page 25 ---
This completes the solution to Problem 3. If you have any questions or need further
clarification, feel free to ask! ğŸ˜Š
image.png
PNG 237.15KB
image.png
PNG 273.11KB
Very good! Now let's do problem 4:  4. Ridge-Attention
 In lecture, you saw how the standard softmax-attention mechanism 
can be viewed as a softened version
 of something akin to a nearest-neighbor model in which the value 
returned for a query reflects a weighted
 combination of the values that correspond to the keys closest to the 
query. In this view, the (key, value) pairs
 in the memory represent a kind of in-context â€œtraining dataâ€ and the 
query is a test input for which we want
 to predict the right output given that data.
 (a) To start, letâ€™s think about why it is possible to efficiently update 
simple averaging. Let m = 1
 n 
n
 i=1 xi
 be the average of n points. Use m,xn+1,n and simple arithmetic 
operations to compute mâ€² =
 1
 n+1
 n+1
 i=1 xi â€”the average of all points including the new point xn+1.
 (HINT: Start by multiplying m by n.)
 (b) Let us now shift to thinking about traditional ridge-regression 
with n training pairs (xi,yi) where xi
 are d-dimensional vectors and yi are scalars. Let the matrix A =
 we can find the familiar closed-form solution
 wâˆ— =(AâŠ¤A+Î»I)âˆ’1AâŠ¤y
   ï£¯
 ï£¯
 ï£¯
 ï£¯
 ï£°
 ï£¹
 ï£º
 ï£º
 xâŠ¤


--- Page 26 ---
 1
 xâŠ¤
 2
 .
 .
 .
 xâŠ¤n
 ï£º
 ï£º
 ï£»
 that allows us to make scalar predictions on any new test input x by 
computing wâŠ¤âˆ—x.
 First, write the two terms (AâŠ¤A + Î»I) and AâŠ¤y as sums involving the 
xi and yi.
 i.e. Complete:
 (AâŠ¤A+Î»I) =Î»I + n
 i=1
 AâŠ¤y = n
 i=1
   ï£¯
 ï£¯
 and vector y =
 ï£¯
 ï£¯
 ï£°
 ï£¹
 ï£º
 ï£º
 y1
 y2
 .
 .
 .
 yn
 ï£º
 ï£º
 ï£»
 so that
 (6)
 (c) Suppose we wanted to do ridge-self-attention (non-causalâ€“ 
â€œencoder-styleâ€) with a context length of
 n and d-dimensional query, key, and value vectors. Recall that this 


--- Page 27 ---
is the style of attention where
 each of the n queries is applied to the same pool of n (key, value) 
pairs. The goal is to calculate
 multi-dimensional ridge-regression predictions, after â€œtrainingâ€ on 
the pool of (key, value) pairs, and
 given the query as a kind of â€œtestâ€ input. (Note: the keys are playing 
the role of the A matrix in
 ridge-regression, each query is like the x we are testing on, and in 
place of the scalar yi, we have an
 entire value vector for a multi-dimensional ridge-regression 
problem so the y vector is replaced with a
 matrix that has a row for each value vector.)
 Assume that the cost of inverting a dÃ—d matrix is O(d3) and the cost 
of multiplying two such matrices
 is also O(d3). Assume that a dÃ—d matrix times either a d-
dimensional row or column vector costs d2
 operations. You should assume d < n.
 What is the computational cost of a non-causal ridge self-attention 
layer?
 âƒ O(d4)
 âƒ O(nd2)
 Homework 8, Â© Faculty teaching EECS 182, Fall 2025. 
AllRightsReserved. Thismaynotbepubliclysharedwithoutexplicit 
permission.
 4
Homework 8 @2025-10-25 08:44:19Z
 âƒ O(n2d3)
 âƒ O(n2d2)
 âƒ O(n2)
 âƒ O(1)
 (HINT: Do not forget that for a single d-dimensional query vector q, 
attention needs to return a d
dimensional result.)
 (d) Assumethat a ridge self-attention layer is used in a Transformer 
architecture and there is a downstream
 loss. For which of these will backprop successfully pass gradients if 
we use ridge self-attention?
 The ridge Î» viewed as a learnable parameter for the self-attention 
layer.
 The keys
 The values
 The queries
 (e) Nowstep back. There is a nice trick (called the Shermanâ€“


--- Page 28 ---
Morrison Formula) by which one can update
 the inverse of an invertible matrix to which you make a rank-1 
update. Let M be an invertible square
 d Ã—dmatrix and let u,v be two d-dimensional vectors. Then:
 (M +uvâŠ¤)âˆ’1 =Mâˆ’1âˆ’
 1
 1 +vâŠ¤Mâˆ’1u(Mâˆ’1u)(vâŠ¤Mâˆ’1)
 (7)
 Assume that a d Ã— d matrix times either a d-dimensional row or 
column vector costs d2 operations,
 and so does the evaluation of a dyad uvâŠ¤. Assume that computing a 
Euclidean inner-product costs d
 operations. Assuming that you already had Mâˆ’1 in hand, what is the 
computational cost of one
 application of (7)?
 âƒ O(d4)
 âƒ O(d3)
 âƒ O(d2)
 âƒ O(d)
 âƒ O(1)
 (f) Consider implementing causal ridge-self-attention with a context 
length of n but where the pool of
 (key, value) vectors that one is querying at position t consists only 
of the t (key, value) pairs so far.
 Describe explicitly how you would compute causal ridge-self-
attention in a computationally effi
cient manner. Leverage your decomposition of the ridge-regression 
formula in part (b) of this problem
 together with the Sherman-Morrison formula from (7) to avoid 
having to do O(n2) computations while
 still calculating causal ridge-self-attention outputs correctly for all n 
positions in the context.
 (HINT: Think recursively. What do you need to track from one time 
step to the next to avoid repeating
 work? )
 (g) Many people consider important the ability to visualize the 
attention weights. For traditional softmax
attention, these are the outputs of the softmax for a given query 
vector. They tell you the exact amounts
 by which the attention outputs at this position are linear 
combinations of the values being fed in at this
 and other positions.
 For ridge-attention and a given query vector, how would you 


--- Page 29 ---
compute the (possibly negative)
 weights associated to each of the value vectors in the context? The 
picture attached also demonstrate the question , in case of you don't 
understand it, and you can have a double check for this question.
ğŸ“˜ Solution for Problem 4: Ridge-Attention
ğŸ”¹ Part (a)
â“ Question Interpretation
We need to update the average 
 of  points to include a new point 
, using
only 
, 
, and .
ğŸ§  Step-by-Step Solution
The current average is 
, so 
.
Adding 
, the new sum is 
.
The new average is:
âœ… Self-Check
For 
, 
, then 
, which is correct.
ğŸ”¹ Part (b)
â“ Question Interpretation
We express the ridge regression terms 
 and 
 as sums over the data
points.
ğŸ§  Step-by-Step Solution
m
n
x
â€‹
n+1
m x
â€‹
n+1
n
m =
â€‹
â€‹ x
â€‹
n
1 âˆ‘i=1
n
i
â€‹ x
â€‹ =
âˆ‘i=1
n
i
n â‹…m
x
â€‹
n+1
n â‹…m + x
â€‹
n+1
m =
â€²
â€‹
n + 1
n â‹…m + x
â€‹
n+1
n = 1 m = x
â€‹1
m =
â€²
â€‹
2
x
â€‹+x
â€‹
1
2
A A +
âŠ¤
Î»I
A y
âŠ¤
å·²æ€è€ƒï¼ˆç”¨æ—¶ 219 ç§’ï¼‰
The interpretation shows its strategy.


--- Page 30 ---
Thus:
âœ… Self-Check
Dimensions match: 
 is 
, and 
 is 
.
ğŸ”¹ Part (c)
â“ Question Interpretation
We compute the computational cost of non-causal ridge self-attention with 
queries and -dimensional keys/values.
ğŸ§  Step-by-Step Solution
Precompute 
: cost 
.
Invert 
: cost 
.
Compute 
: cost 
.
Compute 
: cost 
.
For all queries 
, compute output 
: cost 
.
Total cost: 
. Since 
, this simplifies to 
.
âœ… Self-Check
The dominant term is 
, which matches the option 
.
A A =
âŠ¤
â€‹ x
â€‹x
â€‹
âˆ‘i=1
n
i
i
âŠ¤
A y =
âŠ¤
â€‹ x
â€‹y
â€‹
âˆ‘i=1
n
i i
(A A +
âŠ¤
Î»I) = Î»I +
â€‹x
â€‹x
â€‹
i=1
âˆ‘
n
i
i
âŠ¤
A y =
âŠ¤
â€‹x
â€‹y
â€‹
i=1
âˆ‘
n
i i
x
â€‹x
â€‹
i
i
âŠ¤
d Ã— d
x
â€‹y
â€‹
i i
d Ã— 1
n
d
M = K K +
âŠ¤
Î»I
O(nd )
2
M
O(d )
3
V
K
âŠ¤
O(nd )
2
(V
K)M
âŠ¤
âˆ’1
O(d )
3
Q
Q(V
KM
)
âŠ¤
âˆ’1
O(nd )
2
O(nd +
2
d )
3
d < n
O(nd )
2
O(nd )
2
O(nd )
2


--- Page 31 ---
ğŸ”¹ Part (d)
â“ Question Interpretation
We determine which components can receive gradients during backpropagation
through ridge self-attention.
ğŸ§  Step-by-Step Solution
The solution 
 is differentiable with respect to , 
, 
, and 
.
Gradients can be computed for all parameters via matrix calculus.
Answer: All of them ( , keys, values, queries) can receive gradients.
âœ… Self-Check
The matrix inverse and products are differentiable operations.
ğŸ”¹ Part (e)
â“ Question Interpretation
We compute the cost of applying the Sherman-Morrison formula to update a matrix
inverse.
ğŸ§  Step-by-Step Solution
Compute 
: cost 
.
Compute 
: cost 
.
Compute inner product 
: cost 
.
Compute dyad 
: cost 
.
Scalar division and subtraction: cost 
.
Total cost: 
.
W = (K K +
âŠ¤
Î»I)
K V
âˆ’1
âŠ¤
Î» K V
Q
Î»
M
u
âˆ’1
O(d )
2
v M
âŠ¤
âˆ’1
O(d )
2
v M
u
âŠ¤
âˆ’1
O(d)
(M
u)(v M
)
âˆ’1
âŠ¤
âˆ’1
O(d )
2
O(1)
O(d )
2


--- Page 32 ---
âœ… Self-Check
All steps are 
 or less, so the overall cost is 
.
ğŸ”¹ Part (f)
â“ Question Interpretation
We design an efficient algorithm for causal ridge self-attention using recursive
updates.
ğŸ§  Step-by-Step Solution
Initialize 
, 
.
For 
 to :
Get new key 
 and value 
.
Update 
 from 
 using Sherman-Morrison with 
: cost 
.
Update 
: cost 
.
Compute output 
: cost 
.
Total cost: 
.
âœ… Self-Check
Each step avoids recomputing from scratch, leveraging recursive updates.
ğŸ”¹ Part (g)
â“ Question Interpretation
We compute the attention weights for ridge-attention, which may be negative.
ğŸ§  Step-by-Step Solution
O(d )
2
O(d )
2
M
â€‹ =
0
âˆ’1
Î»
I
âˆ’1
N
â€‹ =
0
0
t = 1
n
k
â€‹t
v
â€‹t
M
â€‹
t
âˆ’1
M
â€‹
tâˆ’1
âˆ’1
u = v = k
â€‹t
O(d )
2
N
â€‹ =
t
N
â€‹ +
tâˆ’1
k
â€‹v
â€‹
t t
âŠ¤
O(d )
2
o
â€‹ =
t
M
â€‹(N
â€‹q
â€‹)
t
âˆ’1
t t
O(d )
2
O(nd )
2
Mostly correct, but there are some 
misunderstandings of the question.


--- Page 33 ---
The output for a query  is 
.
The weights  such that 
 are given by:
Compute 
 (cost 
 if precomputed), then 
 (cost
).
âœ… Self-Check
The weights  are linear in  and can be negative due to the ridge regression
formulation.
âœ… Overall Self-Check
All answers directly address the questions.
Computational costs are justified step by step.
The recursive algorithm in (f) efficiently updates the inverse.
The weight computation in (g) aligns with ridge regression duality.
This completes the solution to Problem 4. If you have any questions or need further
clarification, feel free to ask! ğŸ˜Š
q
o = Wq = (K K +
âŠ¤
Î»I)
K V q
âˆ’1
âŠ¤
Î±
o =
â€‹ Î±
â€‹v
â€‹
âˆ‘i=1
n
i i
Î± = K(K K +
âŠ¤
Î»I)
q
âˆ’1
b = (K K +
âŠ¤
Î»I)
q
âˆ’1
O(d )
2
Î± = Kb
O(nd)
Î±
q
for part g, can you write the weight for v_i explicitely?
ğŸ”¹ Part (g): Computing Attention Weights for Ridge-Attention
â“ Question Interpretation
For ridge-attention, we need to compute the (possibly negative) weights that
determine how much each value vector contributes to the output for a given query
vector .q
å·²æ€è€ƒï¼ˆç”¨æ—¶ 190 ç§’ï¼‰


--- Page 34 ---
ğŸ§  Step-by-Step Solution
In ridge-attention, the output for a query  is given by:
where:
 is the 
 key matrix (each row is a key vector 
)
 is the 
 value matrix (each row is a value vector 
)
 is the 
 query vector
 is the ridge parameter
We want to express  as a linear combination of the value vectors:
where 
 are the attention weights (which can be negative).
Using the dual form of ridge regression, the weights  can be computed as:
Then the output is:
which explicitly shows  as a weighted sum of the columns of 
 (i.e., the value
vectors 
).
Explicit formula for each weight:
where 
 is the weight for value vector 
.
âœ… Self-Check
The derivation uses the dual form of ridge regression, which is valid for multi-
output regression.
q
o = (K K +
âŠ¤
Î»I)
K V q
âˆ’1
âŠ¤
K
n Ã— d
k
â€‹i
V
n Ã— d
â€‹v
v
â€‹i
q
d Ã— 1
Î»
o
o =
â€‹Î±
â€‹v
â€‹
i=1
âˆ‘
n
i i
Î±i
Î±
Î± = (KK
+
âŠ¤
Î»I)
Kq
âˆ’1
o = V
Î±
âŠ¤
o
V âŠ¤
v
â€‹i
Î±
â€‹ =
i
(KK
+ Î»I)
Kq
â€‹
[
âŠ¤
âˆ’1
]i
Î±i
v
â€‹i
I gave him a hint, but he still 
couldn't find the problem.


--- Page 35 ---
The weights 
 can be negative because 
 is positive definite but
 can have negative components.
This matches the intuition from kernel ridge regression where the solution is a
Î±
â€‹i
(KK
+
âŠ¤
Î»I)âˆ’1
Kq
