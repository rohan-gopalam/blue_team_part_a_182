--- Page 1 ---
1/25
Vector calculus derivatives
Vector Calculus Review
Let x, c âˆˆ R
n
and A âˆˆ R
nÃ—n
. For the following parts, before taking any derivatives, 
identify what the
derivative looks like (is it a scalar, vector, or matrix?) and 
how we calculate each term in the derivative.
Then carefully solve for an arbitrary entry of the derivative, 
then stack/arrange all of them to get the final
result. Note that the convention we will use going forward 
is that vector derivatives of a scalar (with respect
to a column vector) are expressed as a row vector, i.e. âˆ‚f
âˆ‚x = [ âˆ‚f
âˆ‚x1
,
âˆ‚f
âˆ‚x2
, ...,
âˆ‚f
âˆ‚xn
] since a row acting on a
column gives a scalar. You may have seen alternative 
conventions before, but the important thing is that you
need to understand the types of objects and how they 
map to the shapes of the multidimensional arrays we
use to represent those types.
Homework 0, Â© Faculty teaching EECS 182, Fall 2025. All 
Rights Reserved. This may not be publicly shared without 
explicit permission. 1
Homework 0 @ 2025-08-29 14:33:01-07:00
(a) Show âˆ‚
âˆ‚x
(x
T c) = c
T
(b) Show âˆ‚
âˆ‚x
||x||2
2 = 2x
T
(c) Show âˆ‚
âˆ‚x
(Ax) = A
(d) Show âˆ‚
âˆ‚x
(x
T Ax) = x
T
(A + AT
)
(e) Under what condition is the previous derivative equal 
to 2x
T A?
Let's go through each part of this vector calculus review step by step, identifying the
shape (scalar/vector/matrix), calculating the derivative component-wise, and then
assembling the final result as a row vector (as per the given convention).
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 2 ---
2/25
(a) Show 
Type Analysis:
, 
 is a scalar.
Derivative of scalar w.r.t. column vector â†’ row vector 
Component-wise:
Let 
, then:
âœ… Final answer:
(b) Show 
Type Analysis:
, a scalar.
Derivative w.r.t. column vector â†’ row vector 
Component-wise:
Let 
, then:
âœ… Final answer:
(c) Show 
Type Analysis:
, 
 is a vector 
Derivative of vector w.r.t. vector â†’ matrix 
Component-wise:
Let 
, so 
, then:
âœ… Final answer:
(d) Show 
Type Analysis:
 is a scalar.
Derivative w.r.t. column vector â†’ row vector 
â€‹(x c) =
âˆ‚x
âˆ‚
T
cT
x âˆˆRn c âˆˆRn
x c
T
âˆˆR1Ã—n
f(x) =
â€‹ x
â€‹c
â€‹
âˆ‘i=1
n
i i
â€‹ =
âˆ‚x
â€‹i
âˆ‚f
c
â€‹ â‡’
i
â€‹ =
âˆ‚x
âˆ‚f
[c
â€‹, c
â€‹, â€¦ , c
â€‹] =
1
2
n
cT
â€‹
â€‹(x c) = c
âˆ‚x
âˆ‚
T
T
â€‹âˆ¥xâˆ¥
â€‹ =
âˆ‚x
âˆ‚
2
2
2xT
âˆ¥xâˆ¥
â€‹ =
2
2
x x
T
âˆˆR1Ã—n
f(x) =
â€‹ x
â€‹
âˆ‘i=1
n
i
2
â€‹ =
âˆ‚x
â€‹i
âˆ‚f
2x
â€‹ â‡’
i
â€‹ =
âˆ‚x
âˆ‚f
[2x
â€‹, 2x
â€‹, â€¦ , 2x
â€‹] =
1
2
n
2xT
â€‹
â€‹âˆ¥xâˆ¥
â€‹ = 2x
âˆ‚x
âˆ‚
2
2
T
â€‹(Ax) =
âˆ‚x
âˆ‚
A
A âˆˆRnÃ—n x âˆˆRn
Ax
âˆˆRn
âˆˆRnÃ—n
y = Ax
y
â€‹ =
i
â€‹ A
â€‹x
â€‹
âˆ‘j
ij
j
â€‹ =
âˆ‚x
â€‹j
âˆ‚y
â€‹i
A
â€‹ â‡’
ij
â€‹ =
âˆ‚x
âˆ‚(Ax)
A
â€‹
â€‹(Ax) = A
âˆ‚x
âˆ‚
â€‹(x Ax) =
âˆ‚x
âˆ‚
T
x (A +
T
A )
T
x Ax
T
âˆˆR1Ã—n
12/6/25, 11:12 PM
Vector calculus derivatives
Seems able to one shot the problem


--- Page 3 ---
3/25
Approach:
Let 
. Then using the product rule (or chain rule):
Alternatively, use index notation:
âœ… Final answer:
(e) Under what condition is the previous derivative equal to 
?
From (d), we have:
We want:
For this to equal 
, we must have:
âœ… Final answer:
Let me know if you want the matrix calculus rules summarized in a cheat sheet.
f(x) = x Ax
T
â€‹(x Ax) =
âˆ‚x
âˆ‚
T
â€‹x Ax
=
(âˆ‚x
âˆ‚
T
)
T
â€‹(x Ax)
=
(âˆ‚x
âˆ‚
T
)
T
(A + A )x â‡’
T
â€‹(x Ax)
=
[âˆ‚x
âˆ‚
T
]
x (A +
T
A )
T
x Ax =
T
â€‹x
â€‹A
â€‹x
â€‹ â‡’
i,j
âˆ‘
i
ij
j
â€‹ =
âˆ‚x
â€‹k
âˆ‚
â€‹A
â€‹x
â€‹ +
j
âˆ‘
kj
j
â€‹x
â€‹A
â€‹ =
i
âˆ‘
i
ik
[Ax]
â€‹ +
k
[A x]
â€‹ â‡’
T
k
â€‹ =
âˆ‚x
âˆ‚
(A + A )x â‡’
T
transpose â‡’
â€‹
â€‹(x Ax) = x (A + A )
âˆ‚x
âˆ‚
T
T
T
2x A
T
â€‹(x Ax) =
âˆ‚x
âˆ‚
T
x (A +
T
A )
T
x (A +
T
A ) =
T
2x A â‡’
T
x (A +
T
A ) =
T
x A +
T
x A
=
T
T
x A +
T
(Ax)T
2x A
T
x A
=
T
T
x A â‡’
T
x A
=
T
T
x A â‡’
T
(A âˆ’
T
A) x =
T
0
âˆ€x â‡’A
=
T
A
â€‹
WhenÂ AÂ isÂ symmetricÂ (i.e.,Â A
= A),Â thenÂ 
â€‹(x Ax) = 2x A
T
âˆ‚x
âˆ‚
T
T
Great! These are amazing and one shot. Now solve the 
SVD problem: 3. Least Squares and the Min-norm problem 
from the Perspective of SVD
Consider the equation Xw = y, where X âˆˆ R
mÃ—n
is a non-square data matrix, w is a weight vector, and y
is vector of labels corresponding to the datapoints in each 
row of X.
Letâ€™s say that X = UÎ£V
T
is the (full) SVD of X. Here, U and V are orthonormal 
square matrices, and Î£
is an m Ã— n matrix with non-zero singular values (Ïƒi) on 
the "diagonal".
For this problem, we define Î£
â€ 
an n Ã— m matrix with the reciprocals of the singular values 
( 1
Ïƒi
) along the
"diagonal".
(a) First, consider the case where m > n, i.e. our data matrix 
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 4 ---
4/25
X has more rows than columns (tall matrix)
and the system is overdetermined. How do we find the 
weights w that minimizes the error between
Xw and y? In other words, we want to solve minw âˆ¥Xw âˆ’ yâˆ¥
2
.
(b) Plug in the SVD X = UÎ£V
T and simplify. Be careful with dimensions!
(c) Youâ€™ll notice that the least-squares solution is in the 
form wâˆ— = Ay. What happens if we leftmultiply X by our 
matrix A? This is why the matrix A of the least-squares 
solution is called the
left-inverse.
(d) Now, letâ€™s consider the case where m < n, i.e. the data 
matrix X has more columns than rows and
the system is underdetermined. There exist infinitely many 
solutions for w, but we seek the minimumnorm solution, 
ie. we want to solve min âˆ¥wâˆ¥
2
s.t.Xw = y. What is the minimum norm solution?
(e) Plug in the SVD X = UÎ£V
T and simplify. Be careful with dimensions!
(f) Youâ€™ll notice that the min-norm solution is in the form 
wâˆ— = By. What happens if we right-multiply
X by our matrix B? This is why the matrix B of the min-
norm solution is called the right-inverse.
Thanks! Let's solve the SVD Least Squares and Min-norm problem step by step using the
tools of linear algebra and matrix calculus. We'll work through parts (a)â€“(f) with clear
reasoning and dimension checks.
ðŸŸ© Setup:
We are given:
Full SVD: 
, orthonormal
, orthonormal
, diagonal "matrix" with singular values 
 (where 
)
Let:
 be the Mooreâ€“Penrose pseudo-inverse of 
ðŸŸ¦ (a) Tall matrix (overdetermined): Solve
This is the classic least squares problem. When 
 with 
, and 
 has full
column rank:
The closed-form solution is:
This minimizes the squared error between the predictions 
 and the true labels 
.
X âˆˆRmÃ—n
X = UÎ£V T
U âˆˆRmÃ—m
V âˆˆRnÃ—n
Î£ âˆˆRmÃ—n
Ïƒ
â€‹, â€¦ , Ïƒ
â€‹
1
r
r =
rank(X)
Î£ âˆˆ
â€ 
RnÃ—m
Î£
â€‹âˆ¥Xw âˆ’
w
min
yâˆ¥
â€‹2
2
X âˆˆRmÃ—n
m > n
X
â€‹
w = (X X)
X y
âˆ—
T
âˆ’1
T
Xw
y âˆˆ
Rm
12/6/25, 11:12 PM
Vector calculus derivatives
This solution is right


--- Page 5 ---
5/25
ðŸŸ¦ (b) Plug in the SVD: 
Substitute into the least squares formula:
Since 
 is orthonormal: 
Use identity: 
 if 
 is orthonormal
But 
 (by definition of pseudo-inverse)
This is the compact and stable SVD-based expression for the least-squares solution.
ðŸŸ¦ (c) Define 
. What is 
?
We are told 
. Compute:
Now, since 
 (identity on the column space of 
), we get:
Note:
If 
 is full rank and 
, then 
More generally, 
 is the orthogonal projector onto the column space of 
âœ… Thatâ€™s why 
 is called the left-inverse of 
:
ðŸŸ© (d) Now the underdetermined case: 
We're solving:
There are infinitely many solutions (since there are more unknowns than equations).
Among all solutions 
 that satisfy 
, we want the one with minimum norm.
âœ… The solution is:
This minimizes 
 while still solving the constraint.
ðŸŸ© (e) Plug in SVD: 
Start with:
Since 
:
X = UÎ£V T
w =
âˆ—
(X X)
X y =
T
âˆ’1
T
(V Î£ U UÎ£V )
(V Î£ U )y
T
T
T âˆ’1
T
T
U
U U =
T
I
â€‹
m
= (V Î£ Î£V )
V Î£ U y
T
T âˆ’1
T
T
(V AV )
=
T âˆ’1
V (A
)V
âˆ’1
T
V
= V (Î£ Î£)
V V Î£ U y =
T
âˆ’1
T
T
T
V (Î£ Î£)
Î£ U y
T
âˆ’1
T
T
(Î£ Î£)
Î£
=
T
âˆ’1
T
Î£â€ 
â€‹
w = V Î£ U y
âˆ—
â€ 
T
A = V Î£ U
â€ 
T
AX
w =
âˆ—
Ay
AX = V Î£ U (UÎ£V ) =
â€ 
T
T
V Î£
Î£V
=
â€ U U
T
T
V (Î£ Î£)V
â€ 
T
Î£ Î£ =
â€ 
I
â€‹n
X
AX = V IV
=
T
V V T
X
m > n
V V
=
T
I
â€‹n
AX
X
A
X
â€‹
AX = I
â€‹
(onÂ theÂ rangeÂ ofÂ X)
n
m < n
â€‹âˆ¥wâˆ¥
â€‹
s.t.
Xw =
w
min
2
2
y
w
Xw = y
â€‹
w = X (XX )
y
âˆ—
T
T âˆ’1
âˆ¥wâˆ¥
â€‹2
X = UÎ£V T
w =
âˆ—
X (XX )
y =
T
T âˆ’1
(V Î£ U )(UÎ£V )(UÎ£V ) )
y =
T
T
T
T T âˆ’1
V Î£ U (UÎ£Î£ U )
y
T
T
T
T âˆ’1
U U =
T
I
= V Î£ (Î£Î£ )
U y
T
T âˆ’1
T
12/6/25, 11:12 PM
Vector calculus derivatives
Generally correct, added additional y
term


--- Page 6 ---
6/25
Now notice:
So:
âœ¨ Same expression as in the least squares case! The SVD gives us a unified framework to
express both solutions.
ðŸŸ© (f) Let 
. What is 
?
Recall 
, so:
Here, 
, and acts as a projector onto the row space of 
.
So:
This is the projection matrix onto the row space of 
.
âœ… Thatâ€™s why 
 is the right-inverse:
âœ… Summary Table
Case
Objective
Solution
Expression via SVD
Result of Applying
Inverse
Overdetermined (
)
Minimize 
 (left-
inverse)
Underdetermined (
)
Minimize 
 s.t.
 (right-
inverse)
Let me know if you'd like a diagram or cheat sheet for this too!
Î£ (Î£Î£ )
=
T
T âˆ’1
Î£â€ 
â€‹
w = V Î£ U y
âˆ—
â€ 
T
B = V Î£ U
â€ 
T
XB
X = UÎ£V T
XB = (UÎ£V )(V Î£ U ) =
T
â€ 
T
UÎ£
Î£ U
=
V V
T
â€ 
T
U(Î£Î£ )U
â€ 
T
Î£Î£ âˆˆ
â€ 
RmÃ—m
X
â€‹
XB = UU T
X
B
â€‹
XB = I
â€‹
(onÂ theÂ rowÂ spaceÂ ofÂ X)
m
m > n
âˆ¥Xw âˆ’
yâˆ¥
w =
âˆ—
(X X)
X y
T
âˆ’1
T
w =
âˆ—
V Î£ U y
â€ 
T
AX = V V T
m < n
âˆ¥wâˆ¥
Xw = y
w =
âˆ—
X (XX )
y
T
T âˆ’1
w =
âˆ—
V Î£ U y
â€ 
T
XB = UU T
it says simplify though, why are you writing some of the 
solutions in terms of y?
Great observation â€” you're absolutely right to call that out. Let's clarify the difference
between expressing solutions in terms of y  vs. simplifying purely symbolically.
In parts (b) and (e), the problem says â€œPlug in the SVD and simplifyâ€, and the goal is to
rewrite the solution in terms of 
 â€” not in terms of . Writing the final answer as
 is already a simplification of expressions like 
 using the
SVD.
However, your point is: can we go further and simplify without mentioning y  at all? So
letâ€™s go back and re-express (b) and (e) purely in terms of matrices â€” no y , just
simplified matrix forms.
ðŸ” Revised (b): Overdetermined Case â€” Simplify SVD
U, Î£, V
y
w =
âˆ—
V Î£ U y
â€ 
T
(X X)
X y
T
âˆ’1
T
12/6/25, 11:12 PM
Vector calculus derivatives
Simplify X, but it gave a y term
These 


--- Page 7 ---
7/25
Original:
Plug in SVD:
Let 
So:
Also:
So the whole expression becomes:
Now notice:
Thus:
âœ… Fully simplified: the matrix-only form of the least squares solution operator is:
No y  mentioned â€” just the simplification of the inverse-expression using the SVD.
ðŸ” Revised (e): Underdetermined Case â€” Simplify SVD
Original:
Again, plug in 
Then:
So:
Also:
Now:
Recognizing again that:
We get:
âœ… So the simplified matrix expression for the minimum-norm operator is again:
w =
âˆ—
(X X)
X y
T
âˆ’1
T
X = UÎ£V T
X X =
T
V Î£ U UÎ£V
=
T
T
T
V Î£ Î£V
â‡’
T
T
(X X)
=
T
âˆ’1
V (Î£ Î£)
V
T
âˆ’1
T
X
=
T
V Î£ U
T
T
w =
âˆ—
(X X)
X
=
T
âˆ’1
T
V (Î£ Î£)
V V Î£ U
=
T
âˆ’1
T
T
T
V (Î£ Î£)
Î£ U
T
âˆ’1
T
T
(Î£ Î£)
Î£
=
T
âˆ’1
T
Î£
(byÂ def)
â€ 
â€‹
(X X)
X
= V Î£ U
T
âˆ’1
T
â€ 
T
â€‹
A = V Î£ U
â€ 
T
w =
âˆ—
X (XX )
y
T
T âˆ’1
X = UÎ£V T
XX
=
T
UÎ£V V Î£ U
=
T
T
T
UÎ£Î£ U
T
T
(XX )
=
T âˆ’1
U(Î£Î£ )
U
T âˆ’1
T
X
=
T
V Î£ U
T
T
X (XX )
=
T
T âˆ’1
V Î£ U U(Î£Î£ )
U
=
T
T
T âˆ’1
T
V Î£ (Î£Î£ )
U
T
T âˆ’1
T
Î£ (Î£Î£ )
=
T
T âˆ’1
Î£â€ 
â€‹
X (XX )
= V Î£ U
T
T âˆ’1
â€ 
T
12/6/25, 11:12 PM
Vector calculus derivatives
Upon reprompting it is now right
These 


--- Page 8 ---
8/25
Same operator, different motivation.
ðŸ’¡ Why This Is Useful:
Both the overdetermined and underdetermined cases result in the same simplified matrix
expression:
This is the Mooreâ€“Penrose pseudoinverse of 
:
So:
In the least squares case: 
In the min-norm case: 
The difference lies in:
Whether 
 is tall (left-invertible)
Or wide (right-invertible)
âœ… Final Answers (Purely Matrix Simplified):
(b)
(e)
Let me know if you want visualizations or numerical examples to test this on real matrices.
â€‹
B = V Î£ U
â€ 
T
â€‹
V Î£ U
â€ 
T
X
X =
â€ 
V Î£ U
â€ 
T
w =
âˆ—
X y
â€ 
w =
âˆ—
X y
â€ 
X
â€‹
(X X)
X
= V Î£ U
T
âˆ’1
T
â€ 
T
â€‹
X (XX )
= V Î£ U
T
T âˆ’1
â€ 
T
4. The 5 Interpretations of Ridge Regression
(a) Perspective 1: Optimization Problem. Ridge regression 
can be understood as the unconstrained optimization 
problem
argmin
w
âˆ¥y âˆ’ Xwâˆ¥
2
2 + Î»âˆ¥wâˆ¥
2
2
, (1)
where X âˆˆ R
nÃ—d
is a data matrix, and y âˆˆ R
n
is the target vector of measurement values. Whatâ€™s new
compared to the simple OLS problem is the addition of 
the Î»âˆ¥wâˆ¥
2
term, which can be interpreted as a
"penalty" on the weights being too big.
Use vector calculus to expand the objective and solve this 
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 9 ---
9/25
optimization problem for w.
(b) Perspective 2: "Hack" of shifting the Singular Values. In 
the previous part, you should have found the
optimal w is given by
w = (XT X + Î»I)
âˆ’1XT y
(If you didnâ€™t get this, you should check your work for the 
previous part).
Homework 0, Â© Faculty teaching EECS 182, Fall 2025. All 
Rights Reserved. This may not be publicly shared without 
explicit permission. 2
Homework 0 @ 2025-08-29 14:33:01-07:00
Let X = UÎ£V
T be the (full) SVD of the X. Recall that U and V are square 
orthonormal (normpreserving) matrices, and Î£ is a n Ã— d 
matrix with singular values Ïƒi along the "diagonal". Plug
this into the Ridge Regression solution and simplify. What 
happens to the singular values of
(XT X + Î»I)
âˆ’1XT when Ïƒi << Î»? What about when Ïƒi >> Î»?
(c) Perspective 3: Maximum A Posteriori (MAP) estimation. 
Ridge Regression can be viewed as finding
the MAP estimate when we apply a prior on the (now 
viewed as random parameters) W. In particular,
we can think of the prior for W as being N (0, I) and view 
the random Y as being generated using
Y = x
TW +
âˆš
Î»N where the noise N is distributed iid (across training 
samples) as N (0, 1). At the
vector level, we have Y = XW +
âˆš
Î»N. Note that the X matrix whose rows are the n different
training points are not random.
Show that (1) is the MAP estimate for W given an 
observation Y = y.
(d) Perspective 4: Fake Data. Another way to interpret 
â€œridge regressionâ€ is as the ordinary least squares
for an augmented data set â€” i.e. adding a bunch of fake 
data points to our data. Consider the following
augmented measurement vector Ë†y and data matrix XË† :
Ë†y =
"
y
0d
#
XË† =
"
X
âˆš
Î»Id
#
,
where 0d is the zero vector in R
d
and Id âˆˆ R
dÃ—d
is the identity matrix. Show that the classical OLS
optimization problem argminw âˆ¥Ë†y âˆ’ XwË† âˆ¥
2
2 has the same minimizer as (1).
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 10 ---
10/25
(e) Perspective 5: Fake Features. For this last interpretation, 
letâ€™s instead construct an augmented design
matrix in the following way:
XË‡ = [X
âˆš
Î»In]
i.e. we stack X with âˆš
Î»In horizontally. Now our problem is underdetermined: the 
new dimension
d + n is larger than the number of points n. Therefore, 
there are infinitely many values Î· âˆˆ R
d+n
for
which XË‡Î· = y. We are interested in the min-norm solution, 
ie. the solution to
argmin
Î·
âˆ¥Î·âˆ¥
2
2
s.t. XË‡Î· = y. (2)
Show that this is yet another form of ridge regression and 
that the first d coordinates of Î·
âˆ—
form
the minimizer of (1).
(f) We know that the Moore-Penrose pseudo-inverse for 
an underdetermined system (wide matrix) is
given by Aâ€  = AT
(AAT
)
âˆ’1
, which corresponds to the min-norm solution for AÎ· = z. 
That is, the
optimization problem
argmin âˆ¥Î·âˆ¥
2
s.t.AÎ· = z
is solved by Î· = Aâ€ z. Let Ë†w be the minimizer of (1).
Use the pseudo-inverse to show that solving to the 
optimization problem in (2) yields
Ë†w = XT
(XXT + Î»I)
âˆ’1y
Then, show that this is equivalent to the standard formula 
for Ridge Regression
Ë†w = (XT X + Î»I)
âˆ’1XT y
Hint: It may be helpful to review Kernel Ridge Form.
Homework 0, Â© Faculty teaching EECS 182, Fall 2025. All 
Rights Reserved. This may not be publicly shared without 
explicit permission. 3
Homework 0 @ 2025-08-29 14:33:01-07:00
(g) We know that the solution to ridge regression (1) is 
given by Ë†wr = (XâŠ¤X + Î»I)
âˆ’1XâŠ¤y. What
happens when Î» â†’ âˆž? It is for this reason that sometimes 
ridge regularization is referred to as
â€œshrinkage.â€
(h) What happens to the solution of ridge regression when 
you take the limit Î» â†’ 0? Consider both
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 11 ---
11/25
the cases when X is wide (underdetermined system) and X 
is tall (overdetermined system).
Awesome â€” this is the Ridge Regression: 5 Perspectives problem from EECS 182 (Fall
2025). Let's walk through each part (a)â€“(h) with step-by-step reasoning.
âœ… (a) Perspective 1: Optimization Problem
We are solving:
Letâ€™s write the objective function explicitly:
Now take derivative w.r.t. 
 (as a row vector):
Set gradient to 0:
âœ… (b) Perspective 2: Shifting Singular Values
Given:
Ridge solution:
Use the identity:
So:
Effect of Î» on singular values:
Let singular values be 
, then:
If 
: denominator is dominated by Î» â‡’ expression â‰ˆ small â‡’ singular
directions with small Ïƒ get suppressed
If 
: denominator â‰ˆ 
 â‡’ expression â‰ˆ 
 â‡’ close to OLS
âœ… Ridge shrinks contributions from small singular directions, improving numerical
stability.
âœ… (c) Perspective 3: MAP Estimation
Given:
Prior: 
Observation model: 
, where 
MAP estimate maximizes:
â€‹âˆ¥y âˆ’
w
min
Xwâˆ¥+
2
Î»âˆ¥wâˆ¥2
L(w) = (y âˆ’Xw) (y âˆ’
T
Xw) + Î»w w =
T
y y âˆ’
T
2y Xw +
T
w X Xw +
T
T
Î»w w
T
w âˆˆRd
â€‹ =
âˆ‚w
âˆ‚L
âˆ’2y X +
T
2w X X +
T
T
2Î»w
=
T
2w (X X +
T
T
Î»I) âˆ’2y X
T
2w (X X +
T
T
Î»I) = 2y X â‡’
T
w
=
T
y X(X X +
T
T
Î»I)
â‡’
âˆ’1
â€‹
w = (X X + Î»I)
X y
T
âˆ’1
T
X = UÎ£V T
w = (X X +
T
Î»I)
X y
âˆ’1
T
X X =
T
V Î£ Î£V
â‡’
T
T
X X +
T
Î»I = V (Î£ Î£ +
T
Î»I)V T
(X X +
T
Î»I)
X
=
âˆ’1
T
V (Î£ Î£ +
T
Î»I)
V V Î£ U
=
âˆ’1
T
T
T
V (Î£ Î£ +
T
Î»I)
Î£ U
âˆ’1
T
T
Ïƒ
â€‹i
â€‹
Ïƒ
â€‹ + Î»
i
2
Ïƒ
â€‹i
Ïƒ
â€‹ â‰ª
i
Î»
Ïƒ
â€‹ â‰«
i
Î»
Ïƒ
â€‹i
2
â€‹
Ïƒ
â€‹i
1
W âˆ¼N(0, I)
Y = XW +
â€‹N
Î»
N âˆ¼N(0, I)
12/6/25, 11:12 PM
Vector calculus derivatives
These are mostly right


--- Page 12 ---
12/25
Log-likelihood:
Prior:
So the MAP objective is:
âœ… Matches ridge regression.
âœ… (d) Perspective 4: Fake Data
Augment data:
Now solve:
âœ… So this is exactly ridge regression.
âœ… (e) Perspective 5: Fake Features (Min-norm form)
Augment horizontally:
Define variable 
, and solve:
Write 
, then:
So the objective becomes:
Therefore:
âœ… The first  entries of the min-norm solution 
 correspond to ridge regression solution
log P(W âˆ£Y ) âˆlog P(Y âˆ£W) + log P(W)
log P(Y âˆ£W) âˆâˆ’
â€‹âˆ¥Y âˆ’
2Î»
1
XWâˆ¥
(GaussianÂ noise)
2
log P(W) âˆâˆ’
â€‹âˆ¥Wâˆ¥
2
1
2
â€‹
â€‹âˆ¥Y âˆ’XWâˆ¥+
â€‹âˆ¥Wâˆ¥
â‡’
W
min (2Î»
1
2
2
1
2)
â€‹âˆ¥Y âˆ’
W
min
XWâˆ¥+
2
Î»âˆ¥Wâˆ¥2
â€‹
MAPÂ estimate = RidgeÂ solution
â€‹ =
y^
â€‹ ,
=
[ y
0
â€‹d]
X^
â€‹
[ X
â€‹I
â€‹
Î» d]
â€‹âˆ¥
â€‹ âˆ’
w
min y^
wâˆ¥=
X^
2
âˆ¥y âˆ’Xwâˆ¥+
2
âˆ¥
â€‹wâˆ¥=
Î»
2
âˆ¥y âˆ’Xwâˆ¥+
2
Î»âˆ¥wâˆ¥2
â€‹
OLSÂ onÂ (
,
â€‹) â‡’ridgeÂ regressionÂ solution
X^ y^
=
XË‡
[X
â€‹I
â€‹] âˆˆ
Î» n
RnÃ—(d+n)
Î· âˆˆRd+n
â€‹âˆ¥Î·âˆ¥
s.t.Â 
Î· =
Î·
min
2
XË‡
y
Î· =
â€‹
[w
z ]
Î· =
XË‡
Xw +
â€‹z =
Î»
y â‡’z =
â€‹(y âˆ’
â€‹Î»
1
Xw)
âˆ¥Î·âˆ¥=
2
âˆ¥wâˆ¥+
2
âˆ¥zâˆ¥=
2
âˆ¥wâˆ¥+
2
â€‹âˆ¥y âˆ’
Î»
1
Xwâˆ¥2
â€‹âˆ¥Î·âˆ¥â‡”
Î·
min
2
â€‹âˆ¥y âˆ’
w
min
Xwâˆ¥+
2
Î»âˆ¥wâˆ¥2
d
Î·âˆ—
w^
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 13 ---
13/25
âœ… (f) Use Pseudo-inverse to show kernel form of ridge:
From part (e), you solve:
Recall 
Then:
So:
First  components of  are:
This is the dual/kernel form of ridge regression.
Now prove equivalence to standard form:
Use matrix identity:
âœ… Proven!
âœ… (g) Behavior as 
Ridge regression solution:
As 
:
 dominates â†’ inverse tends to 0
So 
âœ… This is called shrinkage: all weights are pushed toward zero.
âœ… (h) Behavior as 
Case 1: Tall matrix 
 is full-rank and invertible
So:
âœ… Converges to OLS solution
Case 2: Wide matrix 
 is not invertible
So OLS is ill-posed (infinitely many solutions)
But ridge still yields a unique solution
â€‹âˆ¥Î·âˆ¥
s.t.Â 
Î· =
Î·
min
2
XË‡
y â‡’Î· =
y =
XË‡ â€ 
(
)
y
XË‡ T XË‡XË‡ T âˆ’1
=
XË‡
[X
â€‹I
â€‹]
Î» n
=
XË‡XË‡ T
XX
+
T
Î»I
Î· =
(XX
+
XË‡ T
T
Î»I)
y
âˆ’1
d
Î·
â€‹
= X (XX
+ Î»I)
y
w^
T
T
âˆ’1
=
w^
X (XX
+
T
T
Î»I)
y
âˆ’1
(X X +
T
Î»I)
X
=
âˆ’1
T
X (XX
+
T
T
Î»I)
â‡’
âˆ’1
â€‹
(X X + Î»I)
X y = X (XX
+ Î»I)
y
T
âˆ’1
T
T
T
âˆ’1
Î» â†’âˆž
=
w^
(X X +
T
Î»I)
X y
âˆ’1
T
Î» â†’âˆž
Î»I
â†’
w^
0
â€‹
â†’0Â asÂ Î» â†’âˆž
w^
Î» â†’0
(n > d)
X X
T
=
w^
(X X +
T
Î»I)
X y â†’
âˆ’1
T
(X X)
X y
T
âˆ’1
T
(n < d)
X X
T
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 14 ---
14/25
As 
:
 tends toward a solution of 
But among infinitely many solutions, it converges to the min-norm solution
âœ… So:
Let me know if you want a PDF-style writeup or a diagram showing how all 5 perspectives
are connected.
Î» â†’0
w^
Xw = y
â€‹
â†’OLSÂ solutionÂ ifÂ uniqueÂ (tall)
w^
â€‹
â†’minimum-normÂ solutionÂ ifÂ underdeterminedÂ (wide)
w^
ReLU Elbow Update under SGD
In this question we will explore the behavior of the ReLU 
nonlinearity with Stochastic Gradient Descent
(SGD) updates. The hope is that this problem should help 
you build a more intuitive understanding for how
SGD works and how it iteratively adjusts the learned 
function.
We want to model a 1D function y = f(x) using a 1-hidden 
layer network with ReLU activations and no
biases in the linear output layer. Mathematically, our 
network is
Ë†f(x) = W(2)Î¦
Ä‘
W(1)x + b
Ä’
where x, y âˆˆ R, b âˆˆ R
d
, W(1) âˆˆ R
dÃ—1
, and W(2) âˆˆ R
1Ã—d
. We define our loss function to be the squared
error,
â„“
Ä‘
x, y,W(1)
, b,W(2)Ä’
=
1
2
Ë†f(x) âˆ’ y
2
2
.
For the purposes of this problem, we define the gradient 
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 15 ---
15/25
of a ReLU at 0 to be 0.
(a) Letâ€™s start by examining the behavior of a single ReLU 
with a linear function of x as the input,
Ï•(x) = (
wx + b, wx + b > 0
0, else
.
Notice that the slope of Ï•(x) is w in the non-zero domain.
We define a loss function â„“(x, y, Ï•) = 1
2
âˆ¥Ï•(x) âˆ’ yâˆ¥
2
2
. Find the following:
(i) The location of the â€˜elbowâ€™ e of the function, where it 
transitions from 0 to something else.
(ii) The derivative of the loss w.r.t. Ï•(x), namely dâ„“
dÏ•
(iii) The partial derivative of the loss w.r.t. w, namely âˆ‚â„“
âˆ‚w
(iv) The partial derivative of the loss w.r.t. b, namely âˆ‚â„“
âˆ‚b
(b) Now suppose we have some training point (x, y) such 
that Ï•(x)âˆ’y = 1. In other words, the prediction
Ï•(x) is 1 unit above the target y â€” we are too high and are 
trying to pull the function downward.
Describe what happpens to the slope and elbow of Ï•(x) 
when we perform gradient descent in the
following cases:
(i) Ï•(x) = 0.
(ii) w > 0, x > 0, and Ï•(x) > 0. It is fine to check the 
behavior of the elbow numerically in this
case.
(iii) w > 0, x < 0, and Ï•(x) > 0.
(iv) w < 0, x > 0, and Ï•(x) > 0. It is fine to check the 
behavior of the elbow numerically in this
case.
Homework 0, Â© Faculty teaching EECS 182, Fall 2025. All 
Rights Reserved. This may not be publicly shared without 
explicit permission. 4
Homework 0 @ 2025-08-29 14:33:01-07:00
Additionally, draw and label Ï•(x), the elbow, and the 
qualitative changes to the slope and elbow
after a gradient update to w and b. You should label the 
elbow location and a candidate (x, y)
pair. Remember that the update for some parameter 
vector p and loss â„“ under SGD is
p
â€² = p âˆ’ Î»âˆ‡p(â„“), Î» > 0.
(c) Now we return to the full network function Ë†f(x). Derive 
the location ei of the elbow of the iâ€™th
elementwise ReLU activation.
(d) Derive the new elbow location e
â€²
i
of the iâ€™th elementwise ReLU activation after one stochastic
gradient update with learning rate Î».
Letâ€™s work through the ReLU Elbow Update under SGD problem in EECS 182 step by step.
We'll handle all four subparts (a)â€“(d) with clear derivations and explanations â€” including
formulas, intuitions, and how the elbow changes under SGD.
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 16 ---
16/25
ðŸŸ© (a) Single ReLU Unit
We are given the activation function:
and the loss:
âœ… (i) Location of the "elbow" 
The elbow is where 
 transitions from 0 to non-zero â€” i.e., the argument to ReLU
becomes 0:
ðŸŸ¢ Answer:
âœ… (ii) Derivative of loss w.r.t. output 
We define:
ðŸŸ¢ Answer:
âœ… (iii) Partial derivative of loss w.r.t. 
Use chain rule:
Case 1: If 
, and ReLU derivative is defined to be 0 at 0, so:
Case 2: If 
, so:
ðŸŸ¢ Answer:
âœ… (iv) Partial derivative of loss w.r.t. 
Same reasoning as above:
Ï†(x) =
â€‹
â€‹
{wx + b
0
ifÂ wx + b > 0
otherwise
â„“(x, y, Ï†) =
â€‹(Ï†(x) âˆ’
2
1
y)2
e
Ï†(x)
wx + b = 0 â‡’x = âˆ’
â€‹
w
b
â€‹
e = âˆ’
â€‹
w
b
Ï†(x)
â„“=
â€‹(Ï†(x) âˆ’
2
1
y) â‡’
2
â€‹ =
dÏ†(x)
dâ„“
Ï†(x) âˆ’y
â€‹
â€‹ = Ï†(x) âˆ’y
dÏ†(x)
dâ„“
w
â€‹ =
âˆ‚w
âˆ‚â„“
â€‹ â‹…
dÏ†(x)
dâ„“
â€‹
âˆ‚w
âˆ‚Ï†(x)
wx + b â‰¤0 â‡’Ï†(x) = 0
â€‹ =
âˆ‚w
âˆ‚Ï†(x)
0 â‡’
â€‹ =
âˆ‚w
âˆ‚â„“
0
wx + b > 0 â‡’Ï†(x) = wx + b
â€‹ =
âˆ‚w
âˆ‚Ï†(x)
x â‡’
â€‹ =
âˆ‚w
âˆ‚â„“
(Ï†(x) âˆ’y) â‹…x
â€‹
â€‹ =
â€‹
â€‹
âˆ‚w
âˆ‚â„“
{0
(Ï†(x) âˆ’y) â‹…x
ifÂ wx + b â‰¤0
ifÂ wx + b > 0
b
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 17 ---
17/25
If 
, then derivative is 0.
If 
, then 
ðŸŸ¢ Answer:
ðŸŸ© (b) Behavior of Gradient Descent on Elbow & Slope
Given: 
, so we are too high and want to decrease 
Letâ€™s study what happens to 
 and  under SGD:
Also recall:
âœ… (i) Case: 
Means: 
Then:
So: 
, 
, and nothing changes
ðŸŸ¢ Summary:
Slope: unchanged
Elbow: unchanged
âœ… (ii) Case: 
, 
, 
Then:
Update:
Elbow:
Numerical example: say 
, 
, 
, 
Old: 
New: 
ðŸŸ¢ Summary:
Slope decreases
Elbow shifts slightly right (if x > 0)
â€‹ =
âˆ‚b
âˆ‚â„“
â€‹ â‹…
dÏ†(x)
dâ„“
â€‹
âˆ‚b
âˆ‚Ï†(x)
wx + b â‰¤0
wx + b > 0
â€‹ =
âˆ‚b
âˆ‚Ï†(x)
1
â€‹
â€‹ =
â€‹
â€‹
âˆ‚b
âˆ‚â„“
{0
(Ï†(x) âˆ’y)
ifÂ wx + b â‰¤0
ifÂ wx + b > 0
Ï†(x) âˆ’y = 1
Ï†(x)
w
b
w =
â€²
w âˆ’Î» â‹…
â€‹
b =
âˆ‚w
âˆ‚â„“
â€²
b âˆ’Î» â‹…âˆ‚b
âˆ‚â„“
elbowÂ e = âˆ’
â€‹
w
b
Ï†(x) = 0
wx + b â‰¤0 â‡’ReLUÂ inactive
â€‹ =
âˆ‚w
âˆ‚â„“
0
â€‹ =
âˆ‚b
âˆ‚â„“
0
w =
â€²
w b =
â€²
b
w > 0 x > 0 Ï†(x) > 0
â€‹ =
âˆ‚w
âˆ‚â„“
(1) â‹…x = x
â€‹ =
âˆ‚b
âˆ‚â„“
1
w =
â€²
w âˆ’Î»x
(â†“)
b =
â€²
b âˆ’Î»
(â†“)
e = âˆ’
â€‹ â‡’
w
b
e =
â€²
âˆ’
â€‹
w âˆ’Î»x
b âˆ’Î»
w = 2 b = 1 x = 1 Î» = 0.1
e = âˆ’
â€‹ =
2
1
âˆ’0.5
e =
â€²
âˆ’
â€‹ â‰ˆ
1.9
0.9
âˆ’0.474
12/6/25, 11:12 PM
Vector calculus derivatives
Elbow shifts for (ii) and (iii) are wrong


--- Page 18 ---
18/25
âœ… (iii) Case: 
, 
, 
Update:
Elbow:
â†’ Elbow shifts left
ðŸŸ¢ Summary:
Slope increases
Elbow shifts left
âœ… (iv) Case: 
, 
, 
Now both numerator and denominator decrease (more negative)
â†’ Elbow 
Depending on signs, this might push the elbow right or left, but in general:
ðŸŸ¢ Summary:
Slope decreases (steeper negative)
Elbow shifts
ðŸ–¼ï¸ Drawing Guidance
In your sketch:
Label:
Elbow: 
Original and updated lines
(x, y) point with error
Arrows showing slope and elbow motion
ðŸŸ© (c) Elbow location 
 for i-th ReLU in full network
Each hidden unit computes:
Elbow where pre-activation is zero:
ðŸŸ¢ Answer:
w > 0 x < 0 Ï†(x) > 0
â€‹ =
âˆ‚w
âˆ‚â„“
(1) â‹…x < 0 â‡’w =
â€²
w âˆ’Î»x â†‘
b =
â€²
b âˆ’Î»
e =
â€²
âˆ’
â€‹ â‡’
w + Î»âˆ£xâˆ£
b âˆ’Î»
denominatorÂ increases,Â numeratorÂ decreases
w < 0 x > 0 Ï†(x) > 0
â€‹ =
âˆ‚w
âˆ‚â„“
(1) â‹…x > 0 â‡’w =
â€²
w âˆ’Î»x â†“
b =
â€²
b âˆ’Î»
e =
â€²
âˆ’
â€‹
wâˆ’Î»x
bâˆ’Î»
x = âˆ’b/w
e
â€‹i
Ï†
â€‹(x) =
i
ReLU(w
â€‹x +
i
(1)
b
â€‹)
i
w
â€‹x +
i
(1)
b
â€‹ =
i
0 â‡’x = âˆ’
â€‹
w
â€‹
i
(1)
b
â€‹i
â€‹
e
â€‹ = âˆ’
â€‹
i
w
â€‹
i
(1)
b
â€‹i
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 19 ---
19/25
ðŸŸ© (d) Elbow location after SGD update
Let:
Then new elbow:
Thatâ€™s the updated location based on gradient changes to 
 and .
Let me know if you want me to render these as plots or provide a Python visualization
template!
w
â€‹ =
i
(1)â€²
w
â€‹ âˆ’
i
(1)
Î»
â€‹
b
â€‹ =
âˆ‚w
â€‹
i
(1)
âˆ‚â„“
i
â€²
b
â€‹ âˆ’
i
Î»
â€‹
âˆ‚b
â€‹i
âˆ‚â„“
â€‹
e
â€‹ = âˆ’
â€‹
i
â€²
w
â€‹ âˆ’Î»
â€‹
i
(1)
âˆ‚w
â€‹
i
(1)
âˆ‚â„“
b
â€‹ âˆ’Î»
â€‹
i
âˆ‚b
â€‹i
âˆ‚â„“
w
b
no (ii) elbow can shift both ways and (iii) elbow shifts right 
Youâ€™re absolutely right to call that out â€” let's correct the elbow movement logic for parts
(ii) and (iii) in the ReLU gradient update question.
Letâ€™s now go over each case again carefully, only focusing on the direction the elbow
shifts under a gradient descent update, with all signs and ratios tracked properly.
ðŸ” Reminder:
The ReLU elbow is at:
After a gradient descent step:
Given: 
, so gradient is:
âœ… (ii) 
, 
, 
Update:
 â†’ decreases
 â†’ decreases
So:
Letâ€™s analyze how 
 compares to 
:
Compute difference:
e = âˆ’
â€‹
w
b
w =
â€²
w âˆ’Î»
â€‹,
b =
âˆ‚w
âˆ‚â„“
â€²
b âˆ’Î»
â€‹ â‡’
âˆ‚b
âˆ‚â„“
e =
â€²
âˆ’
â€‹ =
wâ€²
bâ€²
âˆ’
â€‹
w âˆ’Î»
â€‹
âˆ‚w
âˆ‚â„“
b âˆ’Î»
â€‹
âˆ‚b
âˆ‚â„“
Ï†(x) âˆ’y = 1
â€‹ =
âˆ‚w
âˆ‚â„“
x
â€‹ =
âˆ‚b
âˆ‚â„“
1
w > 0 x > 0 Ï†(x) > 0
w =
â€²
w âˆ’Î»x
b =
â€²
b âˆ’Î»
e =
â€²
âˆ’w âˆ’Î»x
b âˆ’Î»
eâ€²
e = âˆ’
â€‹
w
b
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 20 ---
20/25
We want to check the sign of this quantity â€” it depends on the values of 
, so:
Try numerical example:
Then:
ðŸ” So:
âœ… Corrected answer:
Slope decreases
Elbow shifts right
âœ… (iii) 
, 
, 
Update:
 â†’ increases
 â†’ decreases
So:
Try a numerical example:
Then:
ðŸ” So:
âœ… Corrected answer:
Slope increases
Elbow shifts right
âœ… Summary (Corrected):
Case
Conditions
Slope Update
Elbow Shift
(i)
No change
No change
(ii)
Decreases
Right
(iii)
Increases
Right
e âˆ’
â€²
e = âˆ’
â€‹ +
w âˆ’Î»x
b âˆ’Î»
â€‹
w
b
b, w, Î», x
w = 2, b = 1, x = 1, Î» = 0.1
e = âˆ’
â€‹ =
2
1
âˆ’0.5
w =
â€²
2 âˆ’0.1 = 1.9
b =
â€²
1 âˆ’0.1 = 0.9
e =
â€²
âˆ’
â€‹ â‰ˆ
1.9
0.9
âˆ’0.474
e >
â€²
e â‡’ElbowÂ shiftsÂ **right**
w > 0 x < 0 Ï†(x) > 0
â€‹ =
âˆ‚w
âˆ‚â„“
x < 0 â‡’w =
â€²
w âˆ’Î»x = w + Î»âˆ£xâˆ£
b =
â€²
b âˆ’Î»
e =
â€²
âˆ’
â€‹
w + Î»âˆ£xâˆ£
b âˆ’Î»
w = 2, b = 1, x = âˆ’1, Î» = 0.1
e = âˆ’
â€‹ =
2
1
âˆ’0.5
w =
â€²
2 + 0.1 = 2.1
b =
â€²
1 âˆ’0.1 = 0.9
e =
â€²
âˆ’
â€‹ â‰ˆ
2.1
0.9
âˆ’0.429
e >
â€²
e â‡’ElbowÂ shiftsÂ **right**
Ï†(x) = 0
w > 0, x > 0
w > 0, x < 0
12/6/25, 11:12 PM
Vector calculus derivatives
Elbow shifts are wrong


--- Page 21 ---
21/25
Case
Conditions
Slope Update
Elbow Shift
(iv)
Decreases further
(more negative)
Can shift left or right depending
on ratio
Thanks for catching that â€” especially the elbow always shifting right in both (ii) and (iii)
under these update dynamics. Let me know if you want to visualize all these cases!
w < 0, x > 0
no (ii) your expression was right, but the elbow shift can be 
one of two ways
You're absolutely right again â€” letâ€™s take a second, more precise pass at (ii) and settle this
fully.
ðŸ” Revisit: (ii) 
, 
, 
We are analyzing:
Our goal: determine when 
 or 
 â€” i.e., which direction the elbow shifts after
one SGD step with learning rate .
ðŸ§  Let's Analyze the Sign of 
We compute:
Letâ€™s define this expression and analyze it:
Letâ€™s write both terms with a common denominator:
So:
ðŸŽ¯ Now analyze the sign of 
:
, 
So the denominator 
Thus, the sign of 
 is determined by the numerator:
ðŸ” So:
If 
 â†’ 
 â†’ 
 â†’ elbow shifts right
If 
 â†’ 
 â†’ 
 â†’ elbow shifts left
w > 0 x > 0 Ï†(x) > 0
e = âˆ’
â€‹,
e =
w
b
â€²
âˆ’
â€‹
w âˆ’Î»x
b âˆ’Î»
e >
â€²
e
e <
â€²
e
Î»
e âˆ’
â€²
e
e âˆ’
â€²
e = âˆ’
â€‹ +
w âˆ’Î»x
b âˆ’Î»
â€‹
w
b
Î”e := e âˆ’
â€²
e =
â€‹ âˆ’
w
b
â€‹
w âˆ’Î»x
b âˆ’Î»
Î”e =
â€‹ =
w(w âˆ’Î»x)
b(w âˆ’Î»x) âˆ’(b âˆ’Î»)w
â€‹ =
w(w âˆ’Î»x)
bw âˆ’bÎ»x âˆ’bw + Î»w
â€‹
w(w âˆ’Î»x)
Î»(w âˆ’bx)
â€‹
Î”e = e âˆ’e =
â€‹
â€²
w(w âˆ’Î»x)
Î»(w âˆ’bx)
Î”e
Î» > 0
w > 0 x > 0
w(w âˆ’Î»x) > 0
Î”e
Sign(e âˆ’
â€²
e) = sign(w âˆ’bx)
w > bx
Î”e > 0
e >
â€²
e
w < bx
Î”e < 0
e <
â€²
e
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 22 ---
22/25
âœ… Final Corrected Answer for (ii):
Under the update:
Then:
âœ… So the elbow can shift left or right depending on the relationship between 
 and 
.
Summary (Updated Table):
Case
Conditions
Slope Update
Elbow Shift
(i)
No change
No change
(ii)
Decreases
Left or Right depending on
(iii)
Increases
Right
(iv)
Decreases
Mixed â€” depends on update
ratio
Thanks for the correction â€” (ii) is a subtle but important case! Let me know if youâ€™d like
plots to visualize this or a symbolic manipulation notebook.
w =
â€²
w âˆ’Î»x,
b =
â€²
b âˆ’Î»
â‡’
e =
â€²
âˆ’
â€‹
w âˆ’Î»x
b âˆ’Î»
â€‹
e â‰·e âŸºw â‰·bx
â€²
w
bx
Ï†(x) = 0
w > 0, x > 0, Ï†(x) > 0
w â‰·bx
w > 0, x < 0
w < 0, x > 0
(iv) is also wrong it is supposed to JUST move left
Excellent, youâ€™re absolutely correct again â€” letâ€™s do a clean and final pass on Case (iv),
double-checking all signs carefully to confirm that the elbow only moves left in this
situation.
âœ… Case (iv): 
, 
, and 
We're analyzing the elbow location:
Letâ€™s again analyze:
Same derivation as before, but now apply to signs.
ðŸš§ Letâ€™s Analyze the Signs
Given:
So:
w < 0 x > 0
Ï†(x) > 0
e = âˆ’
â€‹,
e =
w
b
â€²
âˆ’
â€‹
w âˆ’Î»x
b âˆ’Î»
Î”e = e âˆ’
â€²
e =
â€‹
w(w âˆ’Î»x)
Î»(w âˆ’bx)
Î» > 0
x > 0
w < 0 â‡’ReLUÂ isÂ "facingÂ left"
Ï†(x) = wx + b > 0 â‡’wx + b > 0
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 23 ---
23/25
Now letâ€™s look at the denominator:
So:
Numerator: < 0
Denominator: > 0
â‡’ 
ðŸŸ¢ So the elbow always shifts left under these conditions.
âœ… Final Verdict for (iv):
âœ… Final Final Table (Everything Corrected):
Case
Conditions
Slope Update
Elbow Shift
(i)
No change
No change
(ii)
Decreases
Left or Right depending on
(iii)
Increases
Right
(iv)
Decreases (steeper
negative)
Left âœ…
Thanks for sticking with the details â€” these sign-and-slope behaviors are core to
understanding SGD + ReLU behavior during training. Let me know if you want animated
plots or Python simulations of these elbows moving in real time.
b > âˆ’wx â‡’bx > âˆ’wx â‡’
2
w âˆ’bx < 0
(numeratorÂ <Â 0)
w < 0,
Î»x > 0 â‡’w âˆ’Î»x < 0 â‡’w(w âˆ’Î»x) > 0
(productÂ ofÂ twoÂ negatives)
Î”e = e âˆ’
â€²
e < 0 â‡’e <
â€²
e
w < 0, x > 0, Ï†(x) > 0 â‡’
â€‹
ElbowÂ shiftsÂ left
Ï†(x) = 0
w > 0, x > 0
w â‰·bx
w > 0, x < 0
w < 0, x > 0
could you derive part (d) in more detail please
Absolutely â€” letâ€™s go through part (d) in full detail:
ðŸ” Recap from earlier parts
We are modeling a 1D input 
 with a 1-hidden-layer ReLU neural network, no bias at
the output layer. The forward pass is:
Where:
 â€” input-to-hidden weights
 â€” hidden layer biases
 â€” hidden-to-output weights
 â€” elementwise ReLU
So the pre-activation of the -th neuron is:
(d) Derive the new elbow location 
 of the i-th elementwise ReLU activation after one
stochastic gradient update with learning rate .
e
â€‹i
â€²
Î»
x âˆˆR
(x) =
f^
W
â‹…
(2) Î¦(W
x +
(1)
b)
W
âˆˆ
(1)
RdÃ—1
b âˆˆRd
W
âˆˆ
(2)
R1Ã—d
Î¦(â‹…)
i
12/6/25, 11:12 PM
Vector calculus derivatives
Finally corrected, although
the reasoning is not that 
clear.
Detailed derivation here
which is correct, but 
I had to prompt it


--- Page 24 ---
24/25
and the elbow is where this transitions from 0 to non-zero (i.e., when 
)
Thus:
ðŸ§  Now: After one SGD step
We assume the network is trained using squared loss:
We're doing SGD updates for parameters 
 and 
. After one SGD step with learning
rate :
Then, the new elbow location becomes:
âœ… Letâ€™s now derive 
 and 
Weâ€™ll use chain rule through the network. Letâ€™s define:
Suppose 
 (i.e., ReLU is active, so its derivative is 1)
Then:
Next:
Now apply:
So finally:
âœ… Now plug into the new elbow expression:
z
â€‹(x) =
i
w
â€‹x +
i
(1)
b
â€‹i
z
â€‹(x) =
i
0
elbowÂ ofÂ neuronÂ i :
â€‹
e
â€‹ = âˆ’
â€‹
i
w
â€‹
i
(1)
bi
â„“(x, y) =
â€‹( (x) âˆ’
2
1 f^
y)2
w
â€‹
i
(1)
b
â€‹i
Î»
w
â€‹ =
i
â€²
w
â€‹ âˆ’
i
Î» â‹…
â€‹
and
b
â€‹ =
âˆ‚w
â€‹i
âˆ‚â„“
i
â€²
b
â€‹ âˆ’
i
Î» â‹…
â€‹
âˆ‚b
â€‹i
âˆ‚â„“
â€‹
e
â€‹ = âˆ’
â€‹ = âˆ’
â€‹
i
â€²
w
â€‹i
â€²
b
â€‹i
â€²
w
â€‹ âˆ’Î» â‹…
â€‹
i
âˆ‚w
â€‹i
âˆ‚â„“
b
â€‹ âˆ’Î» â‹…
â€‹
i
âˆ‚b
â€‹i
âˆ‚â„“
âˆ‚wi
âˆ‚â„“
â€‹
âˆ‚b
â€‹i
âˆ‚â„“
z
â€‹ =
i
w
â€‹x +
i
b
â€‹i
Ï•
â€‹ =
i
ReLU(z
â€‹)
i
â€‹(x) =
f^
â€‹ W
â€‹Ï•
â€‹
âˆ‘j=1
d
j
(2)
j
Ï•
â€‹ >
i
0
â€‹ =
âˆ‚Ï•
â€‹i
âˆ‚â„“
â€‹ â‹…
âˆ‚
â€‹f^
âˆ‚â„“
â€‹ =
âˆ‚Ï•
â€‹i
âˆ‚
â€‹f^
(
â€‹ âˆ’
f^
y) â‹…W
â€‹
i
(2)
â€‹ =
âˆ‚z
â€‹i
âˆ‚Ï•
â€‹i
1[z
â€‹ >
i
0] = 1
(sinceÂ active) â‡’
â€‹ =
âˆ‚z
â€‹i
âˆ‚â„“
(
â€‹ âˆ’
f^
y)W
â€‹
i
(2)
â€‹ =
âˆ‚w
â€‹i
âˆ‚z
â€‹i
x
and
â€‹ =
âˆ‚b
â€‹i
âˆ‚z
â€‹i
1
â€‹
â€‹ = (
â€‹ âˆ’y)W
â€‹ â‹…x
,
â€‹ = (
â€‹ âˆ’y)W
â€‹
âˆ‚w
â€‹i
âˆ‚â„“
f^
i
(2)
âˆ‚b
â€‹i
âˆ‚â„“
f^
i
(2)
e
â€‹ =
i
â€²
âˆ’
â€‹
w
â€‹ âˆ’Î»(
â€‹ âˆ’y)W
â€‹x
i
f^
i
(2)
b
â€‹ âˆ’Î»(
â€‹ âˆ’y)W
â€‹
i
f^
i
(2)
12/6/25, 11:12 PM
Vector calculus derivatives


--- Page 25 ---
25/25
This is the updated location of the elbow for neuron  after one SGD step, under the
assumption that the ReLU is active at  (i.e. 
).
ðŸ§ª Edge case: if ReLU is inactive, then gradients are 0
If 
, then by convention:
âœ… Final Answer (Full Derivation):
Let me know if you'd like a plot or animation to visualize how this elbow moves depending
on the sign of 
, or how this behaves for multiple ReLU neurons.
i
x
z
â€‹(x) >
i
0
z
â€‹(x) â‰¤
i
0 â‡’Ï•
â€‹ =
i
0
â€‹ =
âˆ‚z
â€‹i
âˆ‚Ï•
â€‹i
0 â‡’
â€‹ =
âˆ‚w
â€‹i
âˆ‚â„“
0,
â€‹ =
âˆ‚b
â€‹i
âˆ‚â„“
0 â‡’w
â€‹, b
â€‹ =
i
â€²
i
â€²
w
â€‹, b
â€‹ â‡’
i
i
â€‹
e
â€‹ = e
â€‹
i
â€²
i
â€‹
e
â€‹ =
â€‹
â€‹
â€‹
i
â€²
âŽ©
âŽ¨
âŽ§âˆ’
â€‹
w
â€‹ âˆ’Î»(
â€‹(x) âˆ’y)W
â€‹x
i
(1)
f^
i
(2)
b
â€‹ âˆ’Î»(
â€‹(x) âˆ’y)W
â€‹
i
f^
i
(2)
e
â€‹i
ifÂ w
â€‹x + b
â€‹ > 0
i
(1)
i
ifÂ w
â€‹x + b
â€‹ â‰¤0
i
(1)
i
â€‹(x) âˆ’
f^
y
12/6/25, 11:12 PM
Vector calculus derivatives
