--- Page 1 ---
I'll work through this step-by-step to find the output of the j-th node at layer k.
Setup
Let me denote:
 = the n-dimensional output vector of node  at layer 
The initial input: 
 (the i-th standard basis vector, a one-hot encoding)
Layer-by-layer computation
Layer 0 (Input):
where the 1 is in the i-th position.
Layer 1:Node  receives weighted inputs from all nodes in layer 0. The weight from node  to node  is 
.
Since 
 is the i-th standard basis vector, this sum picks out the i-th component being 
 for each . Therefore:
Wait, let me reconsider the indexing. Actually:
This gives us a vector whose i-th component is 
.
Pattern Recognition:
By the same logic, at layer 2:
Substituting the expression for 
:
The term in parentheses is 
, so:
You asked:
In this problem, we will explore connections between graph dynamics and graph neural networks (GNNs). Diagrams in this question
were taken from https://distill.pub/2021/gnn-intro. This blog post is an excellent resource for understanding GNNs and contains
interactive diagrams. Some graph neural network methods operate on the full adjacency matrix. Others, such as those discussed in
https://distill.pub/2021/gnn-intro/, at each layer apply the same local operation to each node based on inputs from its neighbors. This
problem is designed to: * Show connections between these methods. * Show that for a positive integer (k), the matrix (A^k) has an
interesting interpretation. That is, the entry in row (i) and column (j) gives the number of walks of length (k) (i.e., a collection of (k)
edges) leading from vertex (i) to vertex (j). To do this, let’s consider a very simple deep linear network, defined as follows: * Its
underlying graph has (n) vertices, with adjacency matrix (A). That is, (A_{i,j} = 1) if vertices (i) and (j) are connected in the graph and
(0)
h
i
* I h
( )
i
i
h l
di
h
( )
i
f h
d
l i
h * E
h
h
( )
Claude Replied:
h
​
i
(k)
i
k
h
​ =
i
(0)
e
​i
h
​ =
i
(0)
e
​ =
i
(0,… ,0,1,0,… ,0)T
j
i
j
A
​
i,j
h
​ =
j
(1)
​A
​ ⋅
i=1
∑
n
i,j
h
​ =
i
(0)
​A
​ ⋅
i=1
∑
n
i,j
e
​i
e
​i
A
​
i,j
i
h
​ =
j
(1)
(A
​,A
​,… ,A
​)
=
1,j
2,j
n,j
T
A e
​ =
T
j
j-th column of AT
h
​ =
j
(1)
​A
​ ⋅
i=1
∑
n
i,j
e
​i
A
​
i,j
h
​ =
j
(2)
​A
​ ⋅
i=1
∑
n
i,j
h
​
i
(1)
h
​
i
(1)
h
​ =
j
(2)
​A
​
​A
​e
​ =
i=1
∑
n
i,j
ℓ=1
∑
n
ℓ,i ℓ
​
​A
​A
​
e
​
ℓ=1
∑
n
(
i=1
∑
n
ℓ,i
i,j)
ℓ
(A )
​
2 ℓ,j
h
​ =
j
(2)
​(A )
​e
​
ℓ=1
∑
n
2
ℓ,j
ℓ
Exported via AI Exporter
1 / 15 页


--- Page 2 ---
General Formula
By induction, the output of node  at layer  is:
Or equivalently, 
 is the j-th column of 
, which can also be written as:
This beautifully connects to the interpretation that 
 counts the number of walks of length  from vertex  to vertex : the i-th
component of 
 tells us how many k-step walks end at node  starting from node .
I'll prove that the i-th component of 
 equals 
, the number of paths of length  from vertex  to vertex .
Proof by Induction
Claim: The i-th component of 
 is 
.
From part (a), we know that 
 is an n-dimensional vector whose i-th component is 
.
So we need to prove: 
.
Base Case: 
For 
:
By convention, 
Therefore, 
. ✓
Inductive Step
Inductive Hypothesis: Assume that for some 
, we have 
 for all vertices  and .
**Goal:** Prove that 
.
Proof of Inductive Step
By matrix multiplication:
By the inductive hypothesis, 
, so:
j
k
​
h
​ =
​(A )
​ ⋅e
​
j
(k)
i=1
∑
n
k
i,j
i
h
​
j
(k)
(A )
k T
​
h
​ = (A ) e
​ or h
​ = vector with i-th component equal to (A )
​
j
(k)
k T
j
j
(k)
k
i,j
(A )
​
k i,j
k
i
j
h
​
j
(k)
j
i
You asked:
(b) Recall that a path from (i) to (j) in a graph is a sequence of vertices that starts with (i), ends with (j), and every successive vertex
in the sequence is connected by an edge in the graph. The length of a path is the number of edges in it. Here is some helpful
notation: * (V(i)) is the set of vertices that are connected to vertex (i) in the graph. * (L_k(i,j)) is the number of distinct paths that go
from vertex (i) to vertex (j) in the graph where the number of edges traversed in the path is exactly (k). * By convention, there is
exactly 1 path of length 0 that starts at each node and ends up at itself. That is, (L_0(i,j) = 1_{i=j}). Prove that the (i)-th output of node
(j) at layer (k) in the network above is the count of how many paths there are from (i) to (j) of length (k). (Hint: Induct on (k).)
Claude Replied:
h
​
j
(k)
L
​(i,j)
k
k
i
j
h
​
j
(k)
L
​(i,j)
k
h
​
j
(k)
(A )
​
k i,j
(A )
​ =
k i,j
L
​(i,j)
k
k = 0
k = 0
(A )
​ =
0 i,j
I
​ =
i,j
​
​
{1
0
if i = j
if i = j

L
​(i,j) =
0
1
​ =
i=j
​
​
{1
0
if i = j
if i = j

(A )
​ =
0 i,j
L
​(i,j)
0
k ≥0
(A )
​ =
k i,j
L
​(i,j)
k
i
j
(A
)
​ =
k+1 i,j
L
​(i,j)
k+1
(A
)
​ =
k+1
i,j
(A ⋅
k A)
​ =
i,j
​(A )
​ ⋅
ℓ=1
∑
n
k
i,ℓA
​
ℓ,j
(A )
​ =
k i,ℓ
L
​(i,ℓ)
k
(A
)
​ =
k+1
i,j
​L
​(i,ℓ) ⋅
ℓ=1
∑
n
k
A
​
ℓ,j
Exported via AI Exporter
2 / 15 页
Correct
Although the solution starts with k = 1, I think this is ﬁne.


--- Page 3 ---
Now, recall that 
 if and only if there is an edge from  to  in the graph, i.e., 
 (the set of neighbors of ).
Otherwise, 
.
Therefore:
Combinatorial Interpretation
Now let's count 
 directly:
A path of length 
 from  to  consists of:
1. A path of length  from  to some intermediate vertex 
2. Followed by one edge from  to 
For this to be possible,  must be a neighbor of , i.e., 
.
The number of paths of length 
 from  to  that pass through  as the second-to-last vertex is exactly 
.
Summing over all possible choices of :
Conclusion
We have shown:
By the principle of mathematical induction, 
 for all 
.
Therefore, the i-th component of the output of node  at layer  equals 
, the number of paths of length  from vertex 
to vertex . □
Great question! Let me work through the matrix representation of this GNN.
Matrix Representation
If we stack all node activations into a matrix where the i-th row contains the activation vector of node i, then:
Matrix H at layer k:
where the i-th row is 
, the activation of node  at layer .
From our initial setup:
 (the identity matrix, since node  starts with one-hot vector 
)
The Update Function
From part (a), we found that each node  computes:
A
​ =
ℓ,j
1
ℓ
j
ℓ∈V (j)
j
A
​ =
ℓ,j
0
(A
)
​ =
k+1
i,j
​L
​(i,ℓ)
ℓ∈V (j)
∑
k
L
​(i,j)
k+1
k + 1
i
j
k
i
ℓ
ℓ
j
ℓ
j
ℓ∈V (j)
k + 1
i
j
ℓ
L
​(i,ℓ)
k
ℓ
L
​(i,j) =
k+1
​L
​(i,ℓ)
ℓ∈V (j)
∑
k
(A
)
​ =
k+1
i,j
​L
​(i,ℓ) =
ℓ∈V (j)
∑
k
L
​(i,j)
k+1
(A )
​ =
k i,j
L
​(i,j)
k
k ≥0
j
k
L
​(i,j)
k
k
i
j
You asked:
(c) The GNN we have worked on so far is essentially linear, since the operations performed at each layer are permutation-invariant
locally at each node, and can be viewed as essentially doing the exact same thing at each vertex in the graph based on inputs
coming from its neighbors. This is called “aggregation” in the language of graph neural nets. If we represent the graph as a matrix,
with the activations of the (i)-th node in the (i)-th row, what is the update function? In the case of the computations in previous parts,
what is the update function that takes the aggregated inputs from neighbors and results in the output for this node?
Claude Replied:
H
∈
(k)
Rn×n
(h
​)
i
(k) T
i
k
H
=
(0)
I
​n
i
e
​i
j
Exported via AI Exporter
3 / 15 页
The proof looks ﬁne to me.


--- Page 4 ---
In matrix form, if we stack these column vectors as rows, we need to be careful about the transpose. Let's think about this more
carefully:
The j-th row of 
 is 
.
We have:
Taking the transpose:
This means the j-th row of 
 is a weighted sum of rows of 
, with weights from column  of 
.
Therefore, the update function is:
Or equivalently (and more commonly written):
if we use the convention that 
 represents the weight from node  to node  (which is the more standard GNN convention).
The Two-Step Process
In GNN terminology, the update at each layer consists of:
1. Aggregation: Each node collects (aggregates) information from its neighbors
Node  aggregates: 
In our case with weights: 
2. Update/Combine: Each node updates its representation based on the aggregated information
In our simple linear case: 
More generally in GNNs: 
In Our Specific Case
The update function is simply the identity function applied to the aggregated inputs:
There's no additional transformation—the output is exactly the weighted sum of neighbor activations. This is why it's a "deep linear
network."
In more sophisticated GNNs, you might have:
where 
 is a learnable weight matrix,  is a bias, and  is a nonlinear activation function.
h
​ =
j
(k+1)
​A
​ ⋅
i=1
∑
n
i,j
h
​
i
(k)
H(k+1)
(h
​)
j
(k+1) T
h
​ =
j
(k+1)
​A
​ ⋅
i=1
∑
n
i,j
h
​
i
(k)
(h
​)
=
j
(k+1) T
​A
​ ⋅
i=1
∑
n
i,j
(h
​)
i
(k) T
H(k+1)
H(k)
j
A
​
H
= A H
(k+1)
T
(k)
​
H
= AH
(k+1)
(k)
A
​
i,j
j
i
j
agg
​ =
j
​ h
​
∑i∈V (j)
i
(k)
agg
​ =
j
​ A
​ ⋅
∑i∈V (j)
i,j
h
​
i
(k)
h
​ =
j
(k+1)
agg
​j
h
=
j
(k+1)
UPDATE(h
​,agg
​)
j
(k)
j
​
h
​ = AGGREGATE {h
​ : i ∈V (j)}
=
​A
​ ⋅h
​
j
(k+1)
(
i
(k)
)
i∈V (j)
∑
i,j
i
(k)
h
​ =
j
(k+1)
σ
​W ⋅
​h
​ + b
​
i∈V (j)
∑
i
(k)
W
b
σ
You asked:
(d) The simple GNN described in the previous parts counts paths in the graph. If we were to replace sum  aggregation with max
aggregation, what is the interpretation of the outputs of node (j) at layer (k)?
Exported via AI Exporter
4 / 15 页
Again, this is correct.
This is not exactly what is written in the solution, 
but this is very close, just need to state that A_{i, j} 
= 1. I think this is the right answer.


--- Page 5 ---
Great question! Let me think through what happens when we replace sum aggregation with max aggregation.
Setup with Max Aggregation
With max aggregation, the update rule becomes:
where the max is taken component-wise across the n-dimensional vectors.
More explicitly, the -th component of 
 is:
Analysis
Layer 0:
 (one-hot encoding)
Layer 1:
The -th component: 
So 
 if and only if there exists a path of length 1 from  to  (i.e.,  is a neighbor of ).
Layer 2:
This equals 1 if there exists some 
 such that 
Which means there exists a neighbor  of  such that 
This is equivalent to: there exists a path of length 2 from  to 
General Pattern:
By induction, the -th component of 
 is:
Interpretation
With max aggregation, the output of node  at layer  is a binary indicator vector that tells us which nodes can reach node
 in exactly  steps.
In other words:
Sum aggregation counts: How many paths of length  exist from  to ?
Max aggregation detects: Does there exist any path of length  from  to ?
This is the difference between counting paths (sum) and detecting reachability (max). Max aggregation computes a form of k-hop
reachability in the graph—it answers the question "can I get from node  to node  in exactly  steps?" rather than "how many ways
can I get there?"
This connection shows why different aggregation functions in GNNs capture different structural properties of graphs!
Claude Replied:
h
​ =
j
(k+1)
​h
​
i∈V (j)
max
i
(k)
ℓ
h
​
j
(k+1)
[h
​]
​ =
j
(k+1)
ℓ
​[h
​]
​
i∈V (j)
max
i
(k)
ℓ
h
​ =
i
(0)
e
​i
h
​ =
j
(1)
max
​ e
​
i∈V (j)
i
ℓ
[h
​]
​ =
j
(1)
ℓ
max
​[e
​]
​ =
i∈V (j)
i ℓ
​
​
{1
0
if ℓ∈V (j)
otherwise
[h
​]
​ =
j
(1)
ℓ
1
ℓ
j
ℓ
j
[h
​]
​ =
j
(2)
ℓ
max
​[h
​]
​
i∈V (j)
i
(1)
ℓ
i ∈V (j)
[h
​]
​ =
i
(1)
ℓ
1
i
j
ℓ∈V (i)
ℓ
j
i
h
​
j
(k)
​
[h
​]
​ =
​
​
j
(k)
i
{1
0
if there exists at least one path of length k from i to j
otherwise
j
k
j
k
k
i
j
k
i
j
i
j
k
You asked:
(e) You are studying how organic molecules break down when heated. For each molecule, you know the element for each atom,
which other atoms it is connected to, the length of the bonds, and the type of molecule it is (carbohydrate, protein, etc.) You are
trying to predict which bond, if any, will break first if the molecule is heated. (i) How would you represent this as a graph? (What are
Exported via AI Exporter
5 / 15 页
This is correct


--- Page 6 ---
Great application problem! Let me work through how to formulate this molecular bond-breaking prediction as a graph learning task.
(i) Graph Representation
Nodes: Each atom in the molecule is a node.
Edges: Each chemical bond between atoms is an edge.
Undirected graph: Chemical bonds are symmetric relationships between atoms, so the graph should be undirected.
Edge features: Each edge should have features representing:
Bond length (given in the problem)
Bond type (single, double, triple, aromatic, etc.)
Possibly bond angle information if available
Global state/Graph-level features:
Molecule type (carbohydrate, protein, lipid, etc.) - can be one-hot encoded
Overall molecular properties (total molecular weight, pH environment if relevant, temperature, etc.)
Any experimental conditions
(ii) Using GNN Outputs for Prediction
After running the GNN for 
 layers, we have node embeddings 
 for each atom .
To predict which bond will break first, we need to make a prediction for each edge (bond). For each edge 
:
Edge representation: Combine the representations of the two endpoint nodes:
Common choices for COMBINE:
Concatenation: 
Sum: 
Element-wise product
More sophisticated: 
Prediction head: Apply a classifier to predict bond-breaking probability:
or for each edge independently:
where 
 is a score indicating the likelihood that bond 
 breaks first.
Then apply softmax across all edges to get a probability distribution over which bond breaks first:
Alternative: Include global state information:
where  is a global graph representation obtained by aggregating all node features:
Common readout functions: mean, max, sum, or attention-based pooling.
the nodes, edges, and global state representations? Is it directed or undirected?) (ii) How would you use the outputs of the last GNN
layer to make the prediction? (iii) How would you encode the node representation for the input to the GNN?
Claude Replied:
K
h
​
i
(K)
i
(i,j)
e
​ =
ij
COMBINE(h
​,h
​,edge_features
​)
i
(K)
j
(K)
ij
e
​ =
ij
[h
​∥h
​∥edge_features
​]
i
(K)
j
(K)
ij
e
​ =
ij
h
​ +
i
(K)
h
​
j
(K)
e
​ =
ij
MLP([h
​∥h
​∥edge_features
​])
i
(K)
j
(K)
ij
p
​ =
ij
softmax over all edges MLP(e
​)
(
ij )
s
​ =
ij
MLP(e
​)
ij
s
​
ij
(i,j)
p
​ =
ij
​
​ exp(s
​)
∑(k,ℓ)∈Edges
kℓ
exp(s
​)
ij
s
​ =
ij
MLP([e
​∥g])
ij
g
g = READOUT({h
​ :
i
(K)
i ∈nodes})
Exported via AI Exporter
6 / 15 页
This is correct.
Not sure why node embeddings should be 
involved, but it can certainly be one method.


--- Page 7 ---
(iii) Node Feature Encoding for Input
The initial node representation 
 for each atom  should encode:
1. Atom identity (element type):
One-hot encoding of element: [H, C, N, O, S, P, ...]
Or learnable embedding for each element type
Atomic number (can be normalized)
2. Atomic properties:
Atomic mass
Electronegativity
Number of valence electrons
Formal charge
Hybridization state (sp, sp², sp³)
Aromaticity (binary: is this atom part of an aromatic ring?)
3. Local structural features:
Degree (number of bonds/neighbors)
Number of hydrogen atoms attached
Whether the atom is in a ring
Ring size if applicable
4. Chemical context:
Implicit hydrogen count
Radical electrons if any
Example encoding:
This creates a feature vector of dimension, say, 50-100 for each atom.
Complete Pipeline Summary
1. Input: Molecular graph with atom features at nodes, bond features at edges, molecule type as global feature
2. GNN layers: Message passing for 
 layers to aggregate neighborhood information
3. Edge scoring: For each bond, combine endpoint node embeddings and edge features
4. Output: Softmax over all edges to predict which bond breaks first
This formulation allows the GNN to learn:
Local chemical environment around each bond
How strain and electron distribution affect bond stability
Which structural motifs are associated with bond breaking
How global molecular properties (like molecule type) affect the prediction
h
​
i
(0)
i
h
​ =
i
(0)
[one_hot(element)∥degree∥valence∥formal_charge∥hybridization∥aromatic∥in_ring]
K
You asked:
There are analogs of many ConvNet operations which can be done with GNNs. As Figure 1 illustrates, we can think of pixels as
nodes and pixel adjacencies as similar to edges. Graph-level classification tasks, for instance, are analogous to image classification,
since both produce a single, global prediction. Fill out the rest of the table. (Not all rows have a perfect answer. The goal is to think
about the role an operation serves in one architecture and whether you could use a technique which serves a similar role in the other
architecture.)
Exported via AI Exporter
7 / 15 页
I think this is thinking too much. It uses too 
much expertise knowledge and does not 
actually capture the spirit of the question. 
However, it could be an encoding.


--- Page 8 ---
Great exercise in drawing analogies between CNNs and GNNs! Let me fill out the table by thinking about what role each operation
serves and finding the corresponding analogue.
CNN
Image classification
Graph-level prediction probl
Node-level prediction proble
Color jitter data augmentation (adjusting the color or brightness of an image)
Node feature perturbation (a
Image flip data augmentation
Graph augmentation: node/edg
Channel dropout
Feature dropout (dropping no
Zero padding edges
Adding virtual nodes or self
ResNet skip connections
Skip connections between GNN 
jumping knowledge networks (
Blurring an image
Over-smoothing or adding mor
Predicting missing values of 
Reasoning for each analogy:
Color jitter perturbs pixel features → Node feature perturbation perturbs node features
Image flip creates a semantically equivalent but transformed input → Graph augmentation (edge/node dropping, permutation)
creates structurally similar graphs since graphs have no fixed orientation
Channel dropout randomly zeros out feature channels in CNNs → Feature dropout does the same for node features, or edge
dropout provides regularization by randomly removing graph structure
Zero padding handles boundary conditions in CNNs → Virtual nodes or self-loops help with boundary/connectivity issues in
graphs (virtual nodes allow global information aggregation; self-loops prevent information loss during aggregation)
ResNet skip connections preserve information across layers → Skip connections/Jumping knowledge in GNNs preserve node
information across message-passing layers, preventing over-smoothing
Blurring averages neighboring pixel values → Over-smoothing in deep GNNs makes neighboring node features increasingly
similar, or explicit graph diffusion operations smooth features across the graph
Predicting missing values is a node-level task unique to GNNs, similar to dense prediction tasks in CNNs but operating on
irregular graph structure
Ah yes, good catch! Let me fill in the CNN analogue for "Predicting missing values of nodes":
Claude Replied:
You asked:
Can you also fill out the blank in the CNN section?
Claude Replied:
Exported via AI Exporter
8 / 15 页
The Image ﬂipping part and the zero padding part are incorrect


--- Page 9 ---
CNN
Image classification
Graph-level predicti
Image segmentation or dense prediction tasks (predicting a label/value for each pixel)
Node-level predictio
Color jitter data augmentation (adjusting the color or brightness of an image)
Node feature perturb
Image flip data augmentation
Graph augmentation: 
Channel dropout
Feature dropout (dro
Zero padding edges
Adding virtual nodes 
ResNet skip connections
Skip connections betw
jumping knowledge ne
Blurring an image
Over-smoothing or ad
Image inpainting or denoising (predicting missing/corrupted pixel values)
Predicting missing v
Reasoning:
Image inpainting/denoising involves predicting missing or corrupted pixel values based on surrounding context → Predicting
missing node values uses the graph structure and neighboring nodes to infer missing node features
Both tasks involve filling in missing information using local context - in CNNs this is spatial neighborhoods of pixels, in GNNs this is
graph neighborhoods of nodes. Examples include:
CNN: filling in a masked region of an image, removing noise
GNN: imputing missing node attributes, predicting unknown properties of molecules/users in a social network
Great question! There are several strategies for handling missing node values in graph-level classification:
1. Imputation Before Graph-Level Prediction
First impute the missing node values, then use the complete graph for classification:
Mean/median imputation: Replace missing values with the mean or median of that feature across all nodes (either in this graph
or across the dataset)
GNN-based imputation: Train a separate GNN to predict missing node values based on:
Features of neighboring nodes
Graph structure
Available features of the node itself
Multi-task learning: Jointly train for both node-level imputation and graph-level classification
2. Masking/Indicator Features
Explicitly tell the GNN which values are missing:
Add indicator features: Augment each node's feature vector with binary flags indicating which features are missing
e.g., 
Set missing values to zero or a special value and let the indicator features inform the GNN how to handle them
The GNN can learn to weight information differently from nodes with missing data
3. Aggregation-Aware Approaches
Modify the GNN aggregation to handle missingness:
Attention mechanisms: Learn to down-weight nodes with missing features during message passing
You asked:
If you’re doing a graph-level classification problem, but node values are missing for some of your graph nodes, how would you use
this graph for prediction?
Claude Replied:
h
​ =
i
(0)
[features∥missing_mask]
Exported via AI Exporter
9 / 15 页
Correct


--- Page 10 ---
Robust aggregation functions: Use aggregation functions that are less sensitive to missing data (e.g., median instead of mean)
Weighted aggregation: Weight contributions based on how complete each node's features are
4. Architecture Modifications
Multiple imputation: Create multiple versions of the graph with different imputations, run predictions on each, and ensemble the
results
Uncertainty-aware models: Model uncertainty in the missing values and propagate this uncertainty through to the final
prediction
Variational approaches: Use variational GNNs that can handle missing data by marginalizing over possible values
5. During Training: Leverage Missingness as Regularization
If missingness occurs during training:
Treat as data augmentation: Randomly drop node features during training to make the model robust to missingness
Self-supervised pre-training: Pre-train the GNN to predict masked node features (similar to BERT), then fine-tune for graph
classification
Recommended Approach
For most practical cases, I'd recommend:
1. Add indicator features to mark which values are missing
2. Use a simple imputation strategy (like mean imputation or zero-filling) combined with the indicators
3. Let the GNN learn how to handle these nodes during message passing
This is simple, interpretable, and allows the model to learn which features are critical vs. which can be inferred from graph structure.
The key insight is that GNNs are well-suited to handle missing node features because they can leverage graph structure and
neighboring nodes to propagate information and compensate for missing values - this is actually one of the advantages of GNNs
over methods that treat nodes independently!
Great questions about the computational and architectural properties of GNNs!
(i) Scaling with Number of Nodes
Let's say we have a graph with  nodes, and we double it to 
 nodes.
Number of Learned Weights
Answer: The number of learned weights DOES NOT CHANGE.
Reasoning:
GNNs use weight sharing across all nodes - the same update function 
 is applied to every node
The learned parameters are in the update functions (MLPs, linear layers, etc.), not specific to individual nodes
Just like CNNs use the same convolutional filters across all spatial locations, GNNs use the same parameters for all nodes
You asked:
(h) Consider the graph neural net architecture shown in Figure 2. It includes representations of nodes (Vn), edges (En), and global
state (Un). At each timestep, each node and edge is updated by aggregating neighboring nodes/edges, as well as global state. The
global state is the updated by getting information from all nodes and edges. (i) If we double the number of nodes in a graph which
only has node representations, how does this change the number of learned weights in the graph? How does it change the amount
of computation used for this graph if the average node degree remains the same? What if the graph if fully connected? (Assume you
are not using a global state representation). (ii) Where in this network are learned weights/parameters? (iii) The diagram provided
shows undirected edges. How would you incorporate directed edges?
Claude Replied:
n
2n
f
​
V
Exported via AI Exporter
10 / 15 页
Good enough


--- Page 11 ---
Doubling nodes doesn't add new parameters, it just applies the existing parameters more times
Amount of Computation
This depends on the graph connectivity:
Case 1: Average node degree remains constant (sparse graph)
If each node has degree  on average:
Each node aggregates from  neighbors
With  nodes: computation is 
 since  is constant
With 
 nodes: computation is 
Answer: Computation DOUBLES (scales linearly with number of nodes)
Case 2: Fully connected graph
If the graph is fully connected:
Each node has degree 
 (connects to all other nodes)
With  nodes: computation is 
With 
 nodes: computation is 
Answer: Computation QUADRUPLES (scales quadratically with number of nodes)
Summary:
Learned weights: No change (weight sharing)
Computation (sparse): Doubles - 
Computation (fully connected): Quadruples - 
(ii) Where are the Learned Weights/Parameters?
Looking at Figure 2, learned parameters appear in:
1. Node Update Function 
Takes as input: aggregated neighbor information, current node features, possibly edge features and global state
Typically an MLP or neural network
Parameters: weight matrices and biases in this function
2. Edge Update Function 
Takes as input: features of connected nodes, current edge features, possibly global state
Typically an MLP
Parameters: weight matrices and biases
3. Global State Update Function 
Takes as input: aggregated node features, aggregated edge features, current global state
Typically an MLP
Parameters: weight matrices and biases
4. Aggregation Functions (sometimes)
In simple GNNs: aggregation is parameter-free (sum, mean, max)
In attention-based GNNs (like GAT): the attention mechanism has learned parameters to compute attention weights
e.g., 
 where 
 and  are learned
5. Pooling Function  (if parameterized)
Simple pooling (sum, mean, max): no parameters
Attention-based pooling or learned pooling: has parameters
Key insight: The same set of parameters is shared across all nodes/edges of the same type - this is what makes GNNs scalable
and permutation-invariant!
d
d
n
O(n ⋅d) = O(n)
d
2n
O(2n ⋅d) = O(2n)
n −1
n
O(n ⋅n) = O(n )
2
2n
O(2n ⋅2n) = O(4n )
2
O(n) →O(2n)
O(n ) →
2
O(4n )
2
f
​
V
​n
f
​
E
​n
f
​
Un
α
​ =
ij
softmax(a [Wh
​∥Wh
​])
T
i
j
W
a
ρ
Exported via AI Exporter
11 / 15 页
Correct
Correct


--- Page 12 ---
(iii) Incorporating Directed Edges
For directed edges, you need to distinguish between incoming and outgoing information flow. Here are several approaches:
Approach 1: Separate Aggregation for In-edges and Out-edges
For each node :
Aggregate separately from incoming neighbors 
 and outgoing neighbors 
Use different update functions or concatenate:
Approach 2: Separate Weight Matrices
Use different learned transformations for incoming vs. outgoing messages:
Approach 3: Edge Direction as Edge Feature
Treat edge direction as an edge feature:
Add a binary or categorical feature to 
 indicating direction
The edge update function 
 can learn to handle direction differently
When aggregating to nodes, the direction feature helps distinguish message sources
Approach 4: Reverse Edges
Create a graph with both:
Original directed edges with type "forward"
Reverse edges with type "backward"
Use edge types to learn different message functions
Example in the Figure 2 framework:
For directed edges, the edge update becomes:
And node updates aggregate with awareness of edge direction:
The key is to break the symmetry - make the model aware that information flow from node  to  is different from  to .
v
N
​(v)
in
N
​(v)
out
h
​ =
v
(k+1)
f
​ h
​,AGG
​{h
​ : u ∈N
​(v)},AGG
​{h
​ : u ∈N
​(v)}
V (
v
(k)
in
u
(k)
in
out
u
(k)
out
)
h
​ =
v
(k+1)
f
​
​h
​,
​W
​h
​,
​W
​h
​
​
V
v
(k)
u∈N
​(v)
in
∑
in
u
(k)
u∈N
​(v)
out
∑
out
u
(k)
E
​n
f
​
E
​n
E
​ =
n+1
(i,j)
f
​(E
​,V
​,V
​,U
​,direction)
E
​n
n
(i,j)
n
(i)
n
(j)
n
V
​ =
n+1
(j)
f
​ V
​,ρ
​({E
​ : i ∈N
​(j)}),ρ
​({E
​ : k ∈N
​(j)}),U
​
V
​n (
n
(j)
in
n
(i,j)
in
out
n
(j,k)
out
n)
i
j
j
i
You asked:
For an undirected graph with no labels on edges, the function that we compute at each layer of a Graph Neural Network must
respect certain properties so that the same function (with weight-sharing) can be used at different nodes in the graph. Let’s focus on
a single particular “layer” (\ell). For a given node (i) in the graph, let (s_i^{\ell-1}) be the self-message (i.e. the state computed at the
previous layer for this node) for this node from the preceeding layer, while the preceeding layer messages from the (n_i) neighbors of
node (i) are denoted by (m_{i,j}^{\ell-1}) where (j) ranges from 1 to (n_i). We will use (w) with subscripts and superscripts to denote
learnable scalar weights. If there’s no superscript, the weights are shared across layers. Assume that all dimensions work out. (a)
Tell which of these are valid functions for this node’s computation of the next self-message (s_i^\ell). For any choices that are not
valid, briefly point out why. Note: we are not asking you to judge whether these are useful or will have well behaved gradients.
Validity means that they respect the invariances and equivariances that we need to be able to deploy as a GNN on an undirected
graph. (i) [ s_i^\ell = w_1 s_i^{\ell-1} + w_2 \frac{1}{n_i} \sum_{j=1}^{n_i} m_{i,j}^{\ell-1} ] (ii) [ s_i^\ell = \max(w_1 s_i^{\ell-1},, w_2
m_{i,1}^{\ell-1},, w_3 m_{i,2}^{\ell-1}, \ldots, w_{n_i+1} m_{i,n_i}^{\ell-1}) ] where the max acts component-wise on the vectors. (iii) [
s_i^\ell = \max(w_1 s_i^{\ell-1},, w_2 m_{i,1}^{\ell-1},, w_2 m_{i,2}^{\ell-1}, \ldots, w_2 m_{i,n_i}^{\ell-1}) ] where the max acts
component-wise on the vectors. (b) We are given the following simple graph on which we want to train a GNN. The goal is binary
Exported via AI Exporter
12 / 15 页
Correct


--- Page 13 ---
Great problem! Let me work through each part systematically.
(a) Validity of Update Functions
For a GNN on an undirected graph to be valid, it must be:
Permutation invariant with respect to the ordering of neighbors (since the graph structure doesn't define an ordering)
Able to use weight sharing across all nodes regardless of their degree
(i) 
VALID ✓
The sum over neighbors is permutation invariant (reordering neighbors doesn't change the sum)
Uses fixed weights 
 regardless of node degree
The average aggregation 
 is permutation invariant and handles variable degree
(ii) 
INVALID ✗
Two major issues:
1. Different weights for different neighbors: Uses 
 - different weights for each neighbor position. This breaks
permutation invariance because it matters which neighbor is "first," "second," etc., but undirected graphs have no canonical
ordering of neighbors.
2. Number of parameters depends on degree: Nodes with different degrees would require different numbers of weights, making
weight sharing impossible across nodes with different degrees.
(iii) 
VALID ✓
All neighbor messages use the same weight 
The max operation is permutation invariant (reordering the arguments doesn't change the result)
Works for any node degree
This is essentially: 
(b) Training Loss Calculation
From Table 1 and Figure 3:
Training nodes: 2, 3, 5 (holding back 1 and 4)
Ground truth labels: 
Predicted probabilities: 
The average binary cross-entropy loss with 
 training points:
node classification (i.e. classifying the nodes as belonging to type 1 or 0) and we want to hold back nodes 1 and 4 to evaluate
performance at the end while using the rest for training. We decide that the surrogate loss to be used for training is the average
binary cross-entropy loss. Table 1 (see the attached image) gives you relevant information about the situation. Compute the training
loss at the end of training. Remember that with (n) training points, the formula for average binary cross-entropy loss is [ \frac{1}{n}
\sum_x \left( y(x) \log \frac{1}{\hat{y}(x)} + (1-y(x)) \log\left( \frac{1}{1-\hat{y}(x)} \right) \right) ] where the (x) in the sum ranges over
the training points and (\hat{y}(x)) is the network’s predicted probability that the label for point (x) is 1. (c) Suppose we decide to use
the following update rule for the internal state of the nodes at layer (\ell). [ s_i^\ell = s_i^{\ell-1} + W_1 \frac{\sum_{j=1}^{n_i}
\tanh!\left(W_2 m_{i,j}^{\ell-1}\right)}{n_i} \tag{1} ] where the (\tanh) nonlinearity acts element-wise. For a given node (i) in the graph,
let (s_i^{\ell-1}) be the self-message for this node from the preceding layer, while the preceding layer messages from the (n_i)
neighbors of node (i) are denoted by (m_{i,j}^{\ell-1}) where (j) ranges from 1 to (n_i). We will use (W) with subscripts and
superscripts to denote learnable weights in matrix form. If there’s no superscript, the weights are shared across layers. (i) Which of
the following design patterns does this update rule have? ☐ Residual connection ☐ Batch normalization (ii) If the dimension of the
state (s) is (d)-dimensional and (W_2) has (k) rows, what are the dimensions of the matrix (W_1)? (iii) If we choose to use the state
(s_i^{\ell-1}) itself as the message (m_i^{\ell-1}) going to all of node (i)’s neighbors, please write out the update rules corresponding
to (1) giving (s_i^\ell) for the graph in Figure 3 for nodes (i=2) and (i=3) in terms of information from earlier layers. Expand out all
sums.
Claude Replied:
s
​ =
i
ℓ
w
​s
​ +
1 i
ℓ−1
w
​
​
​ m
​
2 n
​i
1 ∑j=1
n
​i
i,j
ℓ−1
w
​,w
​
1
2
​
n
​i
1 ∑
s
​ =
i
ℓ
max(w
​s
, w
​m
​, w
​m
​, … , w
​m
​)
1 i
ℓ−1
2
i,1
ℓ−1
3
i,2
ℓ−1
n
​+1
i
i,n
​i
ℓ−1
w
​,w
​,… ,w
​
2
3
n
​+1
i
s
​ =
i
ℓ
max(w
​s
​, w
​m
​, w
​m
​, … , w
​m
​)
1 i
ℓ−1
2
i,1
ℓ−1
2
i,2
ℓ−1
2
i,n
​i
ℓ−1
w
​2
s
​ =
i
ℓ
max(w
​s
​,max
​ w
​m
​)
1 i
ℓ−1
j=1
n
​i
2
i,j
ℓ−1
y
​ =
2
1,y
​ =
3
1,y
​ =
5
0
​ =
y^2
b,
​
​ =
y^3
c,
​
​ =
y^5
e
n = 3
Exported via AI Exporter
13 / 15 页
Correct


--- Page 14 ---
Calculating for each node:
Node 2: 
Node 3: 
Node 5: 
Training Loss:
Or equivalently:
(c) Update Rule Analysis
Given: 
(i) Design Pattern
☑ Residual connection
The update has the form 
, which is exactly a residual/skip connection. The state from the previous layer
is added to the processed neighbor information.
☐ Batch normalization - Not present (no normalization across a batch)
(ii) Dimensions of 
State 
 is -dimensional
Messages 
 are also -dimensional (assuming messages have same dimension as states)
 has  rows, so 
 produces a -dimensional vector
After 
 and averaging, we have a -dimensional vector
 must map this -dimensional vector to a -dimensional vector to add to 
Answer: 
(iii) Expanded Update Rules
Given 
 (message equals state).
Looking at Figure 3:
Node 2 has neighbors: 1, 4, 5 (so 
)
Node 3 has neighbors: 4 (so 
)
For node 2:
For node 3:
Loss =
​
​ y
​ log
​ + (1 −y
​) log
​
3
1
i∈{2,3,5}
∑
[ i
​
​y^i
1
i
1 −
​
​y^i
1
]
y
​ =
2
1,
​
​ =
y^2
b
1 ⋅log
​ +
b
1
0 ⋅log
​ =
1 −b
1
−log b
y
​ =
3
1,
​
​ =
y^3
c
1 ⋅log
​ +
c
1
0 ⋅log
​ =
1 −c
1
−log c
y
​ =
5
0,
​
​ =
y^5
e
0 ⋅log
​ +
e
1
1 ⋅log
​ =
1 −e
1
−log(1 −e)
​
Loss =
​ −log b −log c −log(1 −e) = −
​ log b + log c + log(1 −e)
3
1 [
]
3
1 [
]
​
Loss = −
​ log bc(1 −e)
3
1
[
]
s
​ =
i
ℓ
s
​ +
i
ℓ−1
W
​
​
1
n
​i
​ tanh(W
​m
​)
∑j=1
n
​i
2
i,j
ℓ−1
s
​ =
i
ℓ
s
​ +
i
ℓ−1
f(neighbors)
W
​1
s
​
i
ℓ−1
d
m
​
i,j
ℓ−1
d
W
​2
k
W
​m
​
2
i,j
ℓ−1
k
tanh
k
W
​1
k
d
s
​
i
ℓ−1
​
W
​ has dimensions d × k
1
m
​ =
i
ℓ−1
s
​
i
ℓ−1
n
​ =
2
3
n
​ =
3
1
​
s
​ = s
​ + W
​
​
2
ℓ
2
ℓ−1
1
3
tanh(W
​s
​) + tanh(W
​s
​) + tanh(W
​s
​)
2 1
ℓ−1
2 4
ℓ−1
2 5
ℓ−1
Exported via AI Exporter
14 / 15 页
Correct
Correct
Correct


--- Page 15 ---
​
s
​ = s
​ + W
​
​ = s
​ + W
​ tanh(W
​s
​)
3
ℓ
3
ℓ−1
1
1
tanh(W
​s
​)
2 4
ℓ−1
3
ℓ−1
1
2 4
ℓ−1
Exported via AI Exporter
15 / 15 页
This is incorrect probably because Claude does not 
recognize image that well
