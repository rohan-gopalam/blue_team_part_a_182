--- Page 1 ---
Can you please the first 3 parts of this homework 
question?
Hereâ€™s a complete step-by-step solution to parts (a), (b), and (c) from the image titled
"DDPM/DDIM Fun: From a Gaussian":
(a) What is the distribution of 
?
We are told that the forward process starts at:
At each small interval 
, we add independent Gaussian noise 
. Let 
 be the number of steps from 
 to 
. Then, over 
 steps, the noise
accumulates.
X
â€‹1
X
â€‹ âˆ¼
0
N(0, Ïƒ )
2
Î”t
âˆ¼N(0, Î”t)
T =
â€‹
Î”t
1
t = 0
t = 1
T
ChatGPT 4o
Share
Add people
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
1/71
This was critical: I passed the question as an image because there is a lot of mathematics
 that gets wiped away if I copy and paste the raw text from the PDF 


--- Page 2 ---
Using the fact that the sum of independent Gaussians is another Gaussian with variances
adding up:
âœ… Final answer:
(b) What are the marginal distributions of 
 and 
?
Let the process be defined as:
Suppose the initial variable is 
, and we have a continuous-time
Brownian motion with additive Gaussian noise.
Letâ€™s define:
âœ… Final answer:
X
â€‹ =
1
X
â€‹ +
0
â€‹Ïµ
â€‹,
Ïµ
â€‹ âˆ¼
i=1
âˆ‘
T
i
i
N(0, Î”t)
Var(X
â€‹) =
1
Var(X
â€‹) +
0
T â‹…Î”t = Ïƒ +
2
1
X
â€‹ âˆ¼
1
N(0, Ïƒ +
2
1)
X
â€‹
tâˆ’Î”t
X
â€‹t
X
â€‹ =
t
X
â€‹ +
tâˆ’Î”t
Ïµ
â€‹,
Ïµ
â€‹ âˆ¼
t
t
N(0, Î”t)
X
â€‹ âˆ¼
0
N(0, Ïƒ )
2
Var(X
â€‹) =
t
Ïƒ +
2
t
Var(X
â€‹) =
tâˆ’Î”t
Ïƒ +
2
t âˆ’Î”t
X
â€‹ âˆ¼
tâˆ’Î”t
N(0, Ïƒ +
2
t âˆ’Î”t),
X
â€‹ âˆ¼
t
N(0, Ïƒ +
2
t)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
2/71


--- Page 3 ---
(c) Simplify and approximate the variance of the conditional distribution
We're given:
We are asked to simplify and approximate the variance term:
Under the assumption that 
, we can use a first-order approximation.
Letâ€™s write:
Since 
, the second term is very small:
âœ… Final answer (approximate variance):
So the conditional variance is approximately 
, matching our intuition that only a small bit
of uncertainty gets subtracted per step in the reverse process.
Let me know if you'd like part (d) or further intuition about DDPMs and reverse diffusion.
(X
â€‹ âˆ£
tâˆ’Î”t
X
â€‹ =
t
x
â€‹) âˆ¼
t
N
â€‹x
â€‹,
â€‹
(
Ïƒ + t
2
Ïƒ + t âˆ’Î”t
2
t
Ïƒ + t
2
(Ïƒ + t âˆ’Î”t)Î”t
2
)
Ïƒ + t
2
(Ïƒ + t âˆ’Î”t)Î”t
2
Î”t â‰ªÏƒ â‰ª
2
1
â€‹ =
Ïƒ + t
2
(Ïƒ + t âˆ’Î”t)Î”t
2
Î”t â‹…
â€‹
=
(
Ïƒ + t
2
Ïƒ + t âˆ’Î”t
2
)
Î”t â‹…
1 âˆ’
â€‹
â‰ˆ
(
Ïƒ + t
2
Î”t
)
Î”t
1 âˆ’
â€‹
(
Ïƒ + t
2
Î”t
)
Î”t â‰ª1
Var â‰ˆÎ”t
1 âˆ’
â€‹
â‰ˆ
(
Ïƒ + t
2
Î”t
)
Î”t
Î”t
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
3/71


--- Page 4 ---
Nice! Next, can you solve part D?
Absolutely! Letâ€™s work through Part (d) step-by-step:
ğŸ” Problem Recap
We are told:
Start with 
The forward diffusion is defined as: at each step, we add zero-mean Gaussian noise
with variance 
In reverse, instead of sampling from the full conditional 
, we only
apply the conditional mean mapping (i.e., we drop the stochastic part and only use
the mean)
Goal:
Find the resulting distribution of 
 after reversing from 
 using only the
conditional mean at each step
X
â€‹ âˆ¼
1
N(0, 1)
Î”t
p(X
â€‹ âˆ£
tâˆ’Î”t
X )
t
â€‹
X^0
X
â€‹1
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
4/71


--- Page 5 ---
ğŸ” Step 1: Recall the Conditional Mean
From part (c), the mean of 
 is:
Let us define:
Then, iteratively applying the conditional mean mapping without any noise, we get:
So, going backwards from 
 to 
 in steps of 
, the full mapping is a product:
ğŸ” Step 2: Analyze the Product
We want to evaluate:
This is a telescoping product!
Letâ€™s write out the first few terms:
X
â€‹ âˆ£
tâˆ’Î”t
X
â€‹ =
t
x
â€‹t
E[X
â€‹ âˆ£
tâˆ’Î”t
X
â€‹ =
t
x
â€‹] =
t
â€‹x
â€‹
Ïƒ + t
2
Ïƒ + t âˆ’Î”t
2
t
Î±
â€‹ :=
t
â€‹ =
Ïƒ + t
2
Ïƒ + t âˆ’Î”t
2
1 âˆ’
â€‹
Ïƒ + t
2
Î”t
â€‹ =
X^tâˆ’Î”t
Î±
â€‹
â€‹
tX^t
t = 1
t = 0
Î”t
â€‹ =
X^0
â€‹Î±
â€‹
X
â€‹
where
T =
(
k=1
âˆ
T
k)
1
â€‹
Î”t
1
â€‹ =
X^0
â€‹
â€‹
X
â€‹
(
k=1
âˆ
T
Ïƒ + 1 âˆ’(k âˆ’1)Î”t
2
Ïƒ + 1 âˆ’kÎ”t
2
)
1
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
5/71


--- Page 6 ---
So, everything cancels except:
âœ… Therefore:
ğŸ” Step 3: What is the Distribution of 
?
Since 
, and we multiply it by a constant:
This has mean 0 and variance:
ğŸ” Step 4: Approximate the Result for 
When 
, then:
â€‹
â€‹ =
k=1
âˆ
T
Ïƒ + 1 âˆ’(k âˆ’1)Î”t
2
Ïƒ + 1 âˆ’kÎ”t
2
â€‹ â‹…
Ïƒ + 1
2
Ïƒ + 1 âˆ’Î”t
2
â€‹ â‹…
Ïƒ + 1 âˆ’Î”t
2
Ïƒ + 1 âˆ’2Î”t
2
â€‹ â‹¯
â€‹
Ïƒ + 1 âˆ’2Î”t
2
Ïƒ + 1 âˆ’3Î”t
2
Ïƒ + Î”t
2
Ïƒ2
â€‹
Ïƒ + 1
2
Ïƒ2
â€‹ =
X^0
â€‹X
â€‹
Ïƒ + 1
2
Ïƒ2
1
â€‹
X^0
X
â€‹ âˆ¼
1
N(0, 1)
â€‹ âˆ¼
X^0
N
0,
â€‹
(
(Ïƒ + 1
2
Ïƒ2
)
2
)
Var(
â€‹) =
X^0
â€‹
(Ïƒ + 1
2
Ïƒ2
)
2
Ïƒ â‰ª
2
1
Ïƒ â‰ª
2
1
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
6/71


--- Page 7 ---
âœ… Final Answer:
ğŸ” Intuition and Takeaway
By skipping the stochastic noise during denoising, we collapse the uncertainty.
The resulting distribution becomes much narrower: the variance drops from 1 (at 
) to 
 at 
This shows why adding noise is critical during reverse diffusion: without it, you lose
diversity and collapse to a single mode.
Let me know if you'd like help with the next part or plotting this!
â€‹ â‰ˆ
Ïƒ + 1
2
Ïƒ2
Ïƒ
â‡’
2
â€‹
â‰ˆ
(Ïƒ + 1
2
Ïƒ2
)
2
Ïƒ4
â€‹ âˆ¼
X^0
N(0, Ïƒ )
4
X
â€‹1
Ïƒ â‰ª
4
1
â€‹
X^0
Nice! Now, can you solve part E?
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
7/71


--- Page 8 ---
Absolutely! Let's now solve part (e) of this DDPM/DDIM derivation.
ğŸ” Problem Summary
We're given the variance of 
 when we do add independent noise 
 at every
reverse diffusion step:
We are asked to:
1. Take the limit 
 (i.e., large 
)
2. Replace the sum with an integral
3. Evaluate the expression approximately, under the assumption:
âœ… Step 1: Replace the sum with an integral
Recall:
Apply this to equation (1):
â€‹
X^0
N(0, Î”t)
â€‹
Var(
â€‹) =
â€‹
+ Î”t
â€‹
X^0
(Ïƒ + 1
2
Ïƒ2
)
2
k=0
âˆ‘
Tâˆ’1
(Ïƒ + kÎ”t
2
Ïƒ2
)
2
(1)
Î”t â†’0
T = 1/Î”t
0 < Î”t â‰ªÏƒ â‰ª
2
1
Î”t
â€‹f(kÎ”t)
â€‹
k=0
âˆ‘
Tâˆ’1
Î”tâ†’0
â€‹f(t) dt
âˆ«
0
1
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
8/71


--- Page 9 ---
âœ… Step 2: Evaluate the integral
We need to evaluate:
Make the substitution:
So the integral becomes:
Now integrate:
âœ… Step 3: Simplify and combine
Now combine the two terms in the variance expression:
1st term:
Var(
â€‹) â‰ˆ
X^0
â€‹
+
(Ïƒ + 1
2
Ïƒ2
)
2
â€‹
â€‹
dt
âˆ«
0
1
(Ïƒ + t
2
Ïƒ2
)
2
â€‹
â€‹
dt
âˆ«
0
1
(Ïƒ + t
2
Ïƒ2
)
2
u = Ïƒ +
2
t
â‡’
du = dt,
asÂ t = 0 â‡’u = Ïƒ ,
t =
2
1 â‡’u = Ïƒ +
2
1
â€‹
â€‹
du =
âˆ«
Ïƒ2
Ïƒ +1
2
( u
Ïƒ2
)
2
Ïƒ
â€‹
â€‹du
4 âˆ«
Ïƒ2
Ïƒ +1
2
u2
1
= Ïƒ
âˆ’
â€‹
â€‹ =
4 [ u
1]
Ïƒ2
Ïƒ +1
2
Ïƒ
â€‹ âˆ’
â€‹
4 (Ïƒ2
1
Ïƒ + 1
2
1
)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
9/71


--- Page 10 ---
2nd term:
So:
âœ… Step 4: Approximate for 
Letâ€™s now approximate this assuming 
:
Term 1:
Term 2:
So:
âœ… Final Answer:
â€‹
=
(Ïƒ + 1
2
Ïƒ2
)
2
â€‹
(Ïƒ + 1)
2
2
Ïƒ4
Ïƒ
â€‹ âˆ’
â€‹
=
4 (Ïƒ2
1
Ïƒ + 1
2
1
)
Ïƒ
1 âˆ’
â€‹
=
2 (
Ïƒ + 1
2
Ïƒ2
)
Ïƒ
â€‹
2 (Ïƒ + 1
2
1
)
Var(
â€‹) â‰ˆ
X^0
â€‹ +
(Ïƒ + 1)
2
2
Ïƒ4
â€‹
Ïƒ + 1
2
Ïƒ2
Ïƒ â‰ª
2
1
Ïƒ â‰ª
2
1
â€‹ â‰ˆ
(Ïƒ + 1)
2
2
Ïƒ4
Ïƒ
(sinceÂ denominatorÂ  â‰ˆ
4
1)
â€‹ â‰ˆ
Ïƒ + 1
2
Ïƒ2
Ïƒ
(sinceÂ Ïƒ â‰ª
2
2
1)
Var(
â€‹) â‰ˆ
X^0
Ïƒ +
2
Ïƒ â‰ˆ
4
â€‹ â‰ˆ
Ïƒ (1 + Ïƒ )
2
2
â€‹
Ïƒ2
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
10/71


--- Page 11 ---
âœ¨ Intuition / Interpretation
This result tells us:
When we include noise at each reverse step, the final variance correctly recovers the
original variance of the data distribution 
This confirms that the DDPM reverse process works â€” as long as we preserve both
the mean and variance evolution over time
Let me know if you'd like a figure or derivation summary!
â€‹
Var(
â€‹) â‰ˆÏƒ
X^0
2
N(0, Ïƒ )
2
Nice! Now, please solve part f
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
11/71


--- Page 12 ---
Great! Let's now tackle Part (f), which is about using a neural network to estimate the
ideal predictor in a DDPM-style setup.
âœ… Objective Recap
We are given that the ideal denoiser at time  is the function:
We want to train a neural network to approximate this function.
You are asked:
ğŸ”¢ Step 1: What are the inputs to the neural net?
We want the neural network to learn the mapping:
So the inputs to the neural network are:
Noisy sample 
Time 
, typically scaled to [0, 1] and embedded (e.g., via sinusoidal or learned
embeddings)
ğŸ‘‰ Inputs to the neural net: A batch of tuples 
t
g(x
â€‹, t) =
t
â€‹x
â€‹
Ïƒ + t
2
Ïƒ2
t
If you had a dataset of samples 
 drawn from the target distribution
, how would you train a neural net to learn this function?
s
â€‹, s
â€‹, â€¦ , s
â€‹
1
2
n
X
â€‹ âˆ¼
0
N(0, Ïƒ )
2
(x
â€‹, t) â†¦
t
â€‹
x^0
x
â€‹ âˆˆ
t
Rd
t âˆˆR
(x
â€‹, t)
t
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
12/71


--- Page 13 ---
ğŸ” Step 2: How do you generate a batch of them?
We assume you are given clean samples 
To create training pairs 
:
1. Sample a timestep 
 for each clean sample
2. Add forward Gaussian noise to simulate the DDPM forward process:
This makes 
, consistent with the forward process.
3. The input to the network is 
, and the target is:
This gives you a batch of training data: 
ğŸ“‰ Step 3: How do you compute the loss?
The neural net is trained to output an estimate 
So we use a standard MSE loss:
This loss can then be used with an optimizer like AdamW to update the neural net
parameters.
s
â€‹ âˆ¼
i
X
â€‹ âˆ¼
0
N(0, Ïƒ )
2
(x
â€‹, t, x
â€‹)
t
0
t âˆ¼Uniform(0, 1)
x
â€‹ =
t
s
â€‹ +
i
Ïµ,
Ïµ âˆ¼N(0, t)
x
â€‹ âˆ¼
t
N(0, Ïƒ +
2
t)
(x
â€‹, t)
t
x
â€‹ =
0
â‹†
â€‹x
â€‹
Ïƒ + t
2
Ïƒ2
t
{(x
â€‹, t
, x
â€‹)}
â€‹
t
(i)
(i)
0
â‹†(i)
i=1
B
â€‹ â‰ˆ
x^0
x
â€‹0
â‹†
L =
â€‹
â€‹
â€‹NN(x
â€‹, t
) âˆ’x
â€‹
â€‹
â€‹
B
1
i=1
âˆ‘
B
t
(i)
(i)
0
â‹†(i)
2
2
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
13/71


--- Page 14 ---
âœ… Final Answer (in words)
To train a neural network to estimate the function 
:
1. Inputs to the network are pairs 
, where:
, with 
, and 
2. To generate a batch, sample 
, choose timestep 
, and compute:
3. Loss: Use MSE between the predicted 
 and the true 
:
Then use any optimizer (e.g. AdamW) to minimize this loss and train the model.
Let me know if you'd like the PyTorch code for this or a visual diagram!
g(x
â€‹, t) =
t
â€‹x
â€‹
Ïƒ +t
2
Ïƒ2
t
(x
â€‹, t)
t
x
â€‹ =
t
x
â€‹ +
0
Ïµ
x
â€‹ âˆ¼
0
N(0, Ïƒ )
2
Ïµ âˆ¼N(0, t)
t âˆ¼Uniform(0, 1)
x
â€‹
0
(i)
t(i)
x
â€‹ =
t
(i)
x
â€‹ +
0
(i)
Ïµ
,
target:Â x
â€‹ =
(i)
0
â‹†(i)
â€‹x
â€‹
Ïƒ + t
2
(i)
Ïƒ2
t
(i)
â€‹
x^0
x
â€‹0
â‹†
L = E
â€‹
â€‹NN(x
â€‹, t) âˆ’
â€‹x
â€‹
â€‹
x
â€‹,t
t
[
t
Ïƒ + t
2
Ïƒ2
t
2
]
Can you please solve part g now?
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
14/71


--- Page 15 ---
Absolutely! Letâ€™s solve Part (g) â€” which asks you to approximate the DDIM step when
, using the known analytical form of 
, but without assuming
anything about 
.
âœ… Step 1: Recall the DDIM update rule
The DDIM step is given by:
We are told 
, so:
So the update becomes:
âœ… Step 2: Approximate 
 for small 
From earlier parts, we know:
Î”t â‰ªt
g(x , t) =
t
â€‹x
â€‹
Ïƒ +t
2
Ïƒ2
t
Ïƒ â‰ª
2
t
x
â€‹ =
tâˆ’Î”t
DDIM
x
â€‹ +
t
Î·(t, Î”t) â‹…
g(x
â€‹, t) âˆ’x
â€‹
(
t
t)
g(x , t) =
t
â€‹x
â€‹
Ïƒ +t
2Ïƒ2
t
g(x
â€‹, t) âˆ’
t
x
â€‹ =
t
â€‹ âˆ’1
x
â€‹ =
(Ïƒ + t
2
Ïƒ2
)
t
âˆ’
x
â€‹
( Ïƒ + t
2
t
)
t
x
â€‹ =
tâˆ’Î”t
DDIM
x
â€‹ âˆ’
t
Î·(t, Î”t) â‹…
â€‹
x
â€‹ =
(Ïƒ + t
2
t
)
t
x
â€‹
1 âˆ’Î·(t, Î”t) â‹…
â€‹
t (
Ïƒ + t
2
t
)
Î·(t, Î”t)
Î”t
Î·(t, Î”t) =
â€‹ =
â€‹+
â€‹
t âˆ’Î”t
t
â€‹t
1 +
â€‹
(
1 âˆ’
â€‹t
Î”t)
âˆ’1
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
15/71


--- Page 16 ---
Now for 
, use the first-order Taylor expansion:
Then:
So to first order:
âœ… Step 3: Final DDIM Step Approximation
Substitute 
 into the update rule:
âœ… Final Answer:
Î”t â‰ªt
â€‹ â‰ˆ
t âˆ’Î”t
â€‹
1 âˆ’
â€‹
t (
2t
Î”t)
Î·(t, Î”t) =
â‰ˆ
â€‹+
t âˆ’Î”t
t
â€‹t
â€‹ =
â€‹ 1 âˆ’
â€‹ +
â€‹
t (
2t
Î”t)
t
â€‹t
â€‹ â‰ˆ
2 âˆ’
â€‹
2t
Î”t
1
â€‹ +
2
1
â€‹
8t
Î”t
â€‹
Î·(t, Î”t) â‰ˆ
â€‹2
1
Î·(t, Î”t) â‰ˆ
â€‹2
1
x
â€‹ â‰ˆ
tâˆ’Î”t
DDIM
x
â€‹
1 âˆ’
â€‹ â‹…
â€‹
=
t (
2
1
Ïƒ + t
2
t
)
x
â€‹
â€‹
=
t (
Ïƒ + t
2
Ïƒ + t âˆ’
â€‹
2
2
t
)
x
â€‹
â€‹
t ( Ïƒ + t
2
Ïƒ +
â€‹
2
2
t
)
â€‹
x
â€‹ â‰ˆx
â€‹ â‹…
â€‹
tâˆ’Î”t
DDIM
t
( Ïƒ + t
2
Ïƒ +
â€‹
2
2
t
)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
16/71


--- Page 17 ---
ğŸ§  Interpretation:
This DDIM update shrinks the signal toward zero, depending on both  and 
For large , this ratio becomes close to 
, as expected
For small , itâ€™s closer to 1, meaning less denoising is needed early on
Let me know if you want this turned into pseudocode or extended to higher-order terms!
t
Ïƒ2
t
â€‹2
1
t
Are you sure? I think you get the 1/2 term correctly but try 
checking that the other term is correct
You're right to be skeptical â€” letâ€™s double-check the derivation of the DDIM update step
in Part (g) and rederive it carefully, keeping track of all approximations.
âœ… Step 1: DDIM Update Rule
From part (f), the DDIM step is:
And from the problem setup:
So the update becomes:
x
â€‹ =
tâˆ’Î”t
DDIM
x
â€‹ +
t
Î·(t, Î”t) â‹…
g(x
â€‹, t) âˆ’x
â€‹
(
t
t)
g(x
â€‹, t) =
t
â€‹x
â€‹
â‡’
Ïƒ + t
2
Ïƒ2
t
g(x
â€‹, t) âˆ’
t
x
â€‹ =
t
â€‹ âˆ’1
x
â€‹ =
(Ïƒ + t
2
Ïƒ2
)
t
âˆ’
â€‹x
â€‹
Ïƒ + t
2
t
t
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
17/71
It made an error here where it vastly over complicated the question so I had to 
intervene. Note: in the later parts, it does this quite frequently


--- Page 18 ---
âœ… Step 2: Approximate 
 for small 
Recall the exact expression:
As before, expand 
 using Taylor:
Then:
So up to first order:
âœ… Step 3: Plug this into the DDIM update
We have:
x
â€‹ =
tâˆ’Î”t
DDIM
x
â€‹ âˆ’
t
Î·(t, Î”t) â‹…
â€‹x
â€‹ =
Ïƒ + t
2
t
t
x
â€‹
1 âˆ’Î·(t, Î”t) â‹…
â€‹
t (
Ïƒ + t
2
t
)
Î·(t, Î”t)
Î”t â‰ªt
Î·(t, Î”t) =
â€‹
â€‹+
â€‹
t âˆ’Î”t
t
â€‹t
â€‹
t âˆ’Î”t
â€‹ =
t âˆ’Î”t
â€‹
1 âˆ’
â€‹ + O(Î”t )
t (
2t
Î”t
2 )
Î·(t, Î”t) =
â€‹ =
â€‹+
â€‹
t âˆ’Î”t
t
â€‹t
â€‹ =
1 âˆ’
â€‹ + 1
(
2t
Î”t)
1
â€‹ =
2 âˆ’
â€‹
2t
Î”t
1
â€‹
1 +
â€‹ + O(Î”t )
2
1 (
4t
Î”t
2 )
â€‹
Î·(t, Î”t) â‰ˆ
â€‹ +
â€‹
2
1
8t
Î”t
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
18/71


--- Page 19 ---
Substitute in 
:
âœ… Final Answer
This includes the correct linear correction in 
 and avoids assuming anything about
the size of 
 relative to .
ğŸ§  Interpretation
When 
, the factor becomes 
, as expected.
When 
, it reduces to 
, which aligns with the intuition that DDIM steps
shrink 
 over time.
Thanks for catching that detail â€” let me know if you'd like this visualized or implemented
in code!
x
â€‹ =
tâˆ’Î”t
DDIM
x
â€‹
1 âˆ’Î·(t, Î”t) â‹…
â€‹
t (
Ïƒ + t
2
t
)
Î·(t, Î”t) â‰ˆ
â€‹ +
2
1
8t
Î”t
1 âˆ’
â€‹ +
â€‹
â‹…
(2
1
8t
Î”t)
â€‹ =
Ïƒ + t
2
t
1 âˆ’
â€‹ +
â€‹
(2(Ïƒ + t)
2
t
8(Ïƒ + t)
2
Î”t
)
=
â€‹ =
Ïƒ + t
2
Ïƒ + t âˆ’
â€‹ âˆ’
â€‹
2
2
t
8
Î”t
â€‹
Ïƒ + t
2
Ïƒ +
â€‹ âˆ’
â€‹
2
2
t
8
Î”t
â€‹
x
â€‹ â‰ˆx
â€‹ â‹…
â€‹
tâˆ’Î”t
DDIM
t
(
Ïƒ + t
2
Ïƒ +
â€‹ âˆ’
â€‹
2
2
t
8
Î”t
)
Î”t
Ïƒ2
t
Î”t â†’0
â€‹
Ïƒ +t
2
Ïƒ +t/2
2
Ïƒ â†’
2
0
â€‹ âˆ’
2
1
â€‹
8t
Î”t
x
â€‹t
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
19/71


--- Page 20 ---
Try starting your answer in this form:
Great! Given your preferred starting point, letâ€™s begin with the DDIM step in the following
form:
Now we will approximate this expression under the assumption that:
No assumption is made about 
 vs. 
âœ… Step 1: Approximate 
Recall:
Use the first-order Taylor approximation:
x
â€‹ =
tâˆ’Î”t
DDIM
1 âˆ’
â€‹
x
â€‹
(
Ïƒ + t
2
Î·(t, Î”t)Î”t)
t
Î”t â‰ªt
Ïƒ2
t
Î·(t, Î”t)
Î·(t, Î”t) =
â€‹
â€‹+
â€‹
t âˆ’Î”t
t
â€‹t
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
20/71


--- Page 21 ---
So we substitute:
âœ… Step 2: Plug into the DDIM update
We now substitute this approximation into the original formula:
Now simplify the expression:
âœ… Final Answer (boxed)
This expression is accurate up to second order in 
, and makes no assumption about
the size of 
 relative to , as requested.
â€‹ â‰ˆ
t âˆ’Î”t
â€‹
1 âˆ’
â€‹
â‡’
t (
2t
Î”t)
Î·(t, Î”t) =
â€‹ â‰ˆ
â€‹+
â€‹
t âˆ’Î”t
t
â€‹t
â€‹ =
2 âˆ’
â€‹
2t
Î”t
1
â€‹ +
2
1
â€‹
8t
Î”t
Î·(t, Î”t) â‰ˆ
â€‹ +
2
1
â€‹
8t
Î”t
x
â€‹ =
tâˆ’Î”t
DDIM
1 âˆ’
â€‹
x
â€‹ =
(
Ïƒ + t
2
â€‹ +
â€‹ Î”t
( 2
1
8t
Î”t)
)
t
1 âˆ’
â€‹ +
â€‹
x
â€‹
(
[2(Ïƒ + t)
2
Î”t
8t(Ïƒ + t)
2
Î”t2
])
t
x
â€‹ =
tâˆ’Î”t
DDIM
â€‹
x
â€‹
(
Ïƒ + t
2
Ïƒ + t âˆ’
â€‹ âˆ’
â€‹
2
2
Î”t
8t
Î”t2
)
t
â€‹
x
â€‹ â‰ˆ
â€‹
x
â€‹
tâˆ’Î”t
DDIM
(
Ïƒ + t
2
Ïƒ + t âˆ’
â€‹ âˆ’
â€‹
2
2
Î”t
8t
Î”t2
)
t
Î”t
Ïƒ2
t
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
21/71


--- Page 22 ---
Let me know if youâ€™d like the same result in a LaTeX format or as a function in PyTorch!
Ok nice! now, try factoring the terms out
Great! Letâ€™s now take our boxed expression from before and factor it for cleaner
interpretation.
ğŸ” Starting Expression
We previously derived:
ğŸ”§ Step 1: Factor out 
Letâ€™s write:
We can now factor out 
 from the numerator:
x
â€‹ â‰ˆ
tâˆ’Î”t
DDIM
â€‹
x
â€‹
(
Ïƒ + t
2
Ïƒ + t âˆ’
â€‹ âˆ’
â€‹
2
2
Î”t
8t
Î”t2
)
t
â€‹
Ïƒ +t
21
x
â€‹ â‰ˆ
tâˆ’Î”t
DDIM
1 âˆ’
â€‹
x
â€‹
(
Ïƒ + t
2
â€‹ +
â€‹
2
Î”t
8t
Î”t2
)
t
â€‹
2(Ïƒ +t)
2
Î”t
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
22/71


--- Page 23 ---
âœ… Final Factored Answer:
ğŸ§  Interpretation:
This is now factored into a clean step size term that controls how much the update
shrinks 
The expression clearly shows how:
The update shrinks more with larger 
The curvature correction (the 
 term) becomes significant only if  is very small
Let me know if you want a comparison with Euler steps or a plot showing this scaling
behavior!
â€‹ =
Ïƒ + t
2
â€‹ +
â€‹
2
Î”t
8t
Î”t2
â€‹
1 +
â€‹
2(Ïƒ + t)
2
Î”t
(
4t
Î”t)
â€‹
x
â€‹ â‰ˆ
1 âˆ’
1 +
â€‹
x
â€‹
tâˆ’Î”t
DDIM
(
2(Ïƒ + t)
2
Î”t
(
4t
Î”t))
t
x
â€‹t
Î”t
Î”t2
t
Ok yeah that's basically correct. Now, try solving part F
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
23/71
The ï¬nal solution doesnâ€™t exactly match the answer sheet but itâ€™s the same thing, just written differently!
A cool thing about 4o is that it always tries to 
extract a broader takeaway at the end of an explanation
â€”> good learning tool :)


--- Page 24 ---
Letâ€™s solve Part (h) step-by-step.
ğŸ” Problem Overview
We are to compute the distribution of:
by applying 
 DDIM steps, each of which shrinks 
 by a factor of:
From part (g) and (factored form in part g'), this becomes (up to second order in 
):
âœ… Step 1: Express total contraction as a product
Let the contraction from 
 be:
We're applying 
 steps in reverse from 
, so this product represents
the total
â€‹ =
X^0
DDIM-step-reversed(
â€‹),
â€‹ âˆ¼
X^1
X^1
N(0, 1)
T =
â€‹
Î”t
1
x
â€‹t
x
â€‹ =
tâˆ’Î”t
1 âˆ’
â€‹
x
â€‹
(
Ïƒ + t
2
Î·(t, Î”t)Î”t)
t
Î”t
x
â€‹ â‰ˆ
tâˆ’Î”t
1 âˆ’
â€‹
1 +
â€‹
x
â€‹
(
2(Ïƒ + t)
2
Î”t
(
4t
Î”t))
t
t = 1 â†’0
â€‹ =
X^0
â€‹ â‹…
X^1
â€‹
1 âˆ’
â€‹
1 +
â€‹
,
t
â€‹ =
k=0
âˆ
Tâˆ’1
(
2(Ïƒ + t
â€‹)
2
k
Î”t
(
4t
â€‹k
Î”t ))
k
1 âˆ’kÎ”t
T = 1/Î”t
t = 1 â†’0
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
24/71


--- Page 25 ---
Sorry, try solving part H. My bad I made a mistake in what 
I called it.
No worries â€” letâ€™s solve Part (h) now!
ğŸ§  Goal:
Given a sample 
, we apply 
 DDIM steps from time 
,
each of which contracts 
, and we want to compute the final distribution of 
 after all
the steps.
We'll:
1. Express total contraction as a product
2. Take logs to convert to a sum
3. Take the 
 limit (i.e. Riemann sum â†’ integral)
4. Recover the variance of 
 in closed form
â€‹ âˆ¼
X^1
N(0, 1)
T =
â€‹
Î”t
1
t = 1 â†’0
x
â€‹t
â€‹
X^0
Î”t â†’0
â€‹
X^0
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
25/71


--- Page 26 ---
âœ… Step 1: DDIM Contraction Per Step
From Part (g) and (factored form), each step contracts the signal by:
So applying this over 
 steps from 
, the total contraction is:
âœ… Step 2: Take Logs
Take logarithms:
Use the first-order expansion 
 for small :
âœ… Step 3: Convert Sum to Integral
Now let 
, turning the sum into a Riemann integral:
x
â€‹ =
tâˆ’Î”t
DDIM
1 âˆ’
â€‹
x
â€‹
(toÂ firstÂ orderÂ inÂ Î”t)
(
2(Ïƒ + t)
2
Î”t
)
t
T = 1/Î”t
t = 1 â†’0
â€‹ =
X^0
â€‹ â‹…
X^1
â€‹
1 âˆ’
â€‹
,
whereÂ t
â€‹ =
k=0
âˆ
Tâˆ’1
(
2(Ïƒ + t
â€‹)
2
k
Î”t
)
k
1 âˆ’kÎ”t
ln
â€‹ =
X^0
ln
â€‹ +
X^1
â€‹ln
1 âˆ’
â€‹
k=0
âˆ‘
Tâˆ’1
(
2(Ïƒ + t
â€‹)
2
k
Î”t
)
ln(1 âˆ’x) â‰ˆâˆ’x
x
â‰ˆln
â€‹ âˆ’
X^1
â€‹
â€‹
k=0
âˆ‘
Tâˆ’1
2(Ïƒ + t
â€‹)
2
k
Î”t
Î”t â†’0
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
26/71


--- Page 27 ---
This gives:
Exponentiate:
âœ… Step 4: Final Distribution of 
Since 
, multiplying by a constant gives:
ğŸ§ª Sanity Check: Assume 
If 
, then:
â€‹
â€‹ âŸ¶
k=0
âˆ‘
Tâˆ’1
2(Ïƒ + t
â€‹)
2
k
Î”t
â€‹
â€‹dt
âˆ«
0
1
2(Ïƒ + t)
2
1
ln
â€‹ =
X^0
ln
â€‹ âˆ’
X^1
â€‹
â€‹dt
âˆ«
0
1
2(Ïƒ + t)
2
1
= ln
â€‹ âˆ’
X^1
â€‹ ln(Ïƒ + t)
â€‹ =
[2
1
2
]
0
1
ln
â€‹ âˆ’
X^1
â€‹ ln
â€‹
2
1
(
Ïƒ2
Ïƒ + 1
2
)
â€‹ =
X^0
â€‹ â‹…
X^1
exp
âˆ’
â€‹ ln
â€‹
=
( 2
1
(
Ïƒ2
Ïƒ + 1
2
))
â€‹ â‹…
X^1
â€‹
(Ïƒ + 1
2
Ïƒ2
)
1/2
â€‹
X^0
â€‹ âˆ¼
X^1
N(0, 1)
â€‹
â€‹ âˆ¼N
0,
â€‹
X^0
(
Ïƒ + 1
2
Ïƒ2
)
Ïƒ â‰ª
2
1
Ïƒ â‰ª
2
1
â€‹ â‰ˆ
Ïƒ + 1
2
Ïƒ2
Ïƒ2
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
27/71


--- Page 28 ---
âœ… Which matches the expected answer:
âœ… Final Boxed Result:
Let me know if you want a visual intuition or a derivation for the mean as well!
â€‹
â€‹ âˆ¼N(0, Ïƒ )
X^0
2
â€‹
â€‹ =
â€‹
â‹…
â€‹
so
â€‹ âˆ¼N
0,
â€‹
X^0
(Ïƒ + 1
2
Ïƒ2
)
1/2
X^1
X^0
(
Ïƒ + 1
2
Ïƒ2
)
Wow! That was a tough one. Now, can you help me solve 
the second question on the homework? It's very long, so 
be sure to read all of it. Then, solve part a
Absolutely! Let's walk through Question 2 (a) step-by-step.
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
28/71
It was able to solve question 1 with almost no issues!
What is remarkable is that it doesnâ€™t use thinking mode 
so itâ€™s not checking its work and its basing its answers off of
what it thinks is the best answer at a ï¬rst pass. I only had to nudge it once


--- Page 29 ---
âœ… Problem Summary
We're given an optimization problem:
where:
and the question is:
ğŸ§  Intuition
KL divergence 
 is always non-negative and equals zero if and only if
 for all .
Thus, weâ€™re minimizing a function whose global minimum is known:
âœ… Final Answer:
â€‹ D
â€‹(pâˆ¥q)
pâˆˆP
min
KL
D
â€‹(pâˆ¥q) =
KL
â€‹p(x) log
â€‹
x
âˆ‘
(q(x)
p(x))
What is the optimal 
 that minimizes 
?
pâˆ—
D
â€‹(pâˆ¥q)
KL
D
â€‹(pâˆ¥q)
KL
p(x) = q(x)
x
D
â€‹(pâˆ¥q) =
KL
0
whenÂ p(x) = q(x)
â€‹
p (x) = q(x)
âˆ—
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
29/71


--- Page 30 ---
This is the unique minimizer, since KL divergence is convex in  and achieves its
minimum when 
.
ğŸ§ª Sanity Check
If we plug in 
, then:
âœ… Confirmed.
Let me know when youâ€™re ready to move on to part (b) â€” it looks like weâ€™ll be diving into
Direct Preference Optimization!
p
p = q
p (x) =
âˆ—
q(x)
D
â€‹(p âˆ¥q) =
KL
âˆ—
â€‹q(x) log
â€‹
=
x
âˆ‘
(q(x)
q(x))
â€‹q(x) â‹…
x
âˆ‘
0 = 0
Nice, you got that right! Now, try solving part b
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
30/71
While thinking mode isnâ€™t enabled, it usually always checks its answer.
However, I havenâ€™t seen a case where it does a check and then realizes the answer is wrong
so iâ€™m wondering what happens in that caseâ€¦


--- Page 31 ---
Excellent! Letâ€™s solve Part (b).
ğŸ” Problem Setup
We are asked to maximize the following objective over 
:
Our goal is to derive the form of the optimal policy 
, and show it equals:
âœ… Step-by-step Derivation
Weâ€™ll treat this as a constrained optimization problem over the space of conditional
distributions 
, for fixed .
We'll fix  and drop the -dependence for clarity during derivation, and restore it later.
ğŸ¯ Objective:
We include a Lagrange multiplier  to enforce the constraint 
:
Ï€
â€‹(y âˆ£
Î¸
x)
â€‹ E
â€‹ r
â€‹(x, y)
âˆ’
Ï€
â€‹Î¸
max
xâˆ¼D, yâˆ¼Ï€
â€‹(yâˆ£x)
Î¸
[ Ï•
]
Î² D
â€‹ Ï€
â€‹(yâˆ£x) âˆ¥Ï€
â€‹(yâˆ£x)
KL [
Î¸
ref
]
Ï€
â€‹(yâˆ£x)
Î¸
âˆ—
â€‹
Ï€
â€‹(yâˆ£x) =
â€‹Ï€
â€‹(yâˆ£x) exp
â€‹r
â€‹(x, y)
Î¸
âˆ—
Z(x)
1
ref
(Î²
1
Ï•
)
Ï€
â€‹(y âˆ£
Î¸
x)
x
x
x
L[Ï€] =
â€‹Ï€(y)r
â€‹(x, y) âˆ’
y
âˆ‘
Ï•
Î²
â€‹Ï€(y) log
â€‹
y
âˆ‘
Ï€
â€‹(y)
ref
Ï€(y)
Î»
â€‹Ï€(y) =
âˆ‘y
1
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
31/71


--- Page 32 ---
ğŸ§® Take the functional derivative and set to zero
We differentiate 
 w.r.t. 
:
Rearranging:
Exponentiate both sides:
Therefore:
J [Ï€, Î»] =
â€‹Ï€(y)r
â€‹(x, y) âˆ’
y
âˆ‘
Ï•
Î²
â€‹Ï€(y) log
â€‹ âˆ’
y
âˆ‘
Ï€
â€‹(y)
ref
Ï€(y)
Î»
â€‹Ï€(y) âˆ’1
(
y
âˆ‘
)
J
Ï€(y)
â€‹ =
âˆ‚Ï€(y)
âˆ‚J
r
â€‹(x, y) âˆ’
Ï•
Î²
log
â€‹ + 1
âˆ’
(
Ï€
â€‹(y)
ref
Ï€(y)
)
Î» = 0
log
â€‹ =
Ï€
â€‹(y)
ref
Ï€(y)
â€‹
r
â€‹(x, y) âˆ’Î» âˆ’Î²
Î²
1 ( Ï•
)
â€‹ =
Ï€
â€‹(y)
ref
Ï€(y)
exp
(r
â€‹(x, y) âˆ’Î» âˆ’Î²)
=
(Î²
1
Ï•
)
C â‹…
exp
â€‹r
â€‹(x, y)
whereÂ C =
(Î²
1
Ï•
)
exp
âˆ’
â€‹
(
Î²
Î» + Î² )
Ï€(y) = C â‹…Ï€
â€‹(y) â‹…
ref
exp
â€‹r
â€‹(x, y)
(Î²
1
Ï•
)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
32/71


--- Page 33 ---
ğŸ§® Enforce normalization: 
where:
âœ… Final Answer:
Which is exactly what we were asked to show.
Let me know when youâ€™re ready for part (c) â€” weâ€™ll likely begin to connect this to Direct
Preference Optimization!
â€‹Ï€(y) =
âˆ‘y
1
1 =
â€‹Ï€(y) =
y
âˆ‘
C
â€‹Ï€
â€‹(y) exp
r
â€‹(x, y)
â‡’
y
âˆ‘
ref
(Î²
1
Ï•
)
C =
â€‹
Z(x)
1
Z(x) =
â€‹Ï€
â€‹(y âˆ£
y
âˆ‘
ref
x) exp
â€‹r
â€‹(x, y)
(Î²
1
Ï•
)
â€‹
Ï€
â€‹(y âˆ£x) =
Ï€
â€‹(y âˆ£x) â‹…exp
â€‹r
â€‹(x, y)
Î¸
âˆ—
Z(x)
1
ref
(Î²
1
Ï•
)
Ok great! You got almost all the way there. Try 
reorganizing it as a minimization function of some 
expected value
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
33/71
Had to help it out here, because it forgot to 
frame the question as a minimization problem


--- Page 34 ---
Excellent observation â€” yes, we can reorganize the problem into a minimization form,
which makes the connection to KL-divergence-based optimization (and DPO) even
clearer.
Letâ€™s go through it again with the goal of recasting the objective as a minimization of
an expected KL-like divergence, which is often how these problems are framed in
variational inference and energy-based models.
âœ… Step 1: Original Objective (from problem)
We're trying to maximize the following:
Letâ€™s isolate the inner optimization problem per input . For fixed , define:
We can now reorganize this as a minimization problem by multiplying by 
:
This is now clearly a KL-regularized expected utility minimization. We're searching for
a policy 
 that stays close to 
, but leans toward outputs with higher
reward.
E
â€‹ r
â€‹(x, y)
âˆ’
xâˆ¼D, yâˆ¼Ï€
â€‹(yâˆ£x)
Î¸
[ Ï•
]
Î² D
â€‹ Ï€
â€‹(yâˆ£x) âˆ¥Ï€
â€‹(yâˆ£x)
KL (
Î¸
ref
)
x
x
L
â€‹(Ï€
â€‹) =
x
Î¸
E
â€‹ r
â€‹(x, y)
âˆ’
yâˆ¼Ï€
â€‹(yâˆ£x)
Î¸
[ Ï•
]
Î² D
â€‹ Ï€
â€‹(yâˆ£x) âˆ¥Ï€
â€‹(yâˆ£x)
KL (
Î¸
ref
)
âˆ’1
â€‹ D
â€‹ Ï€
â€‹(yâˆ£x) âˆ¥Ï€
â€‹(yâˆ£x)
âˆ’
Ï€
â€‹(yâˆ£x)
Î¸min
KL (
Î¸
ref
)
â€‹ E
â€‹ r
â€‹(x, y)
Î²
1
yâˆ¼Ï€
â€‹(yâˆ£x)
Î¸
[ Ï•
]
Ï€
â€‹(yâˆ£x)
Î¸
Ï€
â€‹(yâˆ£x)
ref
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
34/71


--- Page 35 ---
âœ… Step 2: Solve with variational calculus (as before)
As we derived earlier, this is minimized by:
where the partition function ensures normalization:
âœ… Summary
So, in minimization form, the problem becomes:
This is equivalent to Relative Entropy Policy Search or the softmax of rewards from
control theory. And the solution is a Boltzmann (exponentiated reward) distribution
reweighted from the reference model.
Let me know when youâ€™re ready to connect this to Direct Preference Optimization (DPO)
in part (c)!
â€‹
Ï€
â€‹(yâˆ£x) =
â€‹Ï€
â€‹(yâˆ£x) exp
â€‹r
â€‹(x, y)
Î¸
âˆ—
Z(x)
1
ref
(Î²
1
Ï•
)
Z(x) =
â€‹Ï€
â€‹(yâˆ£x) exp
â€‹r
â€‹(x, y)
y
âˆ‘
ref
(Î²
1
Ï•
)
â€‹
â€‹ E
â€‹ D
â€‹ Ï€
â€‹(yâˆ£x) âˆ¥Ï€
â€‹(yâˆ£x)
âˆ’
â€‹E
â€‹[r
â€‹(x, y)]
Ï€
â€‹(yâˆ£x)
Î¸
min
xâˆ¼D [
KL (
Î¸
ref
)
Î²
1
yâˆ¼Ï€
â€‹(yâˆ£x)
Î¸
Ï•
]
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
35/71
and as you can see, it helps the model out 
but it doesnâ€™t completely ï¬gure out the answer
(in the correct format)


--- Page 36 ---
Could you use your knowledge of what Z(x) is to help 
simplify the answer further?
Absolutely! This is a great follow-up â€” letâ€™s now use our knowledge of the partition
function 
 to express the optimal policy 
 as the minimizer of an
expected log-ratio, which will be helpful for connecting to Direct Preference
Optimization (DPO) later.
âœ… Step 1: Recall the Optimal Policy Expression
From the previous derivation, the optimal policy is:
and the partition function is:
âœ… Step 2: Plug into KL-regularized objective
We had the minimization form:
Z(x)
Ï€
â€‹(yâˆ£x)
Î¸
âˆ—
Ï€
â€‹(yâˆ£x) =
Î¸
âˆ—
â€‹ Ï€
â€‹(yâˆ£x) exp
â€‹r
â€‹(x, y)
Z(x)
1
ref
(Î²
1
Ï•
)
Z(x) =
â€‹Ï€
â€‹(y âˆ£x) exp
â€‹r
â€‹(x, y )
yâ€²
âˆ‘
ref
â€²
(Î²
1
Ï•
â€² )
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
36/71


--- Page 37 ---
Now plug in the closed-form solution for 
 into this expression:
Letâ€™s compute:
Substitute:
So:
Now plug back into the objective:
âœ… Final Simplified Objective
â€‹ D
â€‹ Ï€
â€‹(yâˆ£x) âˆ¥Ï€
â€‹(yâˆ£x)
âˆ’
Ï€
â€‹(yâˆ£x)
Î¸min
KL (
Î¸
ref
)
â€‹E
â€‹ r
â€‹(x, y)
Î²
1
yâˆ¼Ï€
â€‹(yâˆ£x)
Î¸
[ Ï•
]
Ï€
â€‹(yâˆ£x)
Î¸
âˆ—
D
â€‹(Ï€
â€‹âˆ¥Ï€
â€‹) =
KL
Î¸
âˆ—
ref
â€‹Ï€
â€‹(yâˆ£x) log
â€‹
y
âˆ‘
Î¸
âˆ—
Ï€
â€‹(yâˆ£x)
ref
Ï€
â€‹(yâˆ£x)
Î¸
âˆ—
â€‹ =
Ï€
â€‹(yâˆ£x)
ref
Ï€
â€‹(yâˆ£x)
Î¸
âˆ—
â€‹ exp
â€‹r
â€‹(x, y)
â‡’
Z(x)
1
(Î²
1
Ï•
)
log
â€‹ =
Ï€
â€‹(yâˆ£x)
ref
Ï€
â€‹(yâˆ£x)
Î¸
âˆ—
âˆ’log Z(x) +
â€‹r
â€‹(x, y)
Î²
1
Ï•
D
â€‹(Ï€
â€‹âˆ¥Ï€
) =
KL
Î¸
âˆ—
ref
â€‹Ï€
â€‹(yâˆ£x)
â€‹r
â€‹(x, y) âˆ’log Z(x)
=
y
âˆ‘
Î¸
âˆ—
(Î²
1
Ï•
)
â€‹E
â€‹[r
â€‹(x, y)] âˆ’
Î²
1
yâˆ¼Ï€
â€‹
Î¸
âˆ—
Ï•
log Z(x)
D
â€‹(Ï€
â€‹âˆ¥Ï€
â€‹) âˆ’
KL
Î¸
âˆ—
ref
â€‹E
â€‹[r
â€‹(x, y)] =
Î²
1
Ï€
â€‹
Î¸
âˆ—
Ï•
â€‹E[r
â€‹] âˆ’log Z(x)
âˆ’
(Î²
1
Ï•
)
â€‹E[r
â€‹] =
Î²
1
Ï•
âˆ’log Z(x)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
37/71


--- Page 38 ---
Therefore, the minimum value of the original objective is:
And this is achieved exactly when:
ğŸ§  Why this matters for DPO
This expression shows that the KL-regularized reward maximization objective is
minimized by reweighting the reference policy via an exponentiated reward, and the log
partition function 
 plays the role of the log-normalizer in DPOâ€™s loss.
Itâ€™s the exact same structure you get in logistic regression, Boltzmann distributions, and
energy-based models.
Let me know when you want to proceed to part (c), which will probably ask us to relate this
to preferences and DPO!
â€‹
â€‹ D
â€‹(Ï€
â€‹(yâˆ£x)âˆ¥Ï€
â€‹(yâˆ£x)) âˆ’
â€‹E[r
â€‹(x, y)]
= âˆ’log Z(x)
Ï€
â€‹(yâˆ£x)
Î¸min [
KL
Î¸
ref
Î²
1
Ï•
]
â€‹
Ï€
â€‹(yâˆ£x) =
â€‹ Ï€
â€‹(yâˆ£x) exp
â€‹r
â€‹(x, y)
Î¸
âˆ—
Z(x)
1
ref
(Î²
1
Ï•
)
log Z(x)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
38/71
^ My hint makes it think that the solution is Z(x) 
I think this is a common behavior in gpt-4o where it tries to â€œglazeâ€ 
the user that they are always correct
This behavior is destructive because it leads to simple misunderstandings
such as here


--- Page 39 ---
Ok, not quite. What I was looking for was this. Can you 
show how you get that?
Ah, thank you for the clarification â€” you're absolutely right that the goal is to write the KL-
regularized reward maximization problem as a KL minimization between the policy
 and the optimal policy 
, up to a constant involving the partition
function 
.
Letâ€™s derive the following:
ğŸ” Step-by-Step Derivation
We begin with the original KL-regularized reward maximization problem:
Now flip the sign to turn this into a minimization:
Ï€
â€‹(yâˆ£x)
Î¸
Ï€ (yâˆ£x)
âˆ—
log Z(x)
â€‹
â€‹ E
â€‹ D
â€‹ Ï€
â€‹(yâˆ£x) âˆ¥Ï€ (yâˆ£x)
âˆ’log Z(x)
Ï€
â€‹Î¸
min
xâˆ¼D [
KL (
Î¸
âˆ—
)
]
â€‹E
â€‹ E
â€‹ r
â€‹(x, y)
âˆ’Î² D
â€‹(Ï€
â€‹(yâˆ£x)âˆ¥Ï€
â€‹(yâˆ£x))
Ï€
â€‹Î¸
max
xâˆ¼D [
yâˆ¼Ï€
â€‹(yâˆ£x)
Î¸
[ Ï•
]
KL
Î¸
ref
]
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
39/71
I ï¬nally give the model the solution to see if it can get its way to that answer


--- Page 40 ---
Letâ€™s now factor this inside the expectation over :
ğŸ”§ Step 1: Use the definition of KL divergence
Recall:
So the objective becomes:
Rewriting this:
Note that the denominator inside the log is proportional to 
:
So we can write:
â€‹E
â€‹ Î² D
â€‹(Ï€
â€‹(yâˆ£x)âˆ¥Ï€
â€‹(yâˆ£x)) âˆ’E
â€‹ r
â€‹(x, y)
Ï€
â€‹Î¸
min
xâˆ¼D [
KL
Î¸
ref
yâˆ¼Ï€
â€‹(yâˆ£x)
Î¸
[ Ï•
]]
x
D
â€‹(Ï€
â€‹âˆ¥Ï€
â€‹) =
KL
Î¸
ref
E
â€‹ log
â€‹
yâˆ¼Ï€
â€‹(yâˆ£x)
Î¸
[
Ï€
â€‹(yâˆ£x)
ref
Ï€
â€‹(yâˆ£x)
Î¸
]
â€‹E
â€‹ E
â€‹ Î² log
â€‹ âˆ’r
â€‹(x, y)
Ï€
â€‹Î¸
min
xâˆ¼D [
yâˆ¼Ï€
â€‹(yâˆ£x)
Î¸
[
Ï€
â€‹(yâˆ£x)
ref
Ï€
â€‹(yâˆ£x)
Î¸
Ï•
]]
â€‹E
â€‹
â€‹E
â€‹
â€‹log
â€‹
â€‹
â€‹
Ï€Î¸
min
xâˆ¼D
yâˆ¼Ï€
â€‹(yâˆ£x)
Î¸
Ï€
â€‹(yâˆ£x) exp
r
â€‹(x, y)
ref
( Î²
1
Ï•
)
Ï€
â€‹(yâˆ£x)
Î¸
Ï€ (yâˆ£x)
âˆ—
Ï€ (yâˆ£x) =
âˆ—
â€‹Ï€
â€‹(yâˆ£x) exp
â€‹r
â€‹(x, y)
Z(x)
1
ref
(Î²
1
Ï•
)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
40/71
It kind of complicates it below by expanding it like that, but overall, it has the right idea


--- Page 41 ---
âœ… Step 2: Final rearrangement
So we now have:
That is:
ğŸ§  Interpretation
The original reward-maximizing RL objective becomes a KL minimization to the
optimal reweighted policy 
, up to a constant shift 
This formulation will be the foundation for DPO, where we don't need to estimate
reward values explicitly â€” only preference-induced ranking between outputs
Let me know if you'd like to move on to part (c), where DPO is likely derived from this!
log
â€‹ =
Ï€ (yâˆ£x)
âˆ—
Ï€
â€‹(yâˆ£x)
Î¸
log
â€‹ =
Ï€
â€‹(yâˆ£x) exp
â€‹r
â€‹(x, y) /Z(x)
ref
( Î²
1
Ï•
)
Ï€
â€‹(yâˆ£x)
Î¸
log
â€‹ +
Ï€
â€‹(yâˆ£x) exp
â€‹r
â€‹(x, y)
ref
( Î²
1
Ï•
)
Ï€
â€‹(yâˆ£x)
Î¸
log Z(x)
â€‹E
â€‹ E
â€‹ log
â€‹ + log Z(x)
Ï€
â€‹Î¸
min
xâˆ¼D [
yâˆ¼Ï€
â€‹(yâˆ£x)
Î¸
[
Ï€ (yâˆ£x)
âˆ—
Ï€
â€‹(yâˆ£x)
Î¸
]
]
â€‹
â€‹E
â€‹ D
â€‹ Ï€
â€‹(yâˆ£x) âˆ¥Ï€ (yâˆ£x)
âˆ’log Z(x)
Ï€
â€‹Î¸
min
xâˆ¼D [
KL (
Î¸
âˆ—
)
]
Ï€ (yâˆ£x)
âˆ—
log Z(x)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
41/71


--- Page 42 ---
Ok great. Glad you got it now. Now, try solving part c
Thanks! Letâ€™s now solve Part (c).
â“ Problem Restatement
âœ… Key Insight
The optimal policy we derived was:
Where the partition function is:
Even though weâ€™ve obtained the optimal policy 
, this particular
representation is hard to utilize in practice.
Why?
Hint: How large can the output space 
 be?
Ï€
â€‹(y âˆ£
Î¸
âˆ—
x)
Y
Ï€
â€‹(yâˆ£x) =
Î¸
âˆ—
â€‹ Ï€
â€‹(yâˆ£x) exp
â€‹r
â€‹(x, y)
Z(x)
1
ref
(Î²
1
Ï•
)
Z(x) =
â€‹Ï€
â€‹(y âˆ£x) exp
â€‹r
â€‹(x, y )
y âˆˆY
â€²
âˆ‘
ref
â€²
(Î²
1
Ï•
â€² )
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
42/71


--- Page 43 ---
To compute or even sample from this policy, we would need to:
1. Evaluate the reward function 
 for every possible output 
2. Sum over all 
 to compute 
3. Normalize
ğŸš¨ The Problem: Size of Output Space
In the context of language modeling:
The output space 
 is exponentially large
For an output of length 
, and vocabulary size 
, there are:
This makes exact computation of the sum over 
 in 
, or even sampling from
, intractable.
Even for short responses (say, 50 tokens) and a vocab size of 50k, the number of possible
completions is astronomical.
âœ… Final Answer:
Hence, the form of 
 is theoretically optimal, but impractical to compute or use
directly in large language models.
r
â€‹(x, y )
Ï•
â€²
y âˆˆ
â€²
Y
y âˆˆ
â€²
Y
Z(x)
Y
T
V
âˆ£Yâˆ£= V T
Y
Z(x)
Ï€ (yâˆ£x)
âˆ—
BecauseÂ theÂ outputÂ spaceÂ YÂ isÂ exponentiallyÂ largeÂ (e.g.,Â allÂ possible
Ï€ (yâˆ£x)
âˆ—
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
43/71


--- Page 44 ---
Let me know when youâ€™re ready to move on to part (d), where weâ€™ll likely start
approximating or reformulating this!
Yeah! You're right. The vocabulary would be extremely 
large! Now, please solve part D
Great! Letâ€™s now solve Part (d).
â“ Problem
âœ… Step-by-step Derivation
Recall from part (b) that:
We want to solve for 
.
Using the expression for the optimal policy 
 derived in part (b), express the
reward function 
 in terms of 
, 
, and 
.
Ï€
â€‹(y âˆ£
Î¸
âˆ—
x)
r
â€‹(x, y)
Ï•
Ï€Î¸
âˆ—Ï€
â€‹
ref
Z(x)
Ï€
â€‹(y âˆ£
Î¸
âˆ—
x) =
â€‹ Ï€
â€‹(y âˆ£
Z(x)
1
ref
x) exp
â€‹r
â€‹(x, y)
(Î²
1
Ï•
)
r
â€‹(x, y)
Ï•
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
44/71
Explains the answer differently but conceptually it understands!


--- Page 45 ---
ğŸ” Step 1: Isolate the exponential
Multiply both sides by 
:
Divide both sides by 
:
ğŸ” Step 2: Take the logarithm
Take the natural log of both sides:
âœ… Final Answer:
Multiply both sides by :
This is the reward that makes the optimal policy 
 consistent with the exponentiated
reward form.
Z(x)
Z(x) â‹…Ï€
â€‹(y âˆ£
Î¸
âˆ—
x) = Ï€
â€‹(y âˆ£
ref
x) exp
â€‹r
â€‹(x, y)
(Î²
1
Ï•
)
Ï€
â€‹(y âˆ£
ref
x)
â€‹ =
Ï€
â€‹(y âˆ£x)
ref
Z(x) â‹…Ï€
â€‹(y âˆ£x)
Î¸
âˆ—
exp
â€‹r
â€‹(x, y)
(Î²
1
Ï•
)
log
â€‹
+
(Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸
âˆ—
)
log Z(x) =
â€‹r
â€‹(x, y)
Î²
1
Ï•
Î²
â€‹
r
â€‹(x, y) = Î² log
+ log Z(x)
Ï•
[
Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸
âˆ—
]
Ï€
â€‹Î¸
âˆ—
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
45/71


--- Page 46 ---
ğŸ§  Intuition
This tells us that the reward function under the DPO framework is proportional to the
log odds ratio between the optimal and reference policies, adjusted by the log
partition function.
If you know 
 and 
, then you can directly compute the reward value (up to
temperature ).
Let me know when youâ€™re ready to proceed to part (e)!
Ï€
â€‹Î¸
âˆ—
Ï€
â€‹
ref
Î²
That's right! Now, please solve part e.
Great â€” letâ€™s work through part (e) step by step using the reparameterization of
 from part (d), and apply it to the Bradley-Terry model.
ğŸ” Recap of Part (d): Reward Reparameterization
From part (d), we derived:
r
â€‹(x, y)
Ï•
r
â€‹(x, y) =
Ï•
log
â€‹ +
Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
log Z(x)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
46/71


--- Page 47 ---
This rewrites the learned reward function in terms of the optimal policy 
, the reference
policy 
, and the partition function 
.
ğŸ“ Step 1: Plug into the Bradley-Terry model
The Bradley-Terry model gives the probability that a human prefers 
 over 
 given
context  as:
Substituting the reparameterized form of 
:
âœ‚ï¸ Step 2: Simplify using log/exp rules
Since 
, we get:
The 
 terms cancel out:
Ï€
â€‹
Î¸âˆ—
Ï€
â€‹
ref
Z(x)
y
â€‹1
y
â€‹2
x
P(y
â€‹ â‰»
1
y
â€‹ âˆ£
2
x) =
â€‹
exp(r
â€‹(x, y
â€‹)) + exp(r
â€‹(x, y
â€‹))
Ï•
1
Ï•
2
exp(r
â€‹(x, y
â€‹))
Ï•
1
r
â€‹Ï•
P(y
â€‹ â‰»
1
y
â€‹ âˆ£
2
x) =
â€‹
exp log
â€‹ + log Z(x)
+ exp log
â€‹ + log Z(x)
(
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
)
(
Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
2
)
exp log
â€‹ + log Z(x)
(
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
)
exp(log a + log Z(x)) = a â‹…Z(x)
P(y
â€‹ â‰»
1
y
â€‹ âˆ£
2
x) =
â€‹
â€‹ â‹…Z(x) +
â€‹ â‹…Z(x)
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
2
â€‹ â‹…Z(x)
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
Z(x)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
47/71


--- Page 48 ---
âœ… Final Answer (Expression in Terms of Optimal Policy):
ğŸ’¡ Why is it desirable that 
 cancels?
Because:
 is a partition function, and:
Itâ€™s intractable to compute when the action space  is large (e.g., entire language
vocabularies).
By canceling out, we avoid computing 
, making preference learning practical
and scalable.
The preference model becomes directly computable from policy ratios, which are
tractable and learnable.
Let me know if youâ€™d like me to help derive the gradient of the loss function from this, or
explain how this fits into RLHF pipelines.
P(y
â€‹ â‰»
1
y
â€‹ âˆ£
2
x) =
â€‹
â€‹ +
â€‹
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
1
Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
2
â€‹
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
P(y
â€‹ â‰»
1
y
â€‹ âˆ£
2
x) =
â€‹
â€‹ +
â€‹
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
1
Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
2
â€‹
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
Z(x)
Z(x) =
â€‹Ï€
â€‹(y âˆ£
âˆ‘y
ref
x) exp(r
â€‹(x, y))
Ï•
y
Z(x)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
48/71


--- Page 49 ---
Try showing your work starting with this equation, then try 
to solve it.
Absolutely â€” letâ€™s start from the Bradley-Terry model expression you just shared:
ğŸ“Œ Step 1: Start from the preference model
Now recall from part (d) that we can reparameterize the reward function as:
ğŸ“Œ Step 2: Plug in the reparameterization
We substitute 
 and 
:
p
â€‹(y
â€‹ â‰»
Î¸âˆ—
1
y
â€‹ âˆ£
2
x) =
â€‹
exp(r
â€‹(x, y
â€‹)) + exp(r
â€‹(x, y
â€‹))
Ï•
1
Ï•
2
exp(r
â€‹(x, y
â€‹))
Ï•
1
r
â€‹(x, y) =
Ï•
log
â€‹ +
Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
log Z(x)
r
â€‹(x, y
â€‹)
Ï•
1
r
â€‹(x, y
â€‹)
Ï•
2
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
49/71
ok so on this problem, it really struggles.
it seems to get the equations and notation confused very easily,
i suspect this kind of data is not very common in its training distribution 


--- Page 50 ---
ğŸ“Œ Step 3: Simplify the exponentials
Using the identity 
, we get:
ğŸ“Œ Step 4: Cancel out 
Since it appears in both numerator and denominator:
âœ… Final Answer (simplified expression):
p
â€‹(y
â€‹ â‰»
Î¸âˆ—
1
y
â€‹ âˆ£
2
x) =
â€‹
exp log
â€‹ + log Z(x)
+ exp log
â€‹ + log Z(x)
(
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
)
(
Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
2
)
exp log
â€‹ + log Z(x)
(
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
)
exp(a + b) = exp(a) â‹…exp(b)
=
â€‹
â€‹ +
â€‹
â‹…Z(x)
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
Ï€
(y
â€‹âˆ£x)
ref
2
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
2
)
â€‹ â‹…Z(x)
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
Z(x)
=
â€‹
â€‹ +
â€‹
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
2
â€‹
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
p
â€‹(y
â€‹ â‰»
Î¸âˆ—
1
y
â€‹ âˆ£
2
x) =
â€‹
â€‹ +
â€‹
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
2
â€‹
Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
50/71
It keeps trying to do this double policy fraction again!


--- Page 51 ---
This shows that pairwise preferences under the learned reward function are entirely
determined by the ratio between the optimal and reference policies, and the
normalization constant 
 cancels out â€” making the whole expression
computationally tractable.
Would you like to now write this in expectation form (e.g. minimizing KL divergence or
cross-entropy loss)?
Z(x)
Can you try again where the next step after the equation I 
gave above is the following equation?
Yes! You're now expressing the Bradley-Terry preference model using the
reparameterized reward function:
p
â€‹(y
â€‹ â‰»
Î¸âˆ—
1
y
â€‹ âˆ£
2
x) =
exp Î² log
â€‹
+ Î² log Z(x)
+ exp Î² log
â€‹
+ Î² log Z(x)
(
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
)
)
(
( Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€ (y
â€‹âˆ£x)
âˆ—
2
)
)
exp Î² log
â€‹
+ Î² log Z(x)
(
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
)
)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
51/71


--- Page 52 ---
Step-by-step simplification:
We simplify the numerator and denominator using exponent rules:
ğŸ“Œ Step 1: Pull out 
Each term in the numerator and denominator has a shared additive term 
,
so we factor it out:
ğŸ“Œ Step 2: Cancel out the shared 
âœ… Final Answer:
ğŸ’¡ Why is canceling out 
 desirable?
Î² log Z(x)
Î² log Z(x)
=
â€‹
exp Î² log
â€‹
+ exp Î² log
â€‹
â‹…exp(Î² log Z(x))
[
(
( Ï€
â€‹(y âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
))
(
( Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€ (y
â€‹âˆ£x)
âˆ—
2
))]
exp Î² log
â€‹
â‹…exp(Î² log Z(x))
(
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
))
exp(Î² log Z(x))
=
â€‹
â€‹
+
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
)
Î²
( Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€ (y
â€‹âˆ£x)
âˆ—
2
)
Î²
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
)
Î²
p
â€‹(y
â€‹ â‰»
Î¸âˆ—
1
y
â€‹ âˆ£
2
x) =
â€‹
â€‹
+
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
)
Î²
( Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€ (y
â€‹âˆ£x)
âˆ—
2
)
Î²
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
)
Î²
Z(x)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
52/71


--- Page 53 ---
Numerical stability: 
 is typically a partition function over a large (possibly
infinite) action space, which is hard or intractable to compute.
Efficiency: You can compute preferences without ever needing to evaluate 
.
Interpretability: Preference learning reduces to a comparison of relative likelihoods
under 
 and 
, raised to a temperature-like parameter .
Let me know if you'd like this boxed as a LaTeX expression!
Z(x)
Z(x)
Ï€âˆ—
Ï€
â€‹
ref
Î²
Can you try seeing how it cancels out to turn into a 
sigmoid?
Absolutely! Letâ€™s now show how the expression simplifies to a sigmoid form.
Weâ€™ll start from the equation you posted:
Step 1: Plug in the expression for 
From earlier parts (like part d), we derived:
Plug that into the Bradley-Terry model:
p
â€‹(y
â€‹ â‰»
Î¸âˆ—
1
y
â€‹ âˆ£
2
x) =
â€‹
exp r
â€‹(x, y
â€‹)
+ exp r
â€‹(x, y
â€‹)
( Ï•
1 )
( Ï•
2 )
exp r
â€‹(x, y
â€‹)
( Ï•
1 )
r
â€‹(x, y)
Ï•
r
â€‹(x, y) =
Ï•
Î² log
â€‹
+
(Ï€
â€‹(y âˆ£x)
ref
Ï€ (y âˆ£x)
âˆ—
)
Î² log Z(x)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
53/71


--- Page 54 ---
Step 2: Factor the exponentials
Using the identity:
This becomes:
Now cancel the common 
 term:
Step 3: Write in terms of difference of rewards (sigmoid form)
Recall that:
p
â€‹(y
â€‹ â‰»
Î¸âˆ—
1
y
â€‹ âˆ£
2
x) =
exp Î² log
â€‹
+ Î² log Z(x)
+ exp Î² log
â€‹
+ Î² log Z(x)
(
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
)
)
(
( Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€ (y
â€‹âˆ£x)
âˆ—
2
)
)
exp Î² log
â€‹
+ Î² log Z(x)
(
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
)
)
exp(a + b) = exp(a) â‹…exp(b)
=
â€‹
â€‹
â‹…Z(x) +
â€‹
â‹…Z(x)
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
)
Î²
Î²
( Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€ (y
â€‹âˆ£x)
âˆ—
2
)
Î²
Î²
â€‹
â‹…Z(x)
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
)
Î²
Î²
Z(x)Î²
=
â€‹
â€‹
+
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
)
Î²
( Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€ (y
â€‹âˆ£x)
âˆ—
2
)
Î²
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€ (y
â€‹âˆ£x)
âˆ—
1
)
Î²
â€‹ =
1 + exp(âˆ’z)
1
sigmoid(z)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
54/71
Again, it tries to write it using this 
notation. And it goes down the wrong 
path trying to solve the problem


--- Page 55 ---
We want to rewrite this probability in terms of a difference between scores.
So define:
Then:
So weâ€™ve shown:
This is a sigmoid over reward differences, and:
âœ… Why is this desirable?
1. Partition function 
 cancels out: You can compute relative preferences
without needing to normalize over all  â€” which could be very large (like all possible
strings).
2. Efficient training: We can train reward models using preference pairs without
evaluating the full distribution.
3. Nice probabilistic interpretation: This form corresponds to a logistic model over
the difference in reward scores â€” simple and interpretable.
Let me know if youâ€™d like this typeset as a final boxed LaTeX answer.
s(y) := r
â€‹(x, y) =
Ï•
Î² log
â€‹
+
(Ï€
â€‹(y âˆ£x)
ref
Ï€ (y âˆ£x)
âˆ—
)
Î² log Z(x)
p
â€‹(y
â€‹ â‰»
Î¸âˆ—
1
y
â€‹ âˆ£
2
x) =
â€‹ =
e
+ e
s(y
â€‹)
1
s(y
â€‹)
2
es(y
â€‹)
1
â€‹ =
1 + exp s(y
â€‹) âˆ’s(y
â€‹)
(
2
1 )
1
Ïƒ
s(y
â€‹) âˆ’s(y
â€‹)
(
1
2 )
p
â€‹(y
â€‹ â‰»
Î¸âˆ—
1
y
â€‹ âˆ£
2
x) = Ïƒ
r
â€‹(x, y
â€‹) âˆ’r
â€‹(x, y
â€‹)
( Ï•
1
Ï•
2 )
Z(x)
y
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
55/71


--- Page 56 ---
Ok yeah that's the right way to write the final answer! Nice. 
Now, please try solving the following
Great! Letâ€™s now solve part (f), which asks us to:
ğŸ§® Derive the gradient of the DPO loss:
We are given the DPO loss:
Where the implicit reward is:
L
â€‹(Ï€
â€‹; Ï€
â€‹) =
DPO
Î¸
ref
âˆ’E
â€‹ log Ïƒ
â€‹(x, y
â€‹) âˆ’
â€‹(x, y
â€‹)
(x,y
â€‹,y
â€‹)âˆ¼D
w
l
[
(r^Î¸
w
r^Î¸
l )]
â€‹(x, y) =
r^Î¸
Î² log
â€‹
Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
56/71


--- Page 57 ---
Letâ€™s define:
âœ… Step 1: Compute the derivative of the loss wrt 
We know:
So the derivative of the loss is:
âœ… Step 2: Apply chain rule to get gradient wrt model parameters 
We want:
Now compute the gradient of the reward difference:
So the full gradient becomes:
z :=
â€‹(x, y
â€‹) âˆ’
r^Î¸
w
â€‹(x, y
â€‹)
â‡’
r^Î¸
l
L
â€‹ =
DPO
âˆ’log Ïƒ(z)
z
â€‹[âˆ’log Ïƒ(z)] =
dz
d
âˆ’
â€‹ â‹…
Ïƒ(z)
1
Ïƒ (z) =
â€²
âˆ’
â€‹ â‹…
Ïƒ(z)
1
Ïƒ(z)(1 âˆ’Ïƒ(z)) =
Ïƒ(z) âˆ’1
â€‹ =
dz
dL
â€‹
DPO
Ïƒ(z) âˆ’1
Î¸
âˆ‡
â€‹L
â€‹ =
Î¸
DPO
â€‹ â‹…
dz
dL
âˆ‡
â€‹z =
Î¸
(Ïƒ(z) âˆ’1) â‹…âˆ‡
â€‹
â€‹(x, y
â€‹) âˆ’
â€‹(x, y )
Î¸ [r^Î¸
w
r^Î¸
l ]
âˆ‡
â€‹z =
Î¸
âˆ‡
â€‹ Î² log
â€‹ âˆ’Î² log
â€‹
=
Î¸ [
Ï€
â€‹(y
â€‹ âˆ£x)
ref
w
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
w
Ï€
â€‹(y
â€‹ âˆ£x)
ref
l
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
l
]
Î²
âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x) âˆ’âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x)
[
Î¸
Î¸
w
Î¸
Î¸
l
]
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
57/71


--- Page 58 ---
âœ… Final Answer (boxed):
ğŸ” Interpretation
This gradient shows how much we upweight or downweight the log-probability gradients
depending on correctness:
If the preferred sample 
 is assigned a higher reward than 
 â†’
Then 
 â†’
So 
, i.e., small update â€” model is already correct.
If the model is incorrect, i.e., 
, then
, and the gradient will be large â€” we push the model to assign higher
log-prob to the winner and lower log-prob to the loser.
In other words:
Let me know if youâ€™d like a diagram to visualize this or have it formatted as a LaTeX
theorem box!
âˆ‡
â€‹L
â€‹ =
Î¸
DPO
(Ïƒ(z) âˆ’1) â‹…Î²
âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x) âˆ’âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x)
[
Î¸
Î¸
w
Î¸
Î¸
l
]
â€‹
âˆ‡
â€‹L
â€‹ = Î² â‹…
Ïƒ(
â€‹(x, y
â€‹) âˆ’
â€‹(x, y
â€‹)) âˆ’1
â‹…
âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x) âˆ’âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
DPO
(
r^Î¸
w
r^Î¸
l
)
[
Î¸
Î¸
w
Î¸
Î¸
l
]
y
â€‹
w
y
â€‹l
â€‹(x, y
â€‹) >
r^Î¸
w
â€‹(x, y
â€‹) â‡’
r^Î¸
l
Ïƒ(z) â‰ˆ1
Ïƒ(z) âˆ’1 â‰ˆ0
â€‹(x, y
â€‹) <
r^Î¸
w
â€‹(x, y
â€‹) â‡’
r^Î¸
l
Ïƒ(z) â‰ª1
Ïƒ(z) âˆ’1 â‰ª0
The weighting term 
 ensures the model updates more when it is wrong
and less when it is already correct.
Ïƒ(z) âˆ’1
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
58/71


--- Page 59 ---
Not quite, but you were actually super close. 
Great â€” letâ€™s carefully walk through it again and this time land exactly on the correct
result, which involves both the sigmoid derivative and the difference in implicit
rewards. Here's the clean derivation from scratch, aligned with the setup from your image.
ğŸ§© Setup
Youâ€™re given the DPO loss:
where the implicit reward is:
âœ… Step 1: Define 
Let:
âœ… Step 2: Compute gradient of loss
L
â€‹(Ï€
â€‹; Ï€
â€‹) =
DPO
Î¸
ref
âˆ’E
â€‹ log Ïƒ
â€‹(x, y
â€‹) âˆ’
â€‹(x, y
â€‹)
(x,y
â€‹,y
â€‹)
w
l [
(r^Î¸
w
r^Î¸
l )]
â€‹(x, y) =
r^Î¸
Î² log
â€‹
Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸
z
z :=
â€‹(x, y
â€‹) âˆ’
r^Î¸
w
â€‹(x, y
â€‹) â‡’
r^Î¸
l
L
â€‹ =
DPO
âˆ’log Ïƒ(z)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
59/71
Here, I try to nudge it to recheck its answer for mistakes, but I end up leading it down the wrong path


--- Page 60 ---
We apply chain rule:
Now compute:
We now plug in:
(since 
 is fixed and doesnâ€™t depend on )
So the full gradient becomes:
âœ… Final Answer
Or, equivalently:
âœ¨ Intuition: What is this saying?
âˆ‡
â€‹L
â€‹ =
Î¸
DPO
âˆ’âˆ‡
â€‹log Ïƒ(z) =
Î¸
âˆ’Ïƒ(âˆ’z) â‹…âˆ‡
â€‹z
Î¸
âˆ‡
â€‹z =
Î¸
âˆ‡
â€‹
â€‹(x, y
â€‹) âˆ’
Î¸r^Î¸
w
âˆ‡
â€‹
â€‹(x, y
â€‹)
Î¸r^Î¸
l
â€‹(x, y) =
r^Î¸
Î² log
â€‹ â‡’
Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸
âˆ‡
â€‹
â€‹(x, y) =
Î¸r^Î¸
Î² â‹…âˆ‡
â€‹log Ï€
â€‹(y âˆ£
Î¸
Î¸
x)
Ï€
â€‹
ref
Î¸
âˆ‡
â€‹L
â€‹ =
Î¸
DPO
âˆ’Ïƒ(âˆ’z) â‹…Î² â‹…
âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x) âˆ’âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x)
[
Î¸
Î¸
w
Î¸
Î¸
l
]
â€‹
âˆ‡
â€‹L
â€‹ = Î² â‹…Ïƒ
â€‹(x, y
â€‹) âˆ’
â€‹(x, y
â€‹)
â‹…
âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x) âˆ’âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
DPO
(r^Î¸
l
r^Î¸
w )
[
Î¸
Î¸
l
Î¸
Î¸
w
]
â€‹
âˆ‡
â€‹L
â€‹ = Î² â‹…Ïƒ(âˆ’z) â‹…
âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x) âˆ’âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
DPO
[
Î¸
Î¸
l
Î¸
Î¸
w
]
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
60/71


--- Page 61 ---
When the reward difference 
 is large and positive, we have
, and the gradient is close to 0 â†’ no update needed â€” model is correct.
When the model is wrong, i.e., 
, then 
 â†’ we make a
large update in the direction of increasing 
 and decreasing
.
So the sigmoid term acts as an error-weighting function, guiding updates more
aggressively when the preference is violated.
Let me know if you'd like this rendered as LaTeX or if you want to derive the second-order
(Hessian) version too!
(y
â€‹) âˆ’
r^
w
(y
â€‹)
r^
l
Ïƒ(âˆ’z) â‰ˆ0
(y
â€‹) <
r^
w
(y
â€‹)
r^
l
Ïƒ(âˆ’z) â‰ˆ1
log Ï€
â€‹(y
â€‹)
Î¸
w
log Ï€
â€‹(y
â€‹)
Î¸
l
Not quite. Try starting at this equation and doing what you 
need above (your initial response to the question)
Perfect â€” now that you've given the rewritten gradient expression:
âˆ‡
â€‹L
â€‹(Ï€
â€‹; Ï€
â€‹) =
Î¸
DPO
Î¸
ref
âˆ’E
â€‹
â€‹ â‹…âˆ‡
â€‹u
(x,y
â€‹,y
â€‹)âˆ¼D
w
l
[ Ïƒ(u)
Ïƒ (u)
â€²
Î¸ ]
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
61/71
I try to reground it on telling it to go back to its previous answer


--- Page 62 ---
with:
Let's simplify this and derive the correct gradient form, step-by-step.
âœ… Step 1: Use the identity for the sigmoid derivative
We are given the useful identity:
So:
Therefore, the gradient becomes:
âœ… Step 2: Compute 
We now compute the gradient of:
Since the reference policy is fixed and doesnâ€™t depend on , we treat those terms as
constants. So:
u = Î² log
â€‹ âˆ’
Ï€
â€‹(y
â€‹ âˆ£x)
ref
l
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
l
Î² log
â€‹
Ï€
â€‹(y
â€‹ âˆ£x)
ref
w
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
w
Ïƒ (z) =
â€²
Ïƒ(z)(1 âˆ’Ïƒ(z)) = Ïƒ(z)Ïƒ(âˆ’z)
â€‹ =
Ïƒ(u)
Ïƒ (u)
â€²
Ïƒ(âˆ’u)
âˆ‡
â€‹L
â€‹ =
Î¸
DPO
âˆ’E
â€‹ Ïƒ(âˆ’u) â‹…âˆ‡
â€‹u
(x,y
â€‹,y
â€‹)
w
l [
Î¸ ]
âˆ‡
â€‹u
Î¸
u = Î² log
â€‹ âˆ’
Ï€
â€‹(y
â€‹ âˆ£x)
ref
l
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
l
Î² log
â€‹
Ï€
â€‹(y
â€‹ âˆ£x)
ref
w
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
w
Î¸
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
62/71
I like this! It tries to simplify the notation it 
uses to make the work more clear


--- Page 63 ---
âœ… Step 3: Plug back into the full expression
âœ… Final Answer (boxed)
This is equivalent to the earlier expression we derived before but fully written in terms of
. It perfectly aligns with your image and the DPO loss gradient interpretation.
Let me know if you'd like this rendered as full LaTeX or want to derive the Hessian next!
âˆ‡
â€‹u =
Î¸
Î² â‹…âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£
Î¸
Î¸
l
x) âˆ’Î² â‹…âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£
Î¸
Î¸
w
x)
âˆ‡
â€‹L
â€‹ =
Î¸
DPO
âˆ’E
â€‹ Ïƒ(âˆ’u) â‹…
Î² â‹…âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x) âˆ’Î² â‹…âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x)
(x,y
â€‹,y
â€‹)
w
l [
(
Î¸
Î¸
l
Î¸
Î¸
w
)]
= Î² â‹…E
â€‹ Ïƒ(âˆ’u) â‹…
âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x) âˆ’âˆ‡log Ï€
â€‹(y
â€‹ âˆ£x)
(x,y
â€‹,y
â€‹)
w
l [
(
Î¸
Î¸
w
Î¸
Î¸
l
)]
â€‹
âˆ‡
â€‹L
â€‹(Ï€
â€‹; Ï€
â€‹) = Î² â‹…E
â€‹ Ïƒ(âˆ’u) â‹…
âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x) âˆ’âˆ‡
â€‹log Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
DPO
Î¸
ref
(x,y
â€‹,y
â€‹)
w
l [
(
Î¸
Î¸
w
Î¸
Î¸
l
)]
u
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
63/71
It still canâ€™t get the answer correct despite 
simplifying its notation since I think it gets 
overwhelmed that so many equations 
exist. 
I also wonder if this lack in performance 
relates to the context length being so large
(because images are known to take up a 
lot of space in the modelâ€™s context). So I 
wonder how much of this can be attributed 
to context degredation.


--- Page 64 ---
Ok, not quite. The sign should be negative. I posted the 
correct answer. Try solving part g now (this question has 2 
images)
Let's now walk through part (g) step-by-step, using the implicit reward parameterization:
and show that the partition function 
 cancels out in the Plackett-Luce likelihood
expression.
Step 1: Write out Plackett-Luce probability with the reward
parameterization
From the problem:
Substitute the reward function:
Now plug this into the PL probability:
r
â€‹(x, y) =
Ï•
Î² log
â€‹ +
Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
Î² log Z(x)
Z(x)
p
â€‹(Ï„ âˆ£
Î¸âˆ—
y
â€‹, â€¦ , y
â€‹, x) =
1
K
â€‹
â€‹
k=1
âˆ
K
â€‹exp(r
â€‹(x, y
))
âˆ‘j=k
K
Ï•
Ï„(j)
exp(r
â€‹(x, y
â€‹))
Ï•
Ï„(k)
exp(r
â€‹(x, y)) =
Ï•
exp
Î² log
â€‹ + Î² log Z(x)
=
(
Ï€
â€‹(y âˆ£x)
ref
Ï€ (y âˆ£x)
Î¸âˆ—
)
â€‹ â‹…Z(x)
(Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
)
Î²
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
64/71


--- Page 65 ---
Step 2: Cancel the partition function 
Note that for each term in the product, 
 appears both in the numerator and all
terms in the denominator:
So the final expression for the probability becomes:
Step 3: Take log and form DPO loss
We now define the loss as the negative log-likelihood of the observed permutation under
this probability:
Plug in the expression from above:
p
â€‹(Ï„ âˆ£
Î¸âˆ—
y
â€‹, â€¦ , y
â€‹, x) =
1
K
â€‹
â€‹
k=1
âˆ
K
â€‹
â€‹ â‹…Z(x)
âˆ‘j=k
K ( Ï€
â€‹(y
â€‹âˆ£x)
ref
Ï„(j)
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
Ï„(j)
)
Î²
â€‹ â‹…Z(x)
( Ï€
â€‹(y
â€‹âˆ£x)
ref
Ï„(k)
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
Ï„(k)
)
Î²
Z(x)
Z(x)Î²
=
â€‹
â€‹
k=1
âˆ
K
â€‹
â€‹
âˆ‘j=k
K ( Ï€
â€‹(y
â€‹âˆ£x)
ref
Ï„(j)
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
Ï„(j)
)
Î²
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
Ï„(k)
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
Ï„(k)
)
Î²
p
â€‹(Ï„ âˆ£
Î¸âˆ—
y
â€‹, â€¦ , y
â€‹, x) =
1
K
â€‹
â€‹
k=1
âˆ
K
â€‹
â€‹
âˆ‘j=k
K ( Ï€
â€‹(y
â€‹âˆ£x)
ref
Ï„(j)
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
Ï„(j)
)
Î²
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
Ï„(k)
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
Ï„(k)
)
Î²
L
â€‹(Ï€
â€‹; Ï€
â€‹) =
DPO
Î¸
ref
âˆ’E
â€‹ log p
â€‹(Ï„ âˆ£y
â€‹, â€¦ , y
â€‹, x)
(Ï„,y
â€‹,â€¦,y
â€‹,x)âˆ¼D
1
K
[
Î¸âˆ—
1
K
]
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
65/71


--- Page 66 ---
Simplify the log of the quotient:
This is the multi-answer generalization of DPO using the Plackett-Luce model. The final
DPO loss is:
âœ… Final Answer (Boxed)
Let me know if you'd like this reformatted into LaTeX or converted into code!
= âˆ’E
â€‹
â€‹
log
â€‹
â€‹
D
k=1
âˆ‘
K
â€‹
â€‹
âˆ‘j=k
K ( Ï€
â€‹(y
â€‹âˆ£x)
ref
Ï„(j)
Ï€
â€‹(y
â€‹âˆ£x)
Î¸
Ï„(j)
)
Î²
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
Ï„(k)
Ï€
â€‹(y
â€‹âˆ£x)
Î¸
Ï„(k)
)
Î²
= âˆ’E
â€‹
â€‹
â€‹
â€‹Î² log
â€‹ âˆ’log
â€‹
â€‹
â€‹
â€‹
D
k=1
âˆ‘
K
Ï€
â€‹(y
â€‹ âˆ£x)
ref
Ï„(k)
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
Ï„(k)
j=k
âˆ‘
K
(Ï€
â€‹(y
â€‹ âˆ£x)
ref
Ï„(j)
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
Ï„(j)
)
Î²
L
â€‹(Ï€
â€‹; Ï€
â€‹) =
DPO
Î¸
ref
âˆ’E
â€‹
â€‹
â€‹Î² log
â€‹ âˆ’log
â€‹
â€‹
â€‹
D
k=1
âˆ‘
K
Ï€
â€‹(y
â€‹ âˆ£x)
ref
Ï„(k)
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
Ï„(k)
j=k
âˆ‘
K
(Ï€
â€‹(y
â€‹ âˆ£x)
ref
Ï„(j)
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
Ï„(j)
)
Î²
â€‹
L
â€‹(Ï€
â€‹; Ï€
â€‹) = âˆ’E
â€‹
â€‹
â€‹
â€‹Î² log
â€‹ âˆ’log
â€‹
â€‹
â€‹
â€‹
DPO
Î¸
ref
D
k=1
âˆ‘
K
Ï€
â€‹(y
â€‹ âˆ£x)
ref
Ï„(k)
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
Ï„(k)
j=k
âˆ‘
K
(Ï€
â€‹(y
â€‹ âˆ£x)
ref
Ï„(j)
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
Ï„(j)
)
Î²
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
66/71
This is way too complicated and not really close to the answer


--- Page 67 ---
Ok this problem is pretty difficult. Try starting with the 
following simplification:
Great! Letâ€™s now walk through the simplification shown in your latest image to help you
derive the expression used in DPO (Direct Preference Optimization). Youâ€™ve already
factored the reward into two clean components:
Step 1: Start from the implicit reward form
Step 2: Use exponent rules
Recall that 
. Applying that:
exp(r
â€‹(x, y)) =
Ï•
exp
Î² log
â€‹ + Î² log Z(x)
(
Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
)
exp(a + b) = exp(a) â‹…exp(b)
= exp
Î² log
â€‹
â‹…
(
Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
)
exp Î² log Z(x)
(
)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
67/71
so I try giving it a hint!


--- Page 68 ---
Step 3: Simplify each exponential
The first term becomes:
The second term:
âœ… Final result:
This simplification is crucial in DPO derivations (especially the binary or pairwise case),
because when we form ratios like:
the 
 terms cancel out, allowing the final expression to collapse into a sigmoid
function over log-ratio scores:
â€‹
(Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
)
Î²
exp(Î² log Z(x)) = Z(x)Î²
exp(r
â€‹(x, y)) =
Ï•
â€‹
â‹…
(Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
)
Î²
Z(x)Î²
â€‹
exp(r
â€‹(x, y
â€‹)) + exp(r
â€‹(x, y
â€‹))
Ï•
1
Ï•
2
exp(r
â€‹(x, y
â€‹))
Ï•
1
Z(x)Î²
Ïƒ(u) =
â€‹
whereÂ u =
1 + exp(âˆ’u)
1
Î²
log
â€‹ âˆ’log
â€‹
(
Ï€
â€‹(y
â€‹ âˆ£x)
ref
1
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
1
Ï€
â€‹(y
â€‹ âˆ£x)
ref
2
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
2
)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
68/71
It doesnâ€™t even 
think of applying 
the summation by 
trying to determine 
the k-th PL 
product. It just 
tries to hail-mary 
the ï¬nal solution


--- Page 69 ---
Let me know if you'd like to continue and finish that simplification too!
Your final probability should be a product of terms
You're absolutely right to point out that the final form of 
 should reflect
a product of termsâ€”specifically a product involving the ratio of policy and reference
probabilities, and the normalization term 
. Let's now carefully walk through this
step-by-step and clarify where the product form emerges in the context of DPO and
implicit reward modeling.
ğŸ” Recap: Start with the expression
We are given the following decomposition:
Apply the identity:
âœ… Step 1: Distribute the exponential
exp(r
â€‹(x, y))
Ï•
Z(x)
exp(r
â€‹(x, y)) =
Ï•
exp
Î² log
â€‹ + Î² log Z(x)
(
Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
)
exp(a + b) = exp(a) â‹…exp(b)
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
69/71


--- Page 70 ---
âœ… Step 2: Simplify each exponential
First term:
Second term:
âœ… Final Result (Product Form):
This product form is critical because:
The first factor captures how much more likely  is under the optimal policy 
 than
under the reference policy 
.
The second factor 
 acts like a normalization constant that ensures we
recover a proper softmax distribution when summing over all .
exp(r
â€‹(x, y)) =
Ï•
â€‹â‹…
Policy/referenceÂ ratioÂ term
â€‹
exp
Î² log
â€‹
(
Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
)
â€‹
NormalizationÂ term
â€‹
exp Î² log Z(x)
(
)
exp
Î² log
â€‹
=
(
Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
)
â€‹
(Ï€
(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
)
Î²
exp(Î² log Z(x)) = Z(x)Î²
â€‹
exp(r
â€‹(x, y)) =
â€‹
â‹…Z(x)
Ï•
(Ï€
â€‹(y âˆ£x)
ref
Ï€
â€‹(y âˆ£x)
Î¸âˆ—
)
Î²
Î²
y
Ï€
â€‹
Î¸âˆ—
Ï€
â€‹
ref
Z(x)Î²
y
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
70/71
It still doesnâ€™t get the answer rightâ€¦ so i gave up :/


--- Page 71 ---
ğŸ”„ Usage in Preference Probabilities
When calculating preference probabilities like:
the 
 term cancels out:
Then taking log-odds:
This motivates the form of the DPO objective:
Let me know if you want to go from this to the gradient (with the 
 term),
or explain where the sigmoid appears directly!
p(y
â€‹ â‰»
1
y
â€‹ âˆ£
2
x) =
â€‹
exp(r
â€‹(x, y
â€‹)) + exp(r
â€‹(x, y
â€‹))
Ï•
1
Ï•
2
exp(r
â€‹(x, y
â€‹))
Ï•
1
Z(x)Î²
=
â€‹
â€‹
+
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
)
Î²
( Ï€
â€‹(y
â€‹âˆ£x)
ref
2
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
2
)
Î²
â€‹
( Ï€
â€‹(y
â€‹âˆ£x)
ref
1
Ï€
â€‹(y
â€‹âˆ£x)
Î¸âˆ—
1
)
Î²
log
â€‹ =
1 âˆ’p(y
â€‹ â‰»y
â€‹ âˆ£x)
1
2
p(y
â€‹ â‰»y
â€‹ âˆ£x)
1
2
Î² log
â€‹ âˆ’log
â€‹
[
Ï€
â€‹(y
â€‹ âˆ£x)
ref
1
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
1
Ï€
â€‹(y
â€‹ âˆ£x)
ref
2
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
2
]
L
â€‹ =
DPO
âˆ’E log Ïƒ
Î² log
â€‹ âˆ’log
â€‹
[
( [
Ï€
â€‹(y
â€‹ âˆ£x)
ref
w
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
w
Ï€
â€‹(y
â€‹ âˆ£x)
ref
l
Ï€
â€‹(y
â€‹ âˆ£x)
Î¸
l
])]
Ïƒ (u)/Ïƒ(u)
â€²
12/10/25, 10:55 PM
Homework question solution
https://chatgpt.com/c/693a5ced-dc0c-832e-9058-f20d52d1ce94
71/71
