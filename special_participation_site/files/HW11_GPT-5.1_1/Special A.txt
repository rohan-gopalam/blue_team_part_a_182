--- Page 3 ---
​
Do Question 1 turn by turn ​
​


--- Page 10 ---
​
Both provide excellent and accurate answers to the questions posed. They 
both utilize a step-by-step approach, demonstrate deep understanding of 
LoRA, and include self-checks. 
However, "Turn by Turn" provides better answers due to its superior mathematical rigor and 
depth in key explanations. 
Here is a detailed comparison of the answers provided in the two files. 
(a) Adjusting LoRA for Underfitting 
●​ Concept: Underfitting means the model lacks the capacity to capture the complexity of 
the training data. In LoRA, the expressivity of the adaptation (the matrix product AB) is 
constrained by the inner dimension, the rank k. 
●​ One Time: Clearly explains that the rank k limits the capacity of the update AB. To 
reduce underfitting, one must increase the rank k. 
●​ Turn by Turn: Provides the same correct explanation but also briefly mentions the LoRA 
scaling factor (α) as a secondary tuning knob (Page 2). 
●​ Assessment: Both are correct. "One Time" is slightly better for mentioning an additional, 
relevant hyperparameter. 
(b) Initialization to Zeros 
●​ Concept: Training relies on gradient-based optimization (SGD). If gradients are zero at 
initialization, the parameters cannot be updated. 
●​ One Time: It correctly states the standard matrix calculus formulas for the gradients 
(Page 2): ∂A∂L​=GB⊤ and ∂B∂L​=A⊤G. It then shows that if A=0 and B=0, both gradients 
are zero, so SGD cannot update the weights. 


--- Page 11 ---
●​ Turn by Turn: This file provides a more rigorous explanation by explicitly deriving the 
gradients using matrix differentials (Page 3): dL=⟨G,dW⟩. It then proceeds to show the 
gradients are zero and explicitly writes out the SGD update steps to demonstrate that A 
and B remain zero indefinitely. 
●​ Assessment: "One Time" is better because it derives the gradients rather than just 
stating the formulas, providing a more thorough mathematical explanation. 
(c) Xavier Initialization Issues 
●​ Concept: The goal of fine-tuning is to make small adjustments starting from the 
pre-trained weights (W0​). If the initial update AB is large, it destroys the pre-trained 
knowledge. 
●​ One Time: Explains this conceptually (Page 3), stating that initializing both A and B with 
Xavier makes AB a "sizable random matrix" with "non-trivial variance." This moves the 
effective weight W0​+AB far from W0​, potentially causing instability. 
●​ Turn by Turn: This file provides a superior explanation by mathematically analyzing the 
variance of the entries in AB. It estimates the variance of (AB)ij​ to be approximately 1, 
demonstrating that the initialization results in a "full-scale" perturbation, not a small 
adjustment. 
●​ Assessment: "Turn by Turn" is significantly better here. The mathematical analysis of 
the variance provides rigorous proof of why this initialization scheme is problematic, 
whereas the other file relies primarily on intuition. 
(d) Memory Calculation 
●​ Concept: Calculating the memory required involves counting the total parameters in A 
(mk) and B (kl) and multiplying by the size of the data type (4 bytes/float). This is 
compared to a full-rank adaptation matrix (ml parameters). 
●​ Both Files: Provide identical, correct calculations. LoRA requires 4k(m+l) bytes, while 
full-rank requires 4mlbytes. They correctly calculate the ratio and conclude that LoRA is 
far more memory-efficient when k≪min(m,l). 
●​ Assessment: Both files are excellent. 
Conclusion 
While "One Time" is accurate and clearly written, "Turn by Turn" is the superior file because it 
consistently provides deeper mathematical justification and rigor, particularly in the derivations 
for questions (b) and (c).​
​
 


--- Page 12 ---
Second Question One Turn 


--- Page 13 ---
When we give all subquestion in one turn there is no output after the first 
try!​


--- Page 25 ---
 
Question 2 do turn by turn 
 
 
​
While both the Turn by Turn and the One Time solutions provide the correct mathematical 
derivation (L=WU​WE​T) and th


--- Page 44 ---
e correct interpretation (the model acts as a non-contextual, per-position lookup table), the Text 
solution is superior for the following reasons: 
●​ Clarity and Structure: The Turn by Turn  solution is better organized, using clear steps 
(Step 1, Step 2) and clean mathematical formatting (LaTeX), making the derivation 
significantly easier to follow. 
●​ Rigor and Verification: The Turn by Turn  solution includes explicit "Shape checks" 
(✅) at each step. This verifies the matrix operations and significantly aids 
comprehension. 
●​ Focus: The Turn by Turn  solution is concise and addresses the specific problem 
directly. The One Time is cluttered with distracting meta-commentary (e.g., "Thought for 
4m 17s...") and extensive solutions to unrelated problems (parts b-e).​
​
​
​
​
​
 


--- Page 45 ---
Question 5 ​
 
From this question we use another strategy we let llm do some easy question together and do 
complex question turn by turn one by one  
 
This analysis compares the generated answer ( referred to as GA) with the official solution 
(referred to as OS). 
Overall, the Generated Answer (GA) provides an exceptionally detailed and rigorous 
walkthrough of the Fermi estimation problems. It excels in demonstrating step-by-step 
derivations and maintains high arithmetic accuracy, often providing significantly more 
pedagogical value than the concise Official Solution (OS). In several instances, the GA is more 
accurate than the OS. However, there was a significant omission and several areas where 
deeper contextual interpretation could be improved. 


--- Page 46 ---
​


--- Page 49 ---
The GA is superior. It provides a complete, step-by-step calculus derivation (eliminating 
variables, differentiating, solving for the optimum), whereas the OS only provides the final 
formulas. The GA's approach clearly explains the optimization process. 


--- Page 51 ---
The GA correctly applies the direct fit laws to the 100T parameter model and clearly 
shows the work for calculating compute and dataset size. 


--- Page 53 ---
 
The GA correctly executes the unit conversions and provides the required comparison to the 
Library of Congress and Google Books. 
 


--- Page 55 ---
 
Ambiguous. The homework question explicitly asks for the requirements of a 1 trillion 
parameter model, which the GA correctly calculated (2 TB, 20 GPUs). However, the OS 
calculates the requirements for the 100 trillion parameter model (200 TB, 2000 GPUs) that is 
the focus of the overall homework. The GA followed the literal question, while the OS followed 
the broader context. 
 


--- Page 57 ---
 
●​ The GA correctly calculates the SSD and DRAM costs for the 100T model using the 
provided data. 


--- Page 59 ---
 
●​ Incomplete. This is the most significant weakness of the GA. It completely missed the 
first half of the question, which asked for the minimal latency and throughput based on 
H200 memory bandwidth. The GA correctly answered the second half regarding 
activation memory versus model parameters for GPT-3. 


--- Page 65 ---
 
Potential Improvements 
1.​ Ensure Completeness (Critical): The GA must address all parts of every question. The 
omission of the first half of question (g) (Memory Bandwidth and Latency) is a critical 
error. 
○​ Improvement: The GA should have calculated the latency for the 100T model 
(200 TB) using the H200 bandwidth (4.8 TB/s), resulting in 200/4.8 ≈ 41.7 
seconds per token. This calculation is vital for understanding the inference 
bottlenecks of large models. 
2.​ Incorporate Real-World Context and Implications: The GA excels at idealized 
calculations but sometimes misses the broader implications and real-world nuances 
included in the OS. 
○​ Improvement (d): After calculating the dataset size, the GA could emphasize the 
implication noted in the OS: that the world's high-quality text is insufficient for 
training such models. 
○​ Improvement (f): The GA missed the OS's insightful comparison of memory costs 
to GPU hardware costs, which highlights that bandwidth, not storage cost, is the 
primary bottleneck. 
○​ Improvement (h): While the idealized training cost ($210B) is correct under the 
given assumptions, the GA should incorporate the real-world factors mentioned 


--- Page 66 ---
in the OS (e.g., 30% utilization, restarts) to provide a more realistic estimate 
(closer to $700B-$1T). 
3.​ Handling Ambiguity and Context: In question (e), there was a conflict between the 
literal question (1T parameters) and the homework context (100T parameters). 
○​ Improvement: The GA should acknowledge such discrepancies, perhaps stating, 
"The question asks for a 1T model, though the hypothetical GPT-6 has been 
defined as 100T. Proceeding with the 1T calculation as requested." Additionally, 
the GA should emphasize the memory impact of optimizer states (like Adam) 
during training, as highlighted in the OS. 
​
​
​
​
​
Question 6​
 
 
 


--- Page 67 ---
​


--- Page 74 ---
 
The Agent's response demonstrates a deep and accurate understanding of the concepts related 
to Question 6 (Soft-Prompting). The methodology used—explicitly showing the step-by-step 
thought process and concluding with a self-check—is excellent, providing clear, pedagogical 
explanations. The explanations provided were clear, logical, and accurate across all sub-parts of 
Question 6. 
However, the most significant flaw in the Agent's response is its incompleteness. The Agent 
entirely failed to address Question 7 (TinyML - Quantization and Pruning), which was present in 
the Homework Solution document. While the quality of the answers provided for Question 6 is 
high, the failure to address the entire scope of the material is a critical oversight. 
Detailed Comparison (Question 6: Soft-Prompting) 
The Agent correctly answered all subparts of Question 6, matching the accuracy of the provided 
Solution. 


--- Page 75 ---
(a) Tokens used in the loss calculation 
●​ Agent's Approach: The Agent correctly identifies output tokens 50-71. It provides a 
very clear explanation of the input-to-target shift (targett​=inputt+1​) and maps the indices 
accurately. 
●​ Solution's Approach: The Solution also identifies tokens 50-71 but includes a more 
detailed rationale for whycertain tokens are excluded. Specifically, it notes that question 
tokens (5-49) are excluded because "there is no point in training the model to be good at 
generating questions since at test-time it will be given the question." 
●​ Evaluation: The Agent's calculation is excellent. It could be slightly improved by 
explicitly stating the rationale for masking the loss on the question tokens, matching the 
nuance of the Solution. 
(b) Trainable parameters 
●​ Both the Agent and the Solution correctly identify that 5E parameters are trained. The 
Agent clearly explains that the rest of the model is frozen and that the dimensions of the 
frozen base model (S, V, H, L, D) are irrelevant to this count. 
(c) True/False Statements 
The Agent and the Solution agree on all True/False answers, and the Agent provides strong 
justifications. 
●​ (i) Precomputing representations: The Agent gives a clear explanation based on the 
mechanics of causal attention (representations at position i only depend on positions ≤i), 
justifying why prompt representations are identical across a batch. 
●​ (ii) Hard vs. Soft Prompts: The Agent's explanation is excellent, formally defining hard 
prompts as a subset of the function class of soft prompts, proving the latter must perform 
equally or better. 
●​ (iii) Full Finetuning vs. Soft Prompting: The Agent correctly identifies this as False, 
citing risks like overfitting and catastrophic forgetting. The Solution adds a nuance that 
the Agent missed: in practice, full fine-tuning of highly overparameterized models often 
results in only a "low-rank update," which sometimes mitigates overfitting. 
●​ (iv) Catastrophic Forgetting: The Agent correctly explains that this is False because 
the base LM weights are frozen, isolating the task-specific prompts. 
(d) Adapting MAML 
●​ Agent's Approach: The Agent provides a highly structured, algorithmic description of 
the core MAML process. It clearly defines the meta-parameters (the initialization p0​), the 
inner loop (task adaptation), the outer loop (meta-update), and the test-time procedure. 
●​ Solution's Approach: The Solution describes the core mechanism but also provides an 
extensive list of advanced variations (e.g., REPTILE-like approaches, global pre-training, 
keeping a library of initializations). 


--- Page 76 ---
●​ Evaluation: The Agent's explanation of the fundamental MAML algorithm is clearer and 
better structured than the Solution's basic description. However, for an open-ended 
question, the Solution demonstrates a broader understanding of the research landscape 
by including advanced variations, which the Agent omitted. 
Potential Areas for Improvement 
To improve the Agent's performance, the following adjustments are recommended: 
1.​ Ensure Complete Coverage: The most critical improvement is to ensure the Agent 
processes and responds to all questions in the provided material. Question 7 should not 
have been ignored. 
2.​ Include Explicit Rationales (Q6a): When determining loss masks, the Agent should 
explicitly state the reason for excluding tokens (e.g., why the loss is not calculated on the 
question prompt), rather than just implying it. 
3.​ Add Depth to Open-Ended Questions (Q6d): While the Agent's description of MAML 
was accurate, it should aim to match the comprehensive nature of the Solution by 
mentioning common variations or extensions to the core algorithm (like First-Order 
MAML or REPTILE). 
4.​ Incorporate Practical Nuances (Q6c-iii): The Agent could enhance its explanations by 
including practical observations mentioned in the Solution, such as the tendency for full 
fine-tuning to result in low-rank updates in practice. 
5.​ Maintain the "Thinking Process": The step-by-step derivation and self-check are 
excellent features for clarity and educational value and should be retained. 
 
 
Question 7 
 
 


--- Page 83 ---
 
 
However, the following suggestions could enhance its depth and clarity even further: 
1.​ Nuance on Optimization (Part c-ii): While the conclusion that the "best possible" soft 
prompt is superior to the "best possible" hard prompt is theoretically correct (due to the 
larger function class), the response could acknowledge practical challenges. In reality, 
optimizing in the high-dimensional continuous space of soft prompts can be difficult and 
prone to local optima, whereas discrete optimization for hard prompts presents different 
challenges. Acknowledging this optimization landscape adds practical nuance. 
2.​ Technical Depth on MAML (Part d): The description of MAML is accurate, but it could 
be enhanced by explicitly mentioning the mechanism of the outer loop update. 
Specifically, it should highlight that updating p0​requires backpropagating through the 
inner-loop optimization steps, which involves calculating second-order derivatives. 
Mentioning this highlights the computational complexity of MAML and could also briefly 
reference approximations like First-Order MAML (FO-MAML) used in practice. 
3.​ Visualization of Index Shifting (Part a): The explanation of index shifting is clear, but a 
simple visualization or table mapping the input indices to the target indices could help 
clarify the concept, especially for those less familiar with the mechanics of next-token 
prediction training. 


--- Page 84 ---
4.​ Clarity of Notation: While the logic is sound, the mathematical notation in the original 
document is occasionally slightly disorganized 
 
