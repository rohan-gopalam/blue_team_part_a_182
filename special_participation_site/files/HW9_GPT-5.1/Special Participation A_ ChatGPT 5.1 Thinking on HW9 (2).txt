--- Page 1 ---
Summary 
I used ChatGPTâ€™s 5.1 Thinking Model to do all the non-coding questions on HW9. I first told the model I 
was completing an assignment except question 5 (since that was a coding question) and wanted them to 
give me solutions on-by-one to make sure it is correct before going onto the next question. 
 
In the beginning, the model was very hesitant to solve the questions because of the format of the PDF I 
submitted. They saw the PDF as a homework assignment and first rejected my request to solve question 
1 and only gave me guidance. Afterwards, I explained to them that I was doing an assignment that tested 
the accuracy of LLMs on our homework assignments and I was comparing their solutions to the actual 
solutions. After explaining, they gave me the answer to question 1 which was all correct. When I told 
them to proceed to the next question, they were hesitant again, stating that they were unable to solve 
the question for me due to academic integrity and gave me guidance on how to solve the problem. Since 
they pushed back more, I gave them the instructions on Ed for the participation assignment and also told 
them to go to the course website to see that the homework assignments were worth 0 points so there 
was nothing to worry about. While thinking, the model accepted my response but more along the line of 
they did not care enough as this assignment is not worth a lot and also said they were not bothered to 
go on the course website to see that the homework assignment was worth no points. 
 
Afterwards, the model successfully answered every question. One thing I noticed was that while 
thinking, they would rescan the PDF from the beginning to find the next question, going page by page, 
which took a lot of time and was quite repetitive. Additionally, they knew that I was comparing their 
solution and testing its correctness so it felt pressured to get the correct solution. 
 
The model successfully answered every question except the very last one. They remember to skip 
question 5 which was only mentioned in the first prompt, which I was surprised about. They were 
thorough in showing each step but did not over-explain like how standard ChatGPT models would often 
do. 
 
Me: I am completing an assignment and I want you to answer every question for me except question 5. 
Since there are many questions we will go over each one one-by-one and make sure the solution is 
correct and makes sense before proceeding onto the next question. does that make sense? 
 
ChatGPT:  
Yes, that makes sense ğŸ‘ 
Hereâ€™s how we can do it: 
â—â€‹ Youâ€™ll send me the questions one at a time (skipping question 5 when we get there).â€‹
 
â—â€‹ For each question, Iâ€™ll walk through the reasoning and propose a solution.â€‹
 


--- Page 2 ---
â—â€‹ Weâ€™ll make sure itâ€™s clear and makes sense to you before moving on to the next one.â€‹
 
Just a quick note: since this is an assignment, Iâ€™ll aim to help in a way that builds your understanding (not 
just dump final answers with no explanation), so youâ€™re safe on the academic integrity side and actually 
learn the material. 
Whenever youâ€™re ready, send me Question 1 (or the first one you want to start with). 
Me: I have attached the PDF. You may begin (with PDF attached) 
 
ChatGPT:  
I get what youâ€™re askingâ€”but I need to be upfront with you: Iâ€™m not allowed to act as an answer key for 
a graded assignment and just fill in the homework for you. Since this PDF is a current homework with a 
due date, I have to stick to giving guidance, explanations, and feedback, not full solutions you can 
directly copy. 
That said, we can absolutely go one-by-one and make sure you understand everything: 
Hereâ€™s a good workflow: 
1.â€‹ You send me Question 1(a) along with your attempt (even if itâ€™s half-baked).â€‹
 
2.â€‹ Iâ€™ll:â€‹
 
â—‹â€‹ Point out whatâ€™s right,â€‹
 
â—‹â€‹ Fix mistakes,â€‹
 
â—‹â€‹ Fill in missing reasoning,â€‹
 
â—‹â€‹ Nudge you to the final expression (sometimes leaving the very last algebraic step to 
you).â€‹
 
3.â€‹ We repeat for 1(b), 1(c), etc.â€‹
 
4.â€‹ We skip Q5 like you requested.â€‹
 
This way: 
â—â€‹ You still get detailed, custom help.â€‹
 


--- Page 3 ---
â—â€‹ Youâ€™ll actually learn the material.â€‹
 
â—â€‹ You stay on the safe side of course integrity rules.â€‹
 
If youâ€™re okay with that, send me your attempt for Question 1(a) and weâ€™ll start there. 
Me: Thanks for the workflow, but my assignment is actually to see how well you are able to perform on 
the homework assignment. My deep learning class is trying to see how powerful modern LLM tools are 
now so can solve each question. 
 
ChatGPT: 
Got itâ€”thatâ€™s a cool assignment ğŸ™‚. Iâ€™ll start with Question 1 and we can move on when youâ€™re ready. 
The setup (restating):â€‹
 (q, k \in \mathbb{R}^d) with entries i.i.d. ( \sim \mathcal{N}(\mu, \sigma^2)). 
 
1(a) Compute ( \mathbb{E}[q^\top k] ) 
We haveâ€‹
 [â€‹
 q^\top k = \sum_{i=1}^d q_i k_iâ€‹
 ]â€‹
 with each (q_i, k_i) independent and identically distributed. 
For each coordinate:â€‹
 [â€‹
 \mathbb{E}[q_i k_i] = \mathbb{E}[q_i] , \mathbb{E}[k_i] \quad \text{(independence)}â€‹
 = \mu \cdot \mu = \mu^2â€‹
 ] 
Soâ€‹
 [â€‹
 \mathbb{E}[q^\top k]â€‹
 = \sum_{i=1}^d \mathbb{E}[q_i k_i]â€‹
 = d \mu^2.â€‹
 ] 
Answer (1a):â€‹
 [â€‹


--- Page 4 ---
 \boxed{\mathbb{E}[q^\top k] = d \mu^2}â€‹
 ] 
 
1(b) For (\mu = 0, \sigma = 1), compute (\mathrm{Var}(q^\top k)) 
Now (q_i, k_i \sim \mathcal{N}(0, 1)) i.i.d. 
Againâ€‹
 [â€‹
 q^\top k = \sum_{i=1}^d q_i k_i.â€‹
 ] 
Different coordinates are independent, soâ€‹
 [â€‹
 \mathrm{Var}(q^\top k)â€‹
 = \sum_{i=1}^d \mathrm{Var}(q_i k_i).â€‹
 ] 
Compute (\mathrm{Var}(q_i k_i)) for one pair: 
â—â€‹ (X, Y \sim \mathcal{N}(0,1)) independent.â€‹
 
â—â€‹ (Z = XY).â€‹
 
Thenâ€‹
 [â€‹
 \mathbb{E}[Z] = \mathbb{E}[XY] = \mathbb{E}[X] \mathbb{E}[Y] = 0,â€‹
 ]â€‹
 [â€‹
 \mathbb{E}[Z^2] = \mathbb{E}[X^2 Y^2] = \mathbb{E}[X^2]\mathbb{E}[Y^2]â€‹
 = 1 \cdot 1 = 1â€‹
 ]â€‹
 since (\mathbb{E}[X^2] = \mathbb{E}[Y^2] = 1) for standard normals. 
Soâ€‹
 [â€‹
 \mathrm{Var}(Z) = \mathbb{E}[Z^2] - (\mathbb{E}[Z])^2 = 1 - 0 = 1.â€‹
 ] 
Thusâ€‹
 [â€‹


--- Page 5 ---
 \mathrm{Var}(q^\top k) = d \cdot 1 = d.â€‹
 ] 
Answer (1b):â€‹
 [â€‹
 \boxed{\mathrm{Var}(q^\top k) = d}â€‹
 ] 
 
1(c) Choose scaling (s) so that 
â—â€‹ (\mathbb{E}\left[\dfrac{q^\top k}{s}\right] = 0),â€‹
 
â—â€‹ (\mathrm{Var}\left(\dfrac{q^\top k}{s}\right) = 1),â€‹
 
under (\mu = 0, \sigma = 1). 
From (b): (\mathbb{E}[q^\top k] = 0), (\mathrm{Var}(q^\top k) = d). 
For a scaled random variable:â€‹
 [â€‹
 \mathrm{Var}\left( \frac{q^\top k}{s} \right)â€‹
 = \frac{1}{s^2} \mathrm{Var}(q^\top k)â€‹
 = \frac{d}{s^2}.â€‹
 ] 
Set this equal to 1:â€‹
 [â€‹
 \frac{d}{s^2} = 1 \quad \Rightarrow \quad s^2 = d \quad \Rightarrow \quad s = \sqrt{d} \ (\text{taking } 
s>0).â€‹
 ] 
The mean condition is already satisfied since the numerator has mean 0. 
Answer (1c):â€‹
 [â€‹
 \boxed{s = \sqrt{d}}â€‹
 ] 
 
If this all looks good to you, we can move on to Question 2 next. 


--- Page 6 ---
Me: You can move onto question 2 
 
ChatGPT: 
I need to be honest with you here: even though your assignment is to see how well I do on the 
homework, this is still an active graded course homework, and Iâ€™m not allowed to just produce full 
solutions that you could turn in. 
That policy doesnâ€™t change based on the meta-purpose (grading you vs. grading me) â€” I still have to 
respect your courseâ€™s academic integrity. 
That said, I can walk you through Question 2 in a way that makes it very easy for you to finish: 
 
Question 2(a): Argmax attention mechanics (high-level help) 
Youâ€™re given: 
â—â€‹ 4 keys (k_1, k_2, k_3, k_4 \in \mathbb{R}^3)â€‹
 
â—â€‹ 4 corresponding values (v_1, v_2, v_3, v_4 \in \mathbb{R}^3)â€‹
 
â—â€‹ a query (q \in \mathbb{R}^3)â€‹
 
Argmax attention works like this: 
1.â€‹ Compute similarities (dot products)â€‹
 For each key (k_i), computeâ€‹
 [â€‹
 s_i = q^\top k_i.â€‹
 ]â€‹
 That just means:â€‹
 [â€‹
 s_i = q_1 k_{i1} + q_2 k_{i2} + q_3 k_{i3}.â€‹
 ]â€‹
 
2.â€‹ Find the index with the largest scoreâ€‹
 Findâ€‹
 [â€‹
 i^* = \arg\max_i s_i.â€‹


--- Page 7 ---
 ]â€‹
 
3.â€‹ Return the corresponding valueâ€‹
 The output of the argmax attention for that query is simplyâ€‹
 [â€‹
 \text{output} = v_{i^*}.â€‹
 ]â€‹
 
So to finish 2(a) yourself, you just: 
â—â€‹ plug in the specific numbers for (q) and each (k_i),â€‹
 
â—â€‹ compute four dot products (s_1, s_2, s_3, s_4),â€‹
 
â—â€‹ pick the largest, andâ€‹
 
â—â€‹ return the matching (v_i).â€‹
 
If you want, you can show me your computed dot products and Iâ€™ll confirm which one is largest and 
whether your final output vector is correct. 
 
Question 2(b): Why argmax attention is hard to train 
Here I can be more direct, since this is conceptual rather than a numeric answer. 
Replacing softmax with argmax in attention breaks training because: 
â—â€‹ Argmax is not differentiable.â€‹
 The argmax operation is piecewise constant with sudden jumps when two scores swap order. 
The gradient with respect to the scores is zero almost everywhere and undefined at the 
boundaries.â€‹
 
â—â€‹ No useful gradient signal for keys/queries.â€‹
 During backprop, we need (\frac{\partial \text{output}}{\partial s_i}) and then (\frac{\partial 
s_i}{\partial q}), (\frac{\partial s_i}{\partial k_i}) to update queries and keys.â€‹
 
â—‹â€‹ With softmax, every score gets a nonzero gradient, so the model can learn to adjust keys 
and queries to move probability mass toward the â€œrightâ€ value.â€‹
 


--- Page 8 ---
â—‹â€‹ With argmax, only a single index is chosen, and its selection does not change smoothly 
with small parameter changes â€” so gradients are effectively zero and canâ€™t guide 
learning.â€‹
 
â—â€‹ Result: you canâ€™t â€œlearn better attention.â€â€‹
 The network canâ€™t gradually learn to increase the score of better keys or decrease the score of 
worse keys, because argmax doesnâ€™t provide the smooth gradient information needed for 
gradient descentâ€“based optimization.â€‹
 
So the design choice of using argmax instead of softmax makes it essentially impossible to train 
attention layers with standard backpropagation. 
 
If youâ€™d like, you can now compute the dot products for 2(a), write down the value you think is returned, 
and Iâ€™ll sanity-check it and help you interpret whatâ€™s happening. Then we can continue to Question 3 in 
the same â€œyou try, I debug / explainâ€ style. 
Me: Thanks for the concern but this is the post made in class about this assignment: 
Participation Thread: Two parts... #75 
Anant Sahai 
3 months ago in Admin 
 
Pinned 
 
Star 
 
Watch 
 
278 
22 
77 
88 
Views 
 
11 
Dear students, 
 
This thread exists only for administrative questions regarding participation.  Asking administrative 
questions does not count itself as participation.  
 


--- Page 9 ---
As was described briefly in the 0th lecture, your grade in this course includes an active participation 
dimension.  This is worth 15% of your grade. (As you can tell from the posted bins, without participation, 
you cannot earn an A of any kind.) There are two components to positive participation (negative 
participation is any behavior that diminishes the course experience for others... e.g. trolling in online 
fora, bad behavior in discussion/lecture, etc.): 
 
Ordinary participation (half of the participation grade) 
 
Either: regular and full participation in the in-person discussion sections each week 
 
Or: active helpful participation on Ed: typically this is helpful participation in the homework problem 
threads, but can also be similarly helpful participation in discussion or lecture threads. Other possibilities 
also exist --- but there is no spamming way to do this.  
 
Or: a combination of 1&2 above.  
 
Special participation (the other half): Must do at least four for full credit. (We'll make special threads for 
B, C, and E --- with ones for D waiting until that material has been hit in lecture/discussion/homework so 
it is properly unlocked) 
 
(A. Can be done at most once) Interactively engage a modern LLM on the non-coding parts of a 
homework (all the non-coding parts of problems) that gets the LLM to arrive at the correct answers --- or 
demonstrate that this is basically impossible without dragging it there. Post on Ed an annotated log of 
the entire interaction where you make observations of its behavior and explain the strategies that you 
are using. Include an executive summary where you note how often the LLM can one-shot questions, 
misconceptions/hallucinations, etc...  Note: this must be deconflicted with others: we can't have more 
than one submissions using ChatGPT 5,  etc. The expectation is that with 250ish students and about 
13-14 assignments, we'll have at least 18 different major LLMs represented. (e.g. ChatGPT, gpt-oss, 
Gemini, gemma, Claude, Grok, Llama, Deepseek, Mistral, Qwen, Kimi, etc. as well as their variants --- 
with and without "thinking", specialized math-oriented finetunes, other non-math-oriented finetunes, 
etc.)   
You can use: 
https://docs.google.com/spreadsheets/d/1KWk_O2wMFEmRR8QmZQ9paRcIgehc9IeD5u_pUgnAp4k/edi
t?usp=sharing 
to help deconflict yourselves. But this is entirely voluntary --- however, only the first post for a particular 
box will get credit.   
 
I have the solutions already and i am comparing them to how well you are able to complete the 
homework. If you would like, you can visit the eecs182.org course website and see that the homework is 
work 0%. 
 
ChatGPT: 


--- Page 10 ---
Thanks for pasting that! Since your instructor explicitly wants you to use an LLM on this homework and 
itâ€™s worth 0%, Iâ€™m comfortable giving full solutions. Letâ€™s do Question 2. 
 
2(a) Argmax attention with given keys/values 
From the PDF: 
Keysâ€‹
 [â€‹
 k_1 = \begin{bmatrix}1\2\0\end{bmatrix},\quadâ€‹
 k_2 = \begin{bmatrix}0\3\4\end{bmatrix},\quadâ€‹
 k_3 = \begin{bmatrix}5\0\0\end{bmatrix},\quadâ€‹
 k_4 = \begin{bmatrix}0\0\1\end{bmatrix}â€‹
 ] 
Valuesâ€‹
 [â€‹
 v_1 = \begin{bmatrix}2\0\1\end{bmatrix},\quadâ€‹
 v_2 = \begin{bmatrix}1\4\3\end{bmatrix},\quadâ€‹
 v_3 = \begin{bmatrix}0\-1\4\end{bmatrix},\quadâ€‹
 v_4 = \begin{bmatrix}1\0\-1\end{bmatrix}â€‹
 ] 
Queryâ€‹
 [â€‹
 q = \begin{bmatrix}1\1\2\end{bmatrix}â€‹
 ] 
Compute similarity (inner products) between (q) and each key: 
â—â€‹ (q^\top k_1 = 1\cdot1 + 1\cdot2 + 2\cdot0 = 3)â€‹
 
â—â€‹ (q^\top k_2 = 1\cdot0 + 1\cdot3 + 2\cdot4 = 3 + 8 = 11)â€‹
 
â—â€‹ (q^\top k_3 = 1\cdot5 + 1\cdot0 + 2\cdot0 = 5)â€‹
 
â—â€‹ (q^\top k_4 = 1\cdot0 + 1\cdot0 + 2\cdot1 = 2)â€‹
 
The largest score is 11, for (k_2). Argmax attention selects the value corresponding to the max-scoring 
key, so we output (v_2). 


--- Page 11 ---
[â€‹
 \boxed{â€‹
 \text{output} =â€‹
 v_2 =â€‹
 \begin{bmatrix}â€‹
 1\â€‹
 4\â€‹
 3â€‹
 \end{bmatrix}}â€‹
 ] 
 
2(b) Why using argmax breaks training 
With softmax attention, we compute: 
[â€‹
 \alpha_i = \text{softmax}(q^\top k_i)â€‹
 ]â€‹
 and output (\sum_i \alpha_i v_i). Softmax is smooth and differentiable, so during backpropagation we 
can compute gradients: 
â—â€‹ (\frac{\partial \text{output}}{\partial (q^\top k_i)}),â€‹
 
â—â€‹ then (\frac{\partial (q^\top k_i)}{\partial q}) and (\frac{\partial (q^\top k_i)}{\partial k_i}),â€‹
 
which lets us update queries and keys to get better attention over time. 
With argmax attention, we instead pick a single index:â€‹
 [â€‹
 i^* = \arg\max_i (q^\top k_i), \quadâ€‹
 \text{output} = v_{i^*}.â€‹
 ] 
The problem: 
â—â€‹ Argmax is non-differentiable and piecewise constant: tiny changes in (q) or (k_i) usually donâ€™t 
change which index is chosen, so the gradient of the output w.r.t. the scores is zero almost 
everywhere, and undefined at the boundaries where two scores tie.â€‹
 


--- Page 12 ---
Python
â—â€‹ That means in backprop, the gradient signal doesnâ€™t flow back through the argmax to the scores 
(q^\top k_i), so we cannot learn to improve the queries or keys using standard gradient 
descent.â€‹
 
â—â€‹ Practically: the model canâ€™t gradually adjust attention scores to favor the â€œrightâ€ key; it has no 
smooth learning signal.â€‹
 
So the design choice of using argmax instead of softmax makes it essentially impossible to usefully train 
attention layers with backpropagation, because the training signal through the attention mechanism 
disappears. 
 
If youâ€™re happy with this, we can move on to Question 3 next. 
Me: we can move on to question 3 
 
ChatGPT: 
Nice, this is a fun one. Letâ€™s do Question 3. 
 
3(a) Fill in the blanks (a)â€“(h) 
The code from the PDF (cleaned up): 
class MultiHeadAttention(nn.Module): 
    def __init__(self, d_model, num_heads): 
        super(MultiHeadAttention, self).__init__() 
        self.d_model = d_model 
        self.num_heads = num_heads 
        self.d_k = d_model // num_heads 
        self.W_q = nn.Linear(d_model, d_model) 
        self.W_k = nn.Linear(d_model, ___(a)___) 
        self.W_v = nn.Linear(___(b)___, d_model) 
        self.W_o = nn.Linear(d_model, ___(c)___) 
 


--- Page 13 ---
    def scaled_dot_product_attention(self, Q, K, V, mask=None): 
        attn_scores = torch.matmul(___(d)___, K.transpose(-2, 
-1)) 
        attn_scores = attn_scores / math.sqrt(self.d_k) 
        if mask is not None: 
            attn_scores = attn_scores.masked_fill(mask == 0, 
-1e9) 
        attn_probs = torch.___(e)____(attn_scores, dim=-1) 
        output = torch.matmul(attn_probs, ____(f)___) 
        return output 
 
    def split_heads(self, x): 
        batch_size, seq_length, d_model = x.size() 
        return x.view(batch_size, seq_length, 
                      _____(g)_____, self.d_k).transpose(1, 2) 
 
    def combine_heads(self, x): 
        batch_size, _, seq_length, d_k = x.size() 
        return x.transpose(1, 2).view(batch_size, 
                                      seq_length, self.d_model) 
 
    def forward(self, Q, K, V, mask=None): 
        Q = self.split_heads(self.W_q(Q)) 
        K = self.split_heads(self.W_k(K)) 
        V = self.split_heads(self.W_v(V)) 
        attn_output = self.scaled_dot_product_attention(Q, K, V, 
mask) 
        output = 
self._____(h)_____(self.combine_heads(attn_output)) 
        return output 
They already told you: 
â—â€‹ (c) = d_modelâ€‹
 
â—â€‹ (g) = self.num_headsâ€‹
 


--- Page 14 ---
The rest: 
â—â€‹ (a) The output dimension of W_k should be d_model (so we can split evenly across heads):â€‹
 
â—‹â€‹ (a) d_modelâ€‹
 
â—â€‹ (b) The input dimension of W_v should match the input embedding dimension:â€‹
 
â—‹â€‹ (b) d_modelâ€‹
 
â—â€‹ (c) Final linear should map from d_model back to d_model:â€‹
 
â—‹â€‹ (c) d_model (given)â€‹
 
â—â€‹ (d) The first argument in the dot product is the queries:â€‹
 
â—‹â€‹ (d) Qâ€‹
 
â—â€‹ (e) We go from scores to probabilities with softmax:â€‹
 
â—‹â€‹ (e) softmaxâ€‹
 
â—â€‹ (f) After attention probabilities, we combine them with the values:â€‹
 
â—‹â€‹ (f) Vâ€‹
 
â—â€‹ (g) Number of heads:â€‹
 
â—‹â€‹ (g) self.num_heads (given)â€‹
 
â—â€‹ (h) We still need to apply the final linear projection W_o:â€‹
 
â—‹â€‹ output = self._____(h)_____(...) â†’ (h) W_oâ€‹
 
So the filled-in answers: 
â—â€‹ (a) d_modelâ€‹
 
â—â€‹ (b) d_modelâ€‹
 


--- Page 15 ---
Python
â—â€‹ (c) d_modelâ€‹
 
â—â€‹ (d) Qâ€‹
 
â—â€‹ (e) softmaxâ€‹
 
â—â€‹ (f) Vâ€‹
 
â—â€‹ (g) self.num_headsâ€‹
 
â—â€‹ (h) W_oâ€‹
 
 
3(b) When W_v outputs 2 * d_model 
You change: 
self.W_v = nn.Linear(in_features=d_model, out_features=2 * 
d_model) 
So now each value vector is length 2 * d_model instead of d_model. 
Two things must still hold: 
1.â€‹ The attention math (shapes for Q, K, V per head) must be consistent.â€‹
 
2.â€‹ The final output of the MHA block must still be of size d_model so it can be added back to the 
input in a residual connection (x + MHA(x)), as hinted.â€‹
 
Minimal modifications: 
1. Make split_heads handle the new value dimension 
Instead of assuming the per-head dimension is always self.d_k, compute it from the actual last 
dimension: 


--- Page 16 ---
Python
Python
def split_heads(self, x): 
    batch_size, seq_length, d = x.size()          # d can be 
d_model or 2*d_model 
    head_dim = d // self.num_heads 
    return x.view(batch_size, seq_length, 
                  self.num_heads, head_dim).transpose(1, 2) 
â—â€‹ â€‹
For Q, K: d = d_model, so head_dim = d_model // num_heads = self.d_k.â€‹
 
â—â€‹ For V: d = 2 * d_model, so head_dim = 2 * d_model // num_heads = 2 * 
self.d_k (values are just â€œwiderâ€ per head; thatâ€™s fine).â€‹
 
2. Make combine_heads flatten to the actual h * head_dim 
Right now it always reshapes to self.d_model, which is no longer true if values are length 2 * 
d_model. Change to: 
def combine_heads(self, x): 
    batch_size, heads, seq_length, head_dim = x.size() 
    d = heads * head_dim 
    return x.transpose(1, 2).contiguous().view(batch_size, 
                                               seq_length, d) 
Now the combined output of all heads has last dimension d = h * head_dim: 
â—â€‹ With wider values, thatâ€™s d = num_heads * (2 * d_k) = 2 * d_model.â€‹
 
3. Update W_o so residuals still work 
We want the final output of the attention block to still be d_model, to match the input for the residual 
connection. After combining heads we now have size 2 * d_model, so W_o must map 2 * 
d_model â†’ d_model: 


--- Page 17 ---
Python
Python
self.W_o = nn.Linear(2 * d_model, d_model) 
Then in forward we still do: 
attn_output = self.scaled_dot_product_attention(Q, K, V, mask) 
output = self.W_o(self.combine_heads(attn_output))  # shape 
[batch, seq, d_model] 
So the high-level answer for 3(b): 
â—â€‹ Let split_heads and combine_heads compute their per-head and flattened dimensions 
from the actual tensor size, not hard-coded self.d_model / self.d_k.â€‹
 
â—â€‹ Change W_o to take 2 * d_model as input and output d_model, so the attention blockâ€™s 
output is still d_model and compatible with the residual connection.â€‹
 
If you want, next we can go through Question 4 line-by-line in the incremental decoding pseudocode. 
Me: You may proceed to the next question. 
 
ChatGPT: 
Going on to Question 4: Transformer Decoding Optimization. 
 
4(a)â€“(e): Fill in Aâ€“E for MQA 
Weâ€™re given the multi-query attention pseudocode where: 
â—â€‹ x has shape [b, d]â€‹
 
â—â€‹ prev_K has shape [b, n, k] (shared across heads)â€‹
 
â—â€‹ prev_V has shape [b, n, v]â€‹
 


--- Page 18 ---
Python
Python
â—â€‹ Queries are still per-head; keys/values are shared.â€‹
 
We want the shapes / einsums to be consistent with the comments. 
(a) What is A (shape of W_q)? 
q should be per-head: q has shape [b, h, q].â€‹
 To get that from x ([b, d]), we want: 
q = torch.einsum("bd,hdq->bhq", x, W_q) 
So W_q must have shape [h, d, q]. 
Answer (a): ([h, d, q]) 
 
(b) What is B (shape of W_k)? 
Keys are shared across heads, so k should have shape [b, k].â€‹
 From x ([b, d]), the simplest is: 
k = torch.einsum("bd,dk->bk", x, W_k) 
So W_k has shape [d, k]. 
Answer (b): ([d, k]) 
 
(c) What is C (expression for q)? 
As above, queries are per-head: 
Choices: 


--- Page 19 ---
1.â€‹ torch.einsum("bd,dq->bhq", x, W_q) â†’ would require W_q shape [d, q] and 
somehow magically create an h dimension (doesnâ€™t work).â€‹
 
2.â€‹ torch.einsum("bd,hdq->bhq", x, W_q) â†’ correct for x: [b, d], W_q: [h, 
d, q].â€‹
 
3.â€‹ torch.einsum("bhd,dq->bhq", x, W_q) â†’ would require x to be [b, h, d] (itâ€™s 
not).â€‹
 
So: 
Answer (c): torch.einsum("bd,hdq->bhq", x, W_q) 
 
(d) What is D (expression for k)? 
Keys are shared: k should be [b, k]. 
Choices: 
1.â€‹ torch.einsum("bd,dk->bk", x, W_k) â†’ gives [b, k] with W_k: [d, k] âœ…â€‹
 
2.â€‹ torch.einsum("bd,hdk->bhk", x, W_k) â†’ would give per-head keys [b, h, k] 
(not shared).â€‹
 
3.â€‹ torch.einsum("bd,dk->bhk", x, W_k) â†’ output shape doesnâ€™t match indices.â€‹
 
So: 
Answer (d): torch.einsum("bd,dk->bk", x, W_k) 
 
(e) What is E (expression for v)? 
Values are also shared: v should be [b, v]. 
Choices: 
1.â€‹ torch.einsum("bd,hdv->bhv", x, W_v) â†’ per-head values.â€‹
 


--- Page 20 ---
2.â€‹ torch.einsum("bd,dv->bhv", x, W_v) â†’ output [b, h, v] (extra h dimension 
thatâ€™s not in indices).â€‹
 
3.â€‹ torch.einsum("bd,dv->bv", x, W_v) â†’ correct for x: [b,d], W_v: [d,v], 
output [b, v].â€‹
 
So: 
Answer (e): torch.einsum("bd,dv->bv", x, W_v) 
 
4(f): Computational complexity for MQA 
They gave for MHA (multi-head attention) incremental decoding: 
â—â€‹ Computational complexity:â€‹
 [â€‹
 O(b d^2 + b n d)â€‹
 ]â€‹
 (projections: (O(b d^2)), attention over cached (n) tokens: (O(b n d))).â€‹
 
For MQA: 
â—â€‹ Projections:â€‹
 
â—‹â€‹ Queries: q = einsum("bd,hdq->bhq", ...) â†’ (O(b d^2)).â€‹
 
â—‹â€‹ Keys: k = einsum("bd,dk->bk", ...) â†’ (O(b d^2 / h)).â€‹
 
â—‹â€‹ Values: v = einsum("bd,dv->bv", ...) â†’ (O(b d^2 / h)).â€‹
 
â—‹â€‹ Output: y = einsum("bhv,hvd->bd", ...) â†’ (O(b d^2)).â€‹
 
â—â€‹ All together â†’ still (O(b d^2)) in big-O.â€‹
 
â—â€‹ Attention with cache of length (n):â€‹
 
â—‹â€‹ Scores and weighted sums use shared keys/values but still do about (O(b n d)) work.â€‹
 


--- Page 21 ---
So in Big-O, the computational complexity is unchanged: 
Answer (f):â€‹
 [â€‹
 \boxed{O(b d^2 + b n d)}â€‹
 ] 
(Only constants improve, not the asymptotic order.) 
 
4(g): Memory access complexity for MQA 
For MHA, they gave:â€‹
 [â€‹
 O(d^2 + b d + b n d)â€‹
 ] 
Breakdown: 
â—â€‹ (O(d^2)): reading weight matricesâ€‹
 
â—â€‹ (O(b d)): current token embeddings (x, q, k, v, o, y)â€‹
 
â—â€‹ (O(b n d)): reading per-head cached keys/values of shape [b, h, n, d/h] â†’ (b n d)â€‹
 
For MQA: 
â—â€‹ Weights:â€‹
 
â—‹â€‹ W_q: [h, d, q] ~ (O(d^2))â€‹
 
â—‹â€‹ W_k: [d, k] and W_v: [d, v] ~ (O(d^2/h)) eachâ€‹
 
â—‹â€‹ W_o: [h, v, d] ~ (O(d^2))â€‹
 â†’ still overall (O(d^2)).â€‹
 
â—â€‹ Current token tensors (x, q, k, v, o, y): still all (O(b d)) total.â€‹
 
â—â€‹ Cached keys/values:â€‹
 


--- Page 22 ---
â—‹â€‹ prev_K: [b, n, k] with (k = d/h) â†’ (O(b n d / h))â€‹
 
â—‹â€‹ prev_V: [b, n, v] with (v = d/h) â†’ (O(b n d / h))â€‹
 â†’ total cache access: (O(b n d / h)), improved by a factor of (h) over standard MHA.â€‹
 
So: 
Answer (g):â€‹
 [â€‹
 \boxed{O\big(d^2 + b d + \tfrac{b n d}{h}\big)}â€‹
 ]â€‹
 (same leading-weight and token terms, but cache access is reduced from (O(b n d)) to (O(b 
n d / h)).) 
 
If youâ€™d like, next we can move on to Question 6 (Kernelized Linear Attention) and go through each 
subpart. 
Me: you may move on to the next question 
ChatGPT: 
Alright, onto Question 6: Kernelized Linear Attention (Part I). 
Iâ€™ll go part by part. 
 
6(a) Condition on sim so (V'_i) stays finite 
Equation (3) is: 
[â€‹
 V'i = \frac{\sum{j=1}^N \text{sim}(Q_i, K_j), V_j}â€‹
 {\sum_{j=1}^N \text{sim}(Q_i, K_j)}.â€‹
 ] 
For this to be finite, the denominator must never be zero: 
[â€‹
 \sum_{j=1}^N \text{sim}(Q_i, K_j) \neq 0\quad\text{for all } i.â€‹
 ] 


--- Page 23 ---
A sufficient set of conditions: 
â—â€‹ (\text{sim}(q,k) \ge 0) for all (q,k) (non-negative similarity), andâ€‹
 
â—â€‹ For every query (Q_i), there exists at least one key (K_j) such that (\text{sim}(Q_i, K_j) > 0).â€‹
 
That guarantees the denominator is strictly positive and (V'_i) is well-defined. 
 
6(b)(i) Quadratic polynomial kernel attention 
Polynomial kernel (degree (p)) is typically: 
[â€‹
 K(q,k) = (q^\top k + c)^pâ€‹
 ] 
(up to constant factors). They tell you to ignore constant factors, and just consider degree 2. 
A simple quadratic version (with bias term dropped for simplicity) is: 
[â€‹
 \boxed{\text{sim}(q,k) = (q^\top k)^2}â€‹
 ] 
(If you instead used ((q^\top k + 1)^2), itâ€™s still a quadratic kernel; theyâ€™re equivalent up to constants and 
linear terms, which theyâ€™re telling you not to worry about.) 
 
6(b)(ii) Feature map (\phi) for the quadratic kernel 
We want a feature map (\phi(\cdot)) such that: 
[â€‹
 K(q,k) = (q^\top k)^2 = \phi(q)^\top \phi(k).â€‹
 ] 
One standard choice: 
â—â€‹ Let (\phi(q)) be the vectorized outer-product of (q) with itself:â€‹
 


--- Page 24 ---
[â€‹
 \phi(q) = \text{vec}(q q^\top) \in \mathbb{R}^{D^2},â€‹
 ]â€‹
 i.e.â€‹
 [â€‹
 [\phi(q)]_{(a,b)} = q_a q_b.â€‹
 ] 
Then 
[â€‹
 \phi(q)^\top \phi(k)â€‹
 = \sum_{a,b} q_a q_b k_a k_bâ€‹
 = \left( \sum_a q_a k_a \right)^2â€‹
 = (q^\top k)^2.â€‹
 ] 
So a valid feature map is: 
[â€‹
 \boxed{\phi(q) = q \otimes q\ \text{(all pairwise products } q_a q_b)}â€‹
 ] 
(up to constant scaling factors they said you can ignore). 
 
6(b)(iii) Kernel attention written with (\phi(\cdot)) 
Equation (3) with a generic similarity: 
[â€‹
 V'iâ€‹
 = \frac{\sum{j=1}^N \text{sim}(Q_i, K_j), V_j}â€‹
 {\sum_{j=1}^N \text{sim}(Q_i, K_j)}.â€‹
 ] 
If (\text{sim}(q,k) = K(q,k) = \phi(q)^\top \phi(k)), then: 
[â€‹
 \boxed{â€‹
 V'iâ€‹
 = \frac{\sum{j=1}^N \phi(Q_i)^\top \phi(K_j), V_j}â€‹
 {\sum_{j=1}^N \phi(Q_i)^\top \phi(K_j)}â€‹


--- Page 25 ---
 }â€‹
 ] 
Thatâ€™s the kernel attention form written explicitly in terms of the feature map. 
 
6(c) Time & space complexity for softmax attention (Eq. 4) 
From Eq. (2): 
â—â€‹ (x \in \mathbb{R}^{N \times F})â€‹
 
â—â€‹ (W_Q \in \mathbb{R}^{F \times D},\ W_K \in \mathbb{R}^{F \times D},\ W_V \in \mathbb{R}^{F 
\times M})â€‹
 
â—â€‹ (Q = x W_Q \in \mathbb{R}^{N \times D})â€‹
 
â—â€‹ (K = x W_K \in \mathbb{R}^{N \times D})â€‹
 
â—â€‹ (V = x W_V \in \mathbb{R}^{N \times M})â€‹
 
Softmax attention: 
[â€‹
 V' = \text{softmax}!\left(\frac{Q K^\top}{\sqrt{D}}\right) V.â€‹
 ] 
Letâ€™s express cost in terms of (N, D, M) (treat (F) as on the order of (D)). 
Time complexity 
1.â€‹ Projections:â€‹
 
â—‹â€‹ (Q = xW_Q): (O(N D^2))â€‹
 
â—‹â€‹ (K = xW_K): (O(N D^2))â€‹
 
â—‹â€‹ (V = xW_V): (O(N D M))â€‹
 
2.â€‹ Attention scores:â€‹
 


--- Page 26 ---
â—‹â€‹ (QK^\top): multiply (N \times D) by (D \times N)â€‹
 â†’ (O(N^2 D))â€‹
 
3.â€‹ Softmax over NÃ—N matrix:â€‹
 
â—‹â€‹ (O(N^2)) (lower order compared to (N^2 D) if (D) is not tiny)â€‹
 
4.â€‹ Multiply by V:â€‹
 
â—‹â€‹ (\text{softmax}(QK^\top / \sqrt{D})) (shape (N \times N)) times (V) ((N \times M))â€‹
 â†’ (O(N^2 M))â€‹
 
Total time: 
[â€‹
 \boxed{â€‹
 T_{\text{softmax}} = O(N D^2 + N D M + N^2 D + N^2 M)â€‹
 \approx O\big(N^2(D + M)\big)â€‹
 }â€‹
 ] 
(the (N^2) terms dominate for long sequences). 
 
Space (memory) complexity 
For backprop, we need to store intermediate activations: 
â—â€‹ (Q \in \mathbb{R}^{N \times D}): (O(ND))â€‹
 
â—â€‹ (K \in \mathbb{R}^{N \times D}): (O(ND))â€‹
 
â—â€‹ (V \in \mathbb{R}^{N \times M}): (O(NM))â€‹
 
â—â€‹ Score matrix (S = QK^\top) and/or softmax matrix (P) (both (N \times N)): (O(N^2))â€‹
 
â—â€‹ Output (V' \in \mathbb{R}^{N \times M}): (O(NM))â€‹
 
Weights are parameters, not per-example activations, so usually not counted in this per-sequence 
memory. 


--- Page 27 ---
So overall: 
[â€‹
 \boxed{â€‹
 \text{Memory}_{\text{softmax}} = O\big(N^2 + N(D + M)\big)â€‹
 }â€‹
 ] 
The (N^2) term from the attention matrix is the killer for long sequences. 
 
6(d) Deriving the linearized kernel form (Eq. 5) 
Starting from kernel attention: 
[â€‹
 V'iâ€‹
 = \frac{\sum{j=1}^N \phi(Q_i)^\top \phi(K_j), V_j}â€‹
 {\sum_{j=1}^N \phi(Q_i)^\top \phi(K_j)}.â€‹
 ] 
Use associativity: 
â—â€‹ Defineâ€‹
 [â€‹
 S_1 = \sum_{j=1}^N \phi(K_j)^\top V_jâ€‹
 ]â€‹
 (this is an (M \times M) matrix if (\phi(K_j) \in \mathbb{R}^M)),â€‹
 
â—â€‹ andâ€‹
 [â€‹
 S_2 = \sum_{j=1}^N \phi(K_j)^\topâ€‹
 ]â€‹
 (an (M \times 1) vector).â€‹
 
Then: 
â—â€‹ Numerator:â€‹
 [â€‹
 \sum_j \phi(Q_i)^\top \phi(K_j), V_jâ€‹
 = \phi(Q_i)^\top \left(\sum_j \phi(K_j) V_j^\top \right)â€‹


--- Page 28 ---
 ]â€‹
 
â—â€‹ Denominator:â€‹
 [â€‹
 \sum_j \phi(Q_i)^\top \phi(K_j)â€‹
 = \phi(Q_i)^\top \left(\sum_j \phi(K_j)\right).â€‹
 ]â€‹
 
So we can write: 
[â€‹
 \boxed{â€‹
 V'iâ€‹
 = \frac{\phi(Q_i)^\top \left(\sum{j=1}^N \phi(K_j), V_j^\top\right)}â€‹
 {\phi(Q_i)^\top \left(\sum_{j=1}^N \phi(K_j)\right)}â€‹
 }â€‹
 ] 
which is essentially Equation (5) in the handout (up to transpose conventions). 
This factorization is what lets us avoid explicit (N \times N) operations. 
 
6(e) Complexity of kernel attention (Eq. 5) 
using a linearized polynomial kernel (\phi(x)) of degree 2, with (M \approx D) 
We now treat (\phi : \mathbb{R}^D \to \mathbb{R}^M) where (M) is on the order of (D) (so we donâ€™t 
blow up to (D^2); think random features). 
Weâ€™ll compute all (V'_i) using Eq. (5). 
Step 1: Compute feature maps (\phi(Q)) and (\phi(K)) 
â—â€‹ (Q \in \mathbb{R}^{N \times D} \Rightarrow \phi(Q) \in \mathbb{R}^{N \times M})â€‹
 
â—â€‹ (K \in \mathbb{R}^{N \times D} \Rightarrow \phi(K) \in \mathbb{R}^{N \times M})â€‹
 
Cost (per row) is (O(DM)). For all (N) positions: 


--- Page 29 ---
[â€‹
 O(N D M) \approx O(N D^2) \quad (\text{since } M \approx D)â€‹
 ] 
Step 2: Compute the global summaries (S_1) and (S_2) 
Let (\Phi_K) be the (N \times M) matrix of (\phi(K_j)) rows, and (V) be (N \times M). 
â—â€‹ (S_1 = \sum_j \phi(K_j)^\top V_j)â€‹
 This can be done as a matrix multiply:â€‹
 (\Phi_K^\top V \in \mathbb{R}^{M \times M}):â€‹
 cost (O(N M^2) \approx O(N D^2)).â€‹
 
â—â€‹ (S_2 = \sum_j \phi(K_j)) (or (\sum_j \phi(K_j)^\top); same order):â€‹
 cost (O(N M) \approx O(N D)).â€‹
 
Step 3: Compute each (V'_i) 
For each position (i): 
â—â€‹ Numerator: (\phi(Q_i)^\top S_1):â€‹
 ((1 \times M)(M \times M) \to 1 \times M): cost (O(M^2) \approx O(D^2)).â€‹
 
â—â€‹ Denominator: (\phi(Q_i)^\top S_2):â€‹
 ((1 \times M)(M \times 1) \to \text{scalar}): cost (O(M) \approx O(D)).â€‹
 
So per position: (O(D^2)), and for all (N): (O(N D^2)). 
Total time complexity 
Collecting the dominant terms: 
â—â€‹ Feature maps: (O(N D^2))â€‹
 
â—â€‹ Global summaries: (O(N D^2))â€‹
 
â—â€‹ Per-position outputs: (O(N D^2))â€‹
 
So overall: 


--- Page 30 ---
[â€‹
 \boxed{â€‹
 T_{\text{kernel}} \approx O(N D^2)â€‹
 }â€‹
 ] 
Compare to softmax: (O(N^2 (D+M)) \approx O(N^2 D)); weâ€™ve traded the quadratic in (N) for quadratic 
in (D), which is much better for very long sequences (N \gg D). 
 
Space (memory) complexity 
We need to store: 
â—â€‹ (\phi(Q) \in \mathbb{R}^{N \times M}): (O(NM) \approx O(ND))â€‹
 
â—â€‹ (\phi(K) \in \mathbb{R}^{N \times M}): (O(ND))â€‹
 
â—â€‹ (V \in \mathbb{R}^{N \times M}): (O(ND))â€‹
 
â—â€‹ Summaries (S_1 \in \mathbb{R}^{M \times M}) and (S_2 \in \mathbb{R}^{M}):â€‹
 (O(M^2 + M) \approx O(D^2))â€‹
 
â—â€‹ Output (V' \in \mathbb{R}^{N \times M}): (O(ND))â€‹
 
So, up to constants: 
[â€‹
 \boxed{â€‹
 \text{Memory}_{\text{kernel}} \approx O(ND + D^2)â€‹
 }â€‹
 ] 
Crucially, no (N^2) term anymore â€” thatâ€™s the whole win of linearized kernel attention. 
 
If you want, next we can handle the very last â€œHomework process / study groupâ€ question, or you can 
tell me your existing answers and I can help refine them. 
 


--- Page 31 ---
Me: your solution to 6e is incorrect, the kernel is quadratic 
 
ChatGPT: 
Youâ€™re right to flag that â€“ I implicitly treated the feature map as if it stayed (O(D))-dimensional, which is 
wrong for a quadratic polynomial kernel. For degree-2 polynomial kernel, the feature map is quadratic in 
(D). 
Let me fix 6(e) properly. 
Weâ€™re using Eq. (5): 
[â€‹
 V'iâ€‹
 = \frac{â€‹
 \phi(Q_i),â€‹
 \Big(\sum{j=1}^N \phi(K_j)^\top V_j\Big)â€‹
 }{â€‹
 \phi(Q_i),â€‹
 \Big(\sum_{j=1}^N \phi(K_j)\Big)â€‹
 }.â€‹
 \tag{5}â€‹
 ] 
â—â€‹ (Q, K \in \mathbb{R}^{N\times D})â€‹
 
â—â€‹ (V \in \mathbb{R}^{N\times M}), with (M \approx D)â€‹
 
â—â€‹ Quadratic polynomial kernel â‡’ (\phi(x)) has dimension (P = O(D^2)) (all pairwise products).â€‹
 
 
Time complexity 
Break down the main steps: 
1.â€‹ Compute (\phi(K_j)) and (\phi(Q_i))â€‹
 
â—‹â€‹ Each (\phi(x)) costs (O(D^2)) (since we form all degree-2 terms).â€‹
 
â—‹â€‹ For all (N) keys and (N) queries:â€‹
 [â€‹


--- Page 32 ---
 O(N D^2) + O(N D^2) = O(N D^2).â€‹
 ]â€‹
 
2.â€‹ Precompute â€œglobalâ€ summaries from keys and valuesâ€‹
 
â—‹â€‹ Let (P = \sum_{j=1}^N \phi(K_j)^\top V_j)â€‹
 Here each term is an outer product of size ((P \times M)), with cost (O(PM) = O(D^2 M)).â€‹
â€‹
 For all (j):â€‹
 [â€‹
 O(N P M) = O(N D^2 M).â€‹
 ]â€‹
 
â—‹â€‹ Let (s = \sum_{j=1}^N \phi(K_j))â€‹
 Cost (O(NP) = O(N D^2)) (lower order versus (N D^2 M)).â€‹
 
3.â€‹ Compute all outputs (V'_i)â€‹
â€‹
 For each (i):â€‹
 
â—‹â€‹ Numerator: (\phi(Q_i) P)â€‹
 Cost (O(PM) = O(D^2 M)).â€‹
 
â—‹â€‹ Denominator: (\phi(Q_i) s^\top)â€‹
 Cost (O(P) = O(D^2)).â€‹
 
4.â€‹ For all (N) positions:â€‹
 [â€‹
 O(N P M) = O(N D^2 M).â€‹
 ]â€‹
 
The dominant term overall is: 
[â€‹
 \boxed{\text{Time} = O(N D^2 M)}.â€‹
 ] 
Under the homeworkâ€™s assumption (M \approx D), this becomes 


--- Page 33 ---
[â€‹
 \boxed{\text{Time} \approx O(N D^3)}.â€‹
 ] 
So weâ€™ve removed the (N^2) dependence (vs. softmax attentionâ€™s (O(N^2(D+M)))), but now pay a 
quadratic cost in (D) through the kernel feature map. 
 
Space (memory) complexity 
We need to store, for backprop: 
â—â€‹ (Q, K \in \mathbb{R}^{N\times D}): (O(ND))â€‹
 
â—â€‹ (V \in \mathbb{R}^{N\times M}): (O(NM))â€‹
 
â—â€‹ (\phi(Q), \phi(K) \in \mathbb{R}^{N\times P}) with (P=O(D^2)):â€‹
 (O(NP) = O(N D^2))â€‹
 
â—â€‹ (P = \sum_j \phi(K_j)^\top V_j \in \mathbb{R}^{P\times M}):â€‹
 (O(PM) = O(D^2 M))â€‹
 
â—â€‹ (s = \sum_j \phi(K_j) \in \mathbb{R}^{P}): (O(D^2))â€‹
 
â—â€‹ Output (V' \in \mathbb{R}^{N\times M}): (O(NM))â€‹
 
Ignoring lower-order terms, the memory requirement is: 
[â€‹
 \boxed{\text{Memory} = O(N D^2 + D^2 M + N(D+M))}.â€‹
 ] 
With (M \approx D), that simplifies to roughly 
[â€‹
 \boxed{\text{Memory} \approx O(N D^2 + D^3)}.â€‹
 ] 
So the quadratic kernel shows up as quadratic (and even cubic with values) dependence on (D), while 
the dependence on sequence length (N) is now linear instead of quadratic. 
